[
  {
    "objectID": "CONTRIBUTING.html",
    "href": "CONTRIBUTING.html",
    "title": "How to contribute",
    "section": "",
    "text": "Before anything else, please install the git hooks that run automatic scripts during each commit and merge to strip the notebooks of superfluous metadata (and avoid merge conflicts). After cloning the repository, run the following command inside it:\nnbdev_install_git_hooks\n\n\n\n\nEnsure the bug was not already reported by searching on GitHub under Issues.\nIf you’re unable to find an open issue addressing the problem, open a new one. Be sure to include a title and clear description, as much relevant information as possible, and a code sample or an executable test case demonstrating the expected behavior that is not occurring.\nBe sure to add the complete error messages.\n\n\n\n\nOpen a new GitHub pull request with the patch.\nEnsure that your PR includes a test that fails without your patch, and pass with it.\nEnsure the PR description clearly describes the problem and solution. Include the relevant issue number if applicable.\n\n\n\n\n\n\nKeep each PR focused. While it’s more convenient, do not combine several unrelated fixes together. Create as many branches as needing to keep each PR focused.\nDo not mix style changes/fixes with “functional” changes. It’s very difficult to review such PRs and it most likely get rejected.\nDo not add/remove vertical whitespace. Preserve the original style of the file you edit as much as you can.\nDo not turn an already submitted PR into your development playground. If after you submitted PR, you discovered that more work is needed - close the PR, do the required work and then submit a new PR. Otherwise each of your commits requires attention from maintainers of the project.\nIf, however, you submitted a PR and received a request for changes, you should proceed with commits inside that PR, so that the maintainer can see the incremental fixes and won’t need to review the whole PR again. In the exception case where you realize it’ll take many many commits to complete the requests, then it’s probably best to close the PR, do the work and then submit it again. Use common sense where you’d choose one way over another.\n\n\n\n\n\nDocs are automatically created from the notebooks."
  },
  {
    "objectID": "CONTRIBUTING.html#how-to-get-started",
    "href": "CONTRIBUTING.html#how-to-get-started",
    "title": "How to contribute",
    "section": "",
    "text": "Before anything else, please install the git hooks that run automatic scripts during each commit and merge to strip the notebooks of superfluous metadata (and avoid merge conflicts). After cloning the repository, run the following command inside it:\nnbdev_install_git_hooks"
  },
  {
    "objectID": "CONTRIBUTING.html#did-you-find-a-bug",
    "href": "CONTRIBUTING.html#did-you-find-a-bug",
    "title": "How to contribute",
    "section": "",
    "text": "Ensure the bug was not already reported by searching on GitHub under Issues.\nIf you’re unable to find an open issue addressing the problem, open a new one. Be sure to include a title and clear description, as much relevant information as possible, and a code sample or an executable test case demonstrating the expected behavior that is not occurring.\nBe sure to add the complete error messages.\n\n\n\n\nOpen a new GitHub pull request with the patch.\nEnsure that your PR includes a test that fails without your patch, and pass with it.\nEnsure the PR description clearly describes the problem and solution. Include the relevant issue number if applicable."
  },
  {
    "objectID": "CONTRIBUTING.html#pr-submission-guidelines",
    "href": "CONTRIBUTING.html#pr-submission-guidelines",
    "title": "How to contribute",
    "section": "",
    "text": "Keep each PR focused. While it’s more convenient, do not combine several unrelated fixes together. Create as many branches as needing to keep each PR focused.\nDo not mix style changes/fixes with “functional” changes. It’s very difficult to review such PRs and it most likely get rejected.\nDo not add/remove vertical whitespace. Preserve the original style of the file you edit as much as you can.\nDo not turn an already submitted PR into your development playground. If after you submitted PR, you discovered that more work is needed - close the PR, do the required work and then submit a new PR. Otherwise each of your commits requires attention from maintainers of the project.\nIf, however, you submitted a PR and received a request for changes, you should proceed with commits inside that PR, so that the maintainer can see the incremental fixes and won’t need to review the whole PR again. In the exception case where you realize it’ll take many many commits to complete the requests, then it’s probably best to close the PR, do the work and then submit it again. Use common sense where you’d choose one way over another."
  },
  {
    "objectID": "CONTRIBUTING.html#do-you-want-to-contribute-to-the-documentation",
    "href": "CONTRIBUTING.html#do-you-want-to-contribute-to-the-documentation",
    "title": "How to contribute",
    "section": "",
    "text": "Docs are automatically created from the notebooks."
  },
  {
    "objectID": "documentation.html",
    "href": "documentation.html",
    "title": "Model documentation",
    "section": "",
    "text": "Each user has a specific documentation need, ranging from simply logging the model training to a more complex description of the model pipeline with a discusson of the model outcomes. gingado addresses this variety of needs by offering a class of objects, “Documenters”, that facilitate model documentation. A base class facilitates the creation of generic ways to document models, and gingado includes two specific model documentation templates off-the-shelf as described below.\nThe model documentation is performed by Documenters, objects that subclass from the base class ggdModelDocumentation. This base class offers code that can be used by any Documenter to read the model in question, format the information according to a template and save the resulting documentation in a JSON format. Documenters save the underlying information using the JSON format. With the JSON documentation file at hand, the user can then deploy existing third-party libraries to transform the information stored in JSON into a variety of formats (eg, HTML, PDF) as needed.\nOne current area of development is the automatic filing of some fields related to the model. The objective is to automatise documentation of the information that can be fetched automatically from the model, leaving time for the analyst to concentrate on other tasks, such as considering the ethical implications of the machine learning model being trained."
  },
  {
    "objectID": "documentation.html#modelcard",
    "href": "documentation.html#modelcard",
    "title": "Model documentation",
    "section": "ModelCard",
    "text": "ModelCard\nModelCard - the model documentation template inspired by the work of Mitchell et al. (2018) already comes with gingado. Its template can be used by users as is, or tweaked according to each need. The ModelCard template can also serve as inspiration for any custom documentation needs. Users with documentation needs beyond the out-of-the-box solutions provided by gingado can create their own class of Documenters (more information on that below), and compatibility with these custom documentation routines with the rest of the code is ensured. Users are encouraged to submit a pull request with their own documentation models subclassing ggdModelDocumentation if these custom templates can also benefit other users.\nLike all gingado Documenters, a ModelCard is can be easily created on a standalone basis as shown below, or as part of a gingado.ggdBenchmark object.\n\nmodel_doc = ModelCard()\n\nBy default, it autofills the template with the current date and time. Users can add other information to be automatically added by a customised Documenter object.\n\nmodel_doc_with_autofill = ModelCard(autofill=True)\nmodel_doc_no_autofill = ModelCard(autofill=False)\n\nBelow is a comparison of the model_details section of the model document, with and without the autofill.\n\nmodel_doc_with_autofill.show_json()['model_details']\n\n{'developer': 'Person or organisation developing the model',\n 'datetime': '2023-06-22 09:05:40 ',\n 'version': 'Model version',\n 'type': 'Model type',\n 'info': 'Information about training algorithms, parameters, fairness constraints or other applied approaches, and features',\n 'paper': 'Paper or other resource for more information',\n 'citation': 'Citation details',\n 'license': 'License',\n 'contact': 'Where to send questions or comments about the model'}\n\n\n\nmodel_doc_no_autofill.show_json()['model_details']\n\n{'developer': 'Person or organisation developing the model',\n 'datetime': 'Model date',\n 'version': 'Model version',\n 'type': 'Model type',\n 'info': 'Information about training algorithms, parameters, fairness constraints or other applied approaches, and features',\n 'paper': 'Paper or other resource for more information',\n 'citation': 'Citation details',\n 'license': 'License',\n 'contact': 'Where to send questions or comments about the model'}\n\n\n\nsource\n\nModelCard\n\n ModelCard (file_path:str='', autofill:bool=True, indent_level:int|None=2)\n\nA gingado Documenter based on Mitchell et al. (2018)\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfile_path\nstr\n\nPath for the JSON file with the documentation\n\n\nautofill\nbool\nTrue\nWhether the Documenter object should autofill when created\n\n\nindent_level\nint | None\n2\nLevel of indentation during serialisation to JSON\n\n\n\n\nsource\n\nautofill_template\n\n autofill_template ()\n\nCreate an empty model card template, then fills it with information that is automatically obtained from the system"
  },
  {
    "objectID": "documentation.html#forecastcard",
    "href": "documentation.html#forecastcard",
    "title": "Model documentation",
    "section": "ForecastCard",
    "text": "ForecastCard\nForecastCard is a model documentation template inspired by Mitchell et al. (2018), but with fields that are more specifically targeted towards forecasting or nowcasting use cases.\nBecause a ForecastCard Documenter object is targeted to forecasting and nowcasting models, it contains some specialised fields, as illustrated below.\n\nmodel_doc = ForecastCard()\n\nmodel_doc.show_template()\n\n{\n  \"model_details\": {\n    \"field_description\": \"Basic information about the model\",\n    \"variable\": \"Variable(s) being forecasted or nowcasted\",\n    \"jurisdiction\": \"Jurisdiction(s) of the variable being forecasted or nowcasted\",\n    \"developer\": \"Person or organisation developing the model\",\n    \"datetime\": \"Model date\",\n    \"version\": \"Model version\",\n    \"type\": \"Model type\",\n    \"pipeline\": \"Description of the pipeline steps being used\",\n    \"info\": \"Information about training algorithms, parameters, fairness constraints or other applied approaches, and features\",\n    \"econometric_model\": \"Information about the econometric model or technique\",\n    \"paper\": \"Paper or other resource for more information\",\n    \"citation\": \"Citation details\",\n    \"license\": \"License\",\n    \"contact\": \"Where to send questions or comments about the model\"\n  },\n  \"intended_use\": {\n    \"field_description\": \"Use cases that were envisioned during development\",\n    \"primary_uses\": \"Primary intended uses\",\n    \"primary_users\": \"Primary intended users\",\n    \"out_of_scope\": \"Out-of-scope use cases\"\n  },\n  \"factors\": {\n    \"field_description\": \"Factors could include demographic or phenotypic groups, environmental conditions, technical attributes, or others\",\n    \"relevant\": \"Relevant factors\",\n    \"evaluation\": \"Evaluation factors\"\n  },\n  \"metrics\": {\n    \"field_description\": \"Metrics should be chosen to reflect potential real world impacts of the model\",\n    \"performance_measures\": \"Model performance measures\",\n    \"estimation_approaches\": \"How are the evaluation metrics calculated? Include information on the cross-validation approach, if used\"\n  },\n  \"data\": {\n    \"field_description\": \"Details on the dataset(s) used for the training and evaluation of the model\",\n    \"datasets\": \"Datasets\",\n    \"preprocessing\": \"Preprocessing\",\n    \"cutoff_date\": \"Cut-off date that separates training from evaluation data\"\n  },\n  \"ethical_considerations\": {\n    \"field_description\": \"Ethical considerations that went into model development, surfacing ethical challenges and solutions to stakeholders. Ethical analysis does not always lead to precise solutions, but the process of ethical contemplation is worthwhile to inform on responsible practices and next steps in future work.\",\n    \"sensitive_data\": \"Does the model use any sensitive data (e.g., protected classes)?\",\n    \"risks_and_harms\": \"What risks may be present in model usage? Try to identify the potential recipients, likelihood, and magnitude of harms. If these cannot be determined, note that they were considered but remain unknown\",\n    \"use_cases\": \"Are there any known model use cases that are especially fraught?\",\n    \"additional_information\": \"If possible, this section should also include any additional ethical considerations that went into model development, for example, review by an external board, or testing with a specific community.\"\n  },\n  \"caveats_recommendations\": {\n    \"field_description\": \"Additional concerns that were not covered in the previous sections\",\n    \"caveats\": \"For example, did the results suggest any further testing? Were there any relevant groups that were not represented in the evaluation dataset?\",\n    \"recommendations\": \"Are there additional recommendations for model use? What are the ideal characteristics of an evaluation dataset for this model?\"\n  }\n}\n\n\n\nmodel_doc.show_json()\n\n{'model_details': {'variable': 'Variable(s) being forecasted or nowcasted',\n  'jurisdiction': 'Jurisdiction(s) of the variable being forecasted or nowcasted',\n  'developer': 'Person or organisation developing the model',\n  'datetime': '2023-06-22 09:05:40 ',\n  'version': 'Model version',\n  'type': 'Model type',\n  'pipeline': 'Description of the pipeline steps being used',\n  'info': 'Information about training algorithms, parameters, fairness constraints or other applied approaches, and features',\n  'econometric_model': 'Information about the econometric model or technique',\n  'paper': 'Paper or other resource for more information',\n  'citation': 'Citation details',\n  'license': 'License',\n  'contact': 'Where to send questions or comments about the model'},\n 'intended_use': {'primary_uses': 'Primary intended uses',\n  'primary_users': 'Primary intended users',\n  'out_of_scope': 'Out-of-scope use cases'},\n 'factors': {'relevant': 'Relevant factors',\n  'evaluation': 'Evaluation factors'},\n 'metrics': {'performance_measures': 'Model performance measures',\n  'estimation_approaches': 'How are the evaluation metrics calculated? Include information on the cross-validation approach, if used'},\n 'data': {'datasets': 'Datasets',\n  'preprocessing': 'Preprocessing',\n  'cutoff_date': 'Cut-off date that separates training from evaluation data'},\n 'ethical_considerations': {'sensitive_data': 'Does the model use any sensitive data (e.g., protected classes)?',\n  'risks_and_harms': 'What risks may be present in model usage? Try to identify the potential recipients, likelihood, and magnitude of harms. If these cannot be determined, note that they were considered but remain unknown',\n  'use_cases': 'Are there any known model use cases that are especially fraught?',\n  'additional_information': 'If possible, this section should also include any additional ethical considerations that went into model development, for example, review by an external board, or testing with a specific community.'},\n 'caveats_recommendations': {'caveats': 'For example, did the results suggest any further testing? Were there any relevant groups that were not represented in the evaluation dataset?',\n  'recommendations': 'Are there additional recommendations for model use? What are the ideal characteristics of an evaluation dataset for this model?'}}\n\n\n\nsource\n\nForecastCard\n\n ForecastCard (file_path:str='', autofill:bool=True,\n               indent_level:int|None=2)\n\nA gingado Documenter for forecasting or nowcasting use cases\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfile_path\nstr\n\nPath for the JSON file with the documentation\n\n\nautofill\nbool\nTrue\nWhether the Documenter object should autofill when created\n\n\nindent_level\nint | None\n2\nLevel of indentation during serialisation to JSON\n\n\n\n\nsource\n\nautofill_template\n\n autofill_template ()\n\nCreate an empty model card template, then fills it with information that is automatically obtained from the system"
  },
  {
    "objectID": "documentation.html#preliminaries",
    "href": "documentation.html#preliminaries",
    "title": "Model documentation",
    "section": "Preliminaries",
    "text": "Preliminaries\nThe mock dataset below is used to construct models using different libraries, to demonstrate how they are read by Documenters.\n\nfrom sklearn.datasets import make_classification\n\n\n# some mock up data\nX, y = make_classification()\n\nX.shape, y.shape\n\n((100, 20), (100,))"
  },
  {
    "objectID": "documentation.html#gingado-benchmark",
    "href": "documentation.html#gingado-benchmark",
    "title": "Model documentation",
    "section": "gingado Benchmark",
    "text": "gingado Benchmark\n\nfrom gingado.benchmark import ClassificationBenchmark\n\n\n# the gingado benchmark\ngingado_clf = ClassificationBenchmark(verbose_grid=1).fit(X, y)\n\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n\n\n\n# a new instance of ModelCard is created and used to document the model\nmodel_doc_gingado = ModelCard()\nmodel_doc_gingado.read_model(gingado_clf.benchmark)\nprint(model_doc_gingado.show_json()['model_details']['info'])\n\n# but given that gingado Benchmark objects already document the best model at every fit, we can check that they are equal:\nassert model_doc_gingado.show_json()['model_details']['info'] == gingado_clf.model_documentation.show_json()['model_details']['info']\n\n{'_estimator_type': 'classifier', 'best_estimator_': RandomForestClassifier(oob_score=True), 'best_index_': 0, 'best_params_': {'max_features': 'sqrt', 'n_estimators': 100}, 'best_score_': 0.99, 'classes_': array([0, 1]), 'cv_results_': {'mean_fit_time': array([0.13181503, 0.29619505, 0.1136914 , 0.28267403, 0.12027018,\n       0.29762466]), 'std_fit_time': array([0.01625579, 0.02733223, 0.00146566, 0.00806355, 0.0079422 ,\n       0.02163048]), 'mean_score_time': array([0.00838451, 0.01815953, 0.0072778 , 0.01760452, 0.00771282,\n       0.01794319]), 'std_score_time': array([0.0007298 , 0.00164848, 0.00029095, 0.00087866, 0.00064812,\n       0.00179062]), 'param_max_features': masked_array(data=['sqrt', 'sqrt', 'log2', 'log2', None, None],\n             mask=[False, False, False, False, False, False],\n       fill_value='?',\n            dtype=object), 'param_n_estimators': masked_array(data=[100, 250, 100, 250, 100, 250],\n             mask=[False, False, False, False, False, False],\n       fill_value='?',\n            dtype=object), 'params': [{'max_features': 'sqrt', 'n_estimators': 100}, {'max_features': 'sqrt', 'n_estimators': 250}, {'max_features': 'log2', 'n_estimators': 100}, {'max_features': 'log2', 'n_estimators': 250}, {'max_features': None, 'n_estimators': 100}, {'max_features': None, 'n_estimators': 250}], 'split0_test_score': array([0.9, 0.9, 0.9, 0.9, 0.9, 0.9]), 'split1_test_score': array([1., 1., 1., 1., 1., 1.]), 'split2_test_score': array([1., 1., 1., 1., 1., 1.]), 'split3_test_score': array([1., 1., 1., 1., 1., 1.]), 'split4_test_score': array([1., 1., 1., 1., 1., 1.]), 'split5_test_score': array([1., 1., 1., 1., 1., 1.]), 'split6_test_score': array([1., 1., 1., 1., 1., 1.]), 'split7_test_score': array([1., 1., 1., 1., 1., 1.]), 'split8_test_score': array([1., 1., 1., 1., 1., 1.]), 'split9_test_score': array([1., 1., 1., 1., 1., 1.]), 'mean_test_score': array([0.99, 0.99, 0.99, 0.99, 0.99, 0.99]), 'std_test_score': array([0.03, 0.03, 0.03, 0.03, 0.03, 0.03]), 'rank_test_score': array([1, 1, 1, 1, 1, 1], dtype=int32)}, 'multimetric_': False, 'n_features_in_': 20, 'n_splits_': 10, 'refit_time_': 0.13797831535339355, 'scorer_': &lt;function _passthrough_scorer&gt;}"
  },
  {
    "objectID": "documentation.html#scikit-learn",
    "href": "documentation.html#scikit-learn",
    "title": "Model documentation",
    "section": "scikit-learn",
    "text": "scikit-learn\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nsklearn_clf = RandomForestClassifier().fit(X, y)\n\n\nmodel_doc_sklearn = ModelCard()\nmodel_doc_sklearn.read_model(sklearn_clf)\nprint(model_doc_sklearn.show_json()['model_details']['info'])\n\n{'_estimator_type': 'classifier', 'base_estimator_': DecisionTreeClassifier(), 'classes_': array([0, 1]), 'estimators_': [DecisionTreeClassifier(max_features='sqrt', random_state=644629283), DecisionTreeClassifier(max_features='sqrt', random_state=773222412), DecisionTreeClassifier(max_features='sqrt', random_state=172080181), DecisionTreeClassifier(max_features='sqrt', random_state=777314458), DecisionTreeClassifier(max_features='sqrt', random_state=890227001), DecisionTreeClassifier(max_features='sqrt', random_state=638760693), DecisionTreeClassifier(max_features='sqrt', random_state=221026659), DecisionTreeClassifier(max_features='sqrt', random_state=8390130), DecisionTreeClassifier(max_features='sqrt', random_state=432789656), DecisionTreeClassifier(max_features='sqrt', random_state=1972759968), DecisionTreeClassifier(max_features='sqrt', random_state=1576187512), DecisionTreeClassifier(max_features='sqrt', random_state=166168759), DecisionTreeClassifier(max_features='sqrt', random_state=1463202577), DecisionTreeClassifier(max_features='sqrt', random_state=1755000244), DecisionTreeClassifier(max_features='sqrt', random_state=1886503315), DecisionTreeClassifier(max_features='sqrt', random_state=1377389061), DecisionTreeClassifier(max_features='sqrt', random_state=594812349), DecisionTreeClassifier(max_features='sqrt', random_state=191490399), DecisionTreeClassifier(max_features='sqrt', random_state=2094686855), DecisionTreeClassifier(max_features='sqrt', random_state=1782311375), DecisionTreeClassifier(max_features='sqrt', random_state=375006184), DecisionTreeClassifier(max_features='sqrt', random_state=1886779260), DecisionTreeClassifier(max_features='sqrt', random_state=20598082), DecisionTreeClassifier(max_features='sqrt', random_state=1038188436), DecisionTreeClassifier(max_features='sqrt', random_state=1821597520), DecisionTreeClassifier(max_features='sqrt', random_state=850416929), DecisionTreeClassifier(max_features='sqrt', random_state=1853389052), DecisionTreeClassifier(max_features='sqrt', random_state=250119911), DecisionTreeClassifier(max_features='sqrt', random_state=1259325505), DecisionTreeClassifier(max_features='sqrt', random_state=76487423), DecisionTreeClassifier(max_features='sqrt', random_state=1409801089), DecisionTreeClassifier(max_features='sqrt', random_state=598699990), DecisionTreeClassifier(max_features='sqrt', random_state=76174600), DecisionTreeClassifier(max_features='sqrt', random_state=193904761), DecisionTreeClassifier(max_features='sqrt', random_state=1703822078), DecisionTreeClassifier(max_features='sqrt', random_state=1339222994), DecisionTreeClassifier(max_features='sqrt', random_state=2010201174), DecisionTreeClassifier(max_features='sqrt', random_state=1140435560), DecisionTreeClassifier(max_features='sqrt', random_state=752716613), DecisionTreeClassifier(max_features='sqrt', random_state=154000048), DecisionTreeClassifier(max_features='sqrt', random_state=834145348), DecisionTreeClassifier(max_features='sqrt', random_state=1512945615), DecisionTreeClassifier(max_features='sqrt', random_state=150048855), DecisionTreeClassifier(max_features='sqrt', random_state=1569930300), DecisionTreeClassifier(max_features='sqrt', random_state=847545530), DecisionTreeClassifier(max_features='sqrt', random_state=467086770), DecisionTreeClassifier(max_features='sqrt', random_state=904736232), DecisionTreeClassifier(max_features='sqrt', random_state=550161302), DecisionTreeClassifier(max_features='sqrt', random_state=1272240134), DecisionTreeClassifier(max_features='sqrt', random_state=1979934607), DecisionTreeClassifier(max_features='sqrt', random_state=1968170115), DecisionTreeClassifier(max_features='sqrt', random_state=1735755797), DecisionTreeClassifier(max_features='sqrt', random_state=730511601), DecisionTreeClassifier(max_features='sqrt', random_state=1676843219), DecisionTreeClassifier(max_features='sqrt', random_state=1348911102), DecisionTreeClassifier(max_features='sqrt', random_state=819688245), DecisionTreeClassifier(max_features='sqrt', random_state=834285415), DecisionTreeClassifier(max_features='sqrt', random_state=1838527500), DecisionTreeClassifier(max_features='sqrt', random_state=358239969), DecisionTreeClassifier(max_features='sqrt', random_state=1101177373), DecisionTreeClassifier(max_features='sqrt', random_state=994347621), DecisionTreeClassifier(max_features='sqrt', random_state=1882007098), DecisionTreeClassifier(max_features='sqrt', random_state=1912439135), DecisionTreeClassifier(max_features='sqrt', random_state=1021450603), DecisionTreeClassifier(max_features='sqrt', random_state=177239870), DecisionTreeClassifier(max_features='sqrt', random_state=335200354), DecisionTreeClassifier(max_features='sqrt', random_state=66252232), DecisionTreeClassifier(max_features='sqrt', random_state=956249068), DecisionTreeClassifier(max_features='sqrt', random_state=30070017), DecisionTreeClassifier(max_features='sqrt', random_state=1330396499), DecisionTreeClassifier(max_features='sqrt', random_state=207906059), DecisionTreeClassifier(max_features='sqrt', random_state=1323234017), DecisionTreeClassifier(max_features='sqrt', random_state=2083574934), DecisionTreeClassifier(max_features='sqrt', random_state=1721483578), DecisionTreeClassifier(max_features='sqrt', random_state=439587182), DecisionTreeClassifier(max_features='sqrt', random_state=888640081), DecisionTreeClassifier(max_features='sqrt', random_state=1274687214), DecisionTreeClassifier(max_features='sqrt', random_state=1394566340), DecisionTreeClassifier(max_features='sqrt', random_state=1058291198), DecisionTreeClassifier(max_features='sqrt', random_state=1868840268), DecisionTreeClassifier(max_features='sqrt', random_state=1367322994), DecisionTreeClassifier(max_features='sqrt', random_state=217588037), DecisionTreeClassifier(max_features='sqrt', random_state=1419428676), DecisionTreeClassifier(max_features='sqrt', random_state=849946680), DecisionTreeClassifier(max_features='sqrt', random_state=2096213933), DecisionTreeClassifier(max_features='sqrt', random_state=2004379246), DecisionTreeClassifier(max_features='sqrt', random_state=428198071), DecisionTreeClassifier(max_features='sqrt', random_state=774183356), DecisionTreeClassifier(max_features='sqrt', random_state=1452489189), DecisionTreeClassifier(max_features='sqrt', random_state=410945613), DecisionTreeClassifier(max_features='sqrt', random_state=2035389513), DecisionTreeClassifier(max_features='sqrt', random_state=1818418019), DecisionTreeClassifier(max_features='sqrt', random_state=1905831285), DecisionTreeClassifier(max_features='sqrt', random_state=1263948971), DecisionTreeClassifier(max_features='sqrt', random_state=1860077210), DecisionTreeClassifier(max_features='sqrt', random_state=1006638035), DecisionTreeClassifier(max_features='sqrt', random_state=2072402722), DecisionTreeClassifier(max_features='sqrt', random_state=1095293459), DecisionTreeClassifier(max_features='sqrt', random_state=716644424), DecisionTreeClassifier(max_features='sqrt', random_state=844801010)], 'feature_importances_': array([0.01427207, 0.0160448 , 0.07213446, 0.00679389, 0.01636618,\n       0.0119022 , 0.01342794, 0.16113749, 0.01067974, 0.47137566,\n       0.01179527, 0.07481836, 0.00827858, 0.02065695, 0.01097749,\n       0.01589413, 0.02206046, 0.01543262, 0.00987852, 0.01607318]), 'n_classes_': 2, 'n_features_': 20, 'n_features_in_': 20, 'n_outputs_': 1}\n\n\n/Users/douglasaraujo/Coding/.venv_gingado/lib/python3.10/site-packages/sklearn/utils/deprecation.py:103: FutureWarning: Attribute `n_features_` was deprecated in version 1.0 and will be removed in 1.2. Use `n_features_in_` instead.\n  warnings.warn(msg, category=FutureWarning)"
  },
  {
    "objectID": "documentation.html#keras",
    "href": "documentation.html#keras",
    "title": "Model documentation",
    "section": "Keras",
    "text": "Keras\n\nfrom tensorflow import keras\n\n2023-06-22 09:05:56.756655: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\n\nkeras_clf = keras.Sequential()\nkeras_clf.add(keras.layers.Dense(16, activation='relu', input_shape=(20,)))\nkeras_clf.add(keras.layers.Dense(8, activation='relu'))\nkeras_clf.add(keras.layers.Dense(1, activation='sigmoid'))\nkeras_clf.compile(optimizer='sgd', loss='binary_crossentropy')\nkeras_clf.fit(X, y, batch_size=10, epochs=10)\n\nMetal device set to: AMD Radeon Pro 5500M\n\nsystemMemory: 64.00 GB\nmaxCacheSize: 3.99 GB\n\nEpoch 1/10\n\n\n2023-06-22 09:06:05.182863: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:418 : NOT_FOUND: could not find registered platform with id: 0x7fda3b1488a0\n2023-06-22 09:06:05.182925: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:418 : NOT_FOUND: could not find registered platform with id: 0x7fda3b1488a0\n2023-06-22 09:06:05.525360: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:418 : NOT_FOUND: could not find registered platform with id: 0x7fda3b1488a0\n2023-06-22 09:06:05.525391: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:418 : NOT_FOUND: could not find registered platform with id: 0x7fda3b1488a0\n2023-06-22 09:06:05.794085: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:418 : NOT_FOUND: could not find registered platform with id: 0x7fda3b1488a0\n2023-06-22 09:06:05.794121: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:418 : NOT_FOUND: could not find registered platform with id: 0x7fda3b1488a0\n\n\nNotFoundError: Graph execution error:\n\nDetected at node 'StatefulPartitionedCall_4' defined at (most recent call last):\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/Users/douglasaraujo/Coding/.venv_gingado/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in &lt;module&gt;\n      app.launch_new_instance()\n    File \"/Users/douglasaraujo/Coding/.venv_gingado/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/Users/douglasaraujo/Coding/.venv_gingado/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 724, in start\n      self.io_loop.start()\n    File \"/Users/douglasaraujo/Coding/.venv_gingado/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/base_events.py\", line 595, in run_forever\n      self._run_once()\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/base_events.py\", line 1881, in _run_once\n      handle._run()\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/Users/douglasaraujo/Coding/.venv_gingado/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 512, in dispatch_queue\n      await self.process_one()\n    File \"/Users/douglasaraujo/Coding/.venv_gingado/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 501, in process_one\n      await dispatch(*args)\n    File \"/Users/douglasaraujo/Coding/.venv_gingado/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 408, in dispatch_shell\n      await result\n    File \"/Users/douglasaraujo/Coding/.venv_gingado/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 731, in execute_request\n      reply_content = await reply_content\n    File \"/Users/douglasaraujo/Coding/.venv_gingado/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 424, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/Users/douglasaraujo/Coding/.venv_gingado/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/Users/douglasaraujo/Coding/.venv_gingado/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2768, in run_cell\n      result = self._run_cell(\n    File \"/Users/douglasaraujo/Coding/.venv_gingado/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2814, in _run_cell\n      return runner(coro)\n    File \"/Users/douglasaraujo/Coding/.venv_gingado/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/Users/douglasaraujo/Coding/.venv_gingado/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3012, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/Users/douglasaraujo/Coding/.venv_gingado/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3191, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/Users/douglasaraujo/Coding/.venv_gingado/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3251, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/var/folders/b9/p8z57lqd55xfk68xz34dg0s40000gn/T/ipykernel_34473/3587272776.py\", line 6, in &lt;module&gt;\n      keras_clf.fit(X, y, batch_size=10, epochs=10)\n    File \"/Users/douglasaraujo/Coding/.venv_gingado/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/douglasaraujo/Coding/.venv_gingado/lib/python3.10/site-packages/keras/engine/training.py\", line 1650, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/Users/douglasaraujo/Coding/.venv_gingado/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in train_function\n      return step_function(self, iterator)\n    File \"/Users/douglasaraujo/Coding/.venv_gingado/lib/python3.10/site-packages/keras/engine/training.py\", line 1233, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/douglasaraujo/Coding/.venv_gingado/lib/python3.10/site-packages/keras/engine/training.py\", line 1222, in run_step\n      outputs = model.train_step(data)\n    File \"/Users/douglasaraujo/Coding/.venv_gingado/lib/python3.10/site-packages/keras/engine/training.py\", line 1027, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/Users/douglasaraujo/Coding/.venv_gingado/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 527, in minimize\n      self.apply_gradients(grads_and_vars)\n    File \"/Users/douglasaraujo/Coding/.venv_gingado/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1140, in apply_gradients\n      return super().apply_gradients(grads_and_vars, name=name)\n    File \"/Users/douglasaraujo/Coding/.venv_gingado/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 634, in apply_gradients\n      iteration = self._internal_apply_gradients(grads_and_vars)\n    File \"/Users/douglasaraujo/Coding/.venv_gingado/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1166, in _internal_apply_gradients\n      return tf.__internal__.distribute.interim.maybe_merge_call(\n    File \"/Users/douglasaraujo/Coding/.venv_gingado/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1216, in _distributed_apply_gradients_fn\n      distribution.extended.update(\n    File \"/Users/douglasaraujo/Coding/.venv_gingado/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1211, in apply_grad_to_update_var\n      return self._update_step_xla(grad, var, id(self._var_key(var)))\nNode: 'StatefulPartitionedCall_4'\ncould not find registered platform with id: 0x7fda3b1488a0\n     [[{{node StatefulPartitionedCall_4}}]] [Op:__inference_train_function_664]\n\n\n\nmodel_doc_keras = ModelCard()\nmodel_doc_keras.read_model(keras_clf)\nmodel_doc_keras.show_json()['model_details']['info']\n\n'{\"class_name\": \"Sequential\", \"config\": {\"name\": \"sequential\", \"layers\": [{\"class_name\": \"InputLayer\", \"config\": {\"batch_input_shape\": [null, 20], \"dtype\": \"float32\", \"sparse\": false, \"ragged\": false, \"name\": \"dense_input\"}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense\", \"trainable\": true, \"batch_input_shape\": [null, 20], \"dtype\": \"float32\", \"units\": 16, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_1\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 8, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_2\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 1, \"activation\": \"sigmoid\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}]}, \"keras_version\": \"2.8.0\", \"backend\": \"tensorflow\"}'"
  },
  {
    "objectID": "documentation.html#other-models",
    "href": "documentation.html#other-models",
    "title": "Model documentation",
    "section": "Other models",
    "text": "Other models\nNative support for automatic documentation of other model types, such as from fastai, pytorch is expected to be available in future versions. Until then, any models coded form scratch by the user as well as any other model can be documented by passing the information as an argument to the Documenter’s fill_model_info method. This can be done with a string or dictionary. For example:\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\n\nclass MockDataset(torch.utils.data.Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X.astype(np.float32))\n        self.y = torch.from_numpy(y.astype(np.float32))\n        self.len = self.X.shape[0]\n\n    def __len__(self):\n        return self.len\n\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n\nclass PytorchNet(torch.nn.Module):\n    def __init__(self):\n        super(PytorchNet, self).__init__()\n        self.layer1 = torch.nn.Linear(20, 16)\n        self.layer2 = torch.nn.Linear(16, 8)\n        self.layer3 = torch.nn.Linear(8, 1)\n\n    def forward(self, x):\n        x = torch.relu(self.layer1(x))\n        x = torch.relu(self.layer2(x))\n        x = torch.sigmoid(self.layer3(x))\n        return x\n\npytorch_clf = PytorchNet()\n\ndataloader = MockDataset(X, y)\n\n\nloss_func = torch.nn.BCELoss()\noptimizer = torch.optim.SGD(pytorch_clf.parameters(), lr=0.001, momentum=0.9)\n\nfor epoch in range(10):\n    running_loss = 0.0\n    for i, data in enumerate(dataloader, 0):\n        _X, _y = data\n        optimizer.zero_grad()\n        y_pred_epoch = pytorch_clf(_X)\n        loss = loss_func(y_pred_epoch, _y.reshape(1))\n        loss.backward()\n        optimizer.step()\n\n\nmodel_doc_pytorch = ModelCard()\nmodel_doc_pytorch.fill_model_info(\"This model is a neural network consisting of two fully connected layers and ending in a linear layer with a sigmoid activation\")\nmodel_doc_pytorch.show_json()['model_details']['info']\n\n'This model is a neural network consisting of two fully connected layers and ending in a linear layer with a sigmoid activation'"
  },
  {
    "objectID": "augmentation.html",
    "href": "augmentation.html",
    "title": "Data augmentation",
    "section": "",
    "text": "gingado provides data augmentation functionalities that can help users to augment their datasets with a time series dimension. This can be done both on a stand-alone basis as the user incorporates new data on top of the original dataset, or as part of a scikit-learn Pipeline that also includes other steps like data transformation and model estimation."
  },
  {
    "objectID": "augmentation.html#compatibility-with-scikit-learn",
    "href": "augmentation.html#compatibility-with-scikit-learn",
    "title": "Data augmentation",
    "section": "Compatibility with scikit-learn",
    "text": "Compatibility with scikit-learn\nAs mentioned above, gingado’s transformers are built to be compatible with scikit-learn. The code below demonstrates this compatibility.\nFirst, we create the example dataset. In this case, it comprises the daily foreign exchange rate of selected currencies to the Euro. The Brazilian Real (BRL) is chosen for this example as the dependent variable.\n\nfrom gingado.utils import load_SDMX_data, Lag\nfrom sklearn.model_selection import TimeSeriesSplit\n\n\nX = load_SDMX_data(\n    sources={'ECB': 'EXR'}, \n    keys={'FREQ': 'D', 'CURRENCY': ['EUR', 'AUD', 'BRL', 'CAD', 'CHF', 'GBP', 'JPY', 'SGD', 'USD']},\n    params={\"startPeriod\": 2003}\n    )\n# drop rows with empty values\nX.dropna(inplace=True)\n# adjust column names in this simple example for ease of understanding:\n# remove parts related to source and dataflow names\nX.columns = X.columns.str.replace(\"ECB__EXR_D__\", \"\").str.replace(\"__EUR__SP00__A\", \"\")\nX = Lag(lags=1, jump=0, keep_contemporaneous_X=True).fit_transform(X)\ny = X.pop('BRL')\n# retain only the lagged variables in the X variable\nX = X[X.columns[X.columns.str.contains('_lag_')]]\n\nQuerying data from ECB's dataflow 'EXR' - Exchange Rates...\n\n\n2023-06-18 11:29:19,756 pandasdmx.reader.sdmxml - INFO: Use supplied dsd=… argument for non–structure-specific message\n\n\n\nX_train, X_test = X.iloc[:-1], X.tail(1)\ny_train, y_test = y.iloc[:-1], y.tail(1)\n\nX_train.shape, y_train.shape, X_test.shape, y_test.shape\n\n((5239, 8), (5239,), (1, 8), (1,))\n\n\nNext, the data augmentation object provided by gingado adds more data. In this case, for brevity only one dataflow from one source is listed. If users want to add more SDMX sources, simply add more keys to the dictionary. And if users want data from all dataflows from a given source provided the keys and parameters such as frequency and dates match, the value should be set to 'all', as in {'ECB': ['CISS'], 'BIS': 'all'}.\n\ntest_src = {'ECB': ['CISS'], 'BIS': ['WS_CBPOL_D']}\n\nX_train__fit_transform = AugmentSDMX(sources=test_src).fit_transform(X=X_train)\nX_train__fit_then_transform = AugmentSDMX(sources=test_src).fit(X=X_train).transform(X=X_train, training=True)\n\nassert X_train__fit_transform.shape == X_train__fit_then_transform.shape\n\nQuerying data from ECB's dataflow 'CISS' - Composite Indicator of Systemic Stress...\nQuerying data from BIS's dataflow 'WS_CBPOL_D' - Policy rates daily...\nQuerying data from ECB's dataflow 'CISS' - Composite Indicator of Systemic Stress...\nQuerying data from BIS's dataflow 'WS_CBPOL_D' - Policy rates daily...\n\n\n2023-06-18 11:31:53,806 pandasdmx.reader.sdmxml - INFO: Use supplied dsd=… argument for non–structure-specific message\n2023-06-18 11:33:30,655 pandasdmx.reader.sdmxml - INFO: Use supplied dsd=… argument for non–structure-specific message\n\n\nThis is the dataset now after this particular augmentation:\n\nprint(f\"No of columns: {len(X_train__fit_transform.columns)} {X_train__fit_transform.columns}\")\nX_train__fit_transform\n\nNo of columns: 69 Index(['AUD_lag_1', 'BRL_lag_1', 'CAD_lag_1', 'CHF_lag_1', 'GBP_lag_1',\n       'JPY_lag_1', 'SGD_lag_1', 'USD_lag_1',\n       'ECB__CISS_D__AT__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__BE__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__CN__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__DE__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__ES__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__FI__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__FR__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__GB__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__IE__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__IT__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__NL__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__PT__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_BM__CON',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_CI__IDX',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_CO__CON',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_EM__CON',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_FI__CON',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_FX__CON',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_MM__CON',\n       'ECB__CISS_D__US__Z0Z__4F__EC__SS_CI__IDX',\n       'ECB__CISS_D__US__Z0Z__4F__EC__SS_CIN__IDX', 'BIS__WS_CBPOL_D_D__AR',\n       'BIS__WS_CBPOL_D_D__AU', 'BIS__WS_CBPOL_D_D__BR',\n       'BIS__WS_CBPOL_D_D__CA', 'BIS__WS_CBPOL_D_D__CH',\n       'BIS__WS_CBPOL_D_D__CL', 'BIS__WS_CBPOL_D_D__CN',\n       'BIS__WS_CBPOL_D_D__CO', 'BIS__WS_CBPOL_D_D__CZ',\n       'BIS__WS_CBPOL_D_D__DK', 'BIS__WS_CBPOL_D_D__GB',\n       'BIS__WS_CBPOL_D_D__HK', 'BIS__WS_CBPOL_D_D__HR',\n       'BIS__WS_CBPOL_D_D__HU', 'BIS__WS_CBPOL_D_D__ID',\n       'BIS__WS_CBPOL_D_D__IL', 'BIS__WS_CBPOL_D_D__IN',\n       'BIS__WS_CBPOL_D_D__IS', 'BIS__WS_CBPOL_D_D__JP',\n       'BIS__WS_CBPOL_D_D__KR', 'BIS__WS_CBPOL_D_D__MA',\n       'BIS__WS_CBPOL_D_D__MK', 'BIS__WS_CBPOL_D_D__MX',\n       'BIS__WS_CBPOL_D_D__MY', 'BIS__WS_CBPOL_D_D__NO',\n       'BIS__WS_CBPOL_D_D__NZ', 'BIS__WS_CBPOL_D_D__PE',\n       'BIS__WS_CBPOL_D_D__PH', 'BIS__WS_CBPOL_D_D__PL',\n       'BIS__WS_CBPOL_D_D__RO', 'BIS__WS_CBPOL_D_D__RS',\n       'BIS__WS_CBPOL_D_D__RU', 'BIS__WS_CBPOL_D_D__SA',\n       'BIS__WS_CBPOL_D_D__SE', 'BIS__WS_CBPOL_D_D__TH',\n       'BIS__WS_CBPOL_D_D__TR', 'BIS__WS_CBPOL_D_D__US',\n       'BIS__WS_CBPOL_D_D__XM', 'BIS__WS_CBPOL_D_D__ZA'],\n      dtype='object')\n\n\n\n\n\n\n\n\n\nAUD_lag_1\nBRL_lag_1\nCAD_lag_1\nCHF_lag_1\nGBP_lag_1\nJPY_lag_1\nSGD_lag_1\nUSD_lag_1\nECB__CISS_D__AT__Z0Z__4F__EC__SS_CIN__IDX\nECB__CISS_D__BE__Z0Z__4F__EC__SS_CIN__IDX\n...\nBIS__WS_CBPOL_D_D__RO\nBIS__WS_CBPOL_D_D__RS\nBIS__WS_CBPOL_D_D__RU\nBIS__WS_CBPOL_D_D__SA\nBIS__WS_CBPOL_D_D__SE\nBIS__WS_CBPOL_D_D__TH\nBIS__WS_CBPOL_D_D__TR\nBIS__WS_CBPOL_D_D__US\nBIS__WS_CBPOL_D_D__XM\nBIS__WS_CBPOL_D_D__ZA\n\n\nTIME_PERIOD\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2003-01-03\n1.8554\n3.6770\n1.6422\n1.4528\n0.65200\n124.40\n1.8188\n1.0446\n0.021899\n0.043292\n...\nNaN\n9.5\nNaN\nNaN\n3.75\n1.75\n44.0\n1.250\n2.75\n13.50\n\n\n2003-01-06\n1.8440\n3.6112\n1.6264\n1.4555\n0.65000\n124.56\n1.8132\n1.0392\n0.020801\n0.039924\n...\n19.75\n9.5\nNaN\n2.00\n3.75\n1.75\n44.0\n1.250\n2.75\n13.50\n\n\n2003-01-07\n1.8281\n3.5145\n1.6383\n1.4563\n0.64950\n124.40\n1.8210\n1.0488\n0.019738\n0.038084\n...\n19.75\n9.5\nNaN\n2.00\n3.75\n1.75\n44.0\n1.250\n2.75\n13.50\n\n\n2003-01-08\n1.8160\n3.5139\n1.6257\n1.4565\n0.64960\n124.82\n1.8155\n1.0425\n0.019947\n0.040338\n...\n19.75\n9.5\n21.0\n2.00\n3.75\n1.75\n44.0\n1.250\n2.75\n13.50\n\n\n2003-01-09\n1.8132\n3.4405\n1.6231\n1.4586\n0.64950\n124.90\n1.8102\n1.0377\n0.017026\n0.040535\n...\n19.75\n9.5\n21.0\n2.00\n3.75\n1.75\n44.0\n1.250\n2.75\n13.50\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-06-09\n1.6061\n5.2866\n1.4342\n0.9751\n0.86113\n149.98\n1.4460\n1.0737\n0.167185\n0.124553\n...\n7.00\n6.0\n7.5\n5.75\n3.50\n2.00\n8.5\n5.125\n3.75\n8.25\n\n\n2023-06-12\n1.6023\n5.2965\n1.4362\n0.9716\n0.85795\n150.24\n1.4480\n1.0780\n0.171139\n0.123640\n...\n7.00\n6.0\n7.5\n5.75\n3.50\n2.00\n8.5\n5.125\n3.75\n8.25\n\n\n2023-06-13\n1.5920\n5.2549\n1.4357\n0.9751\n0.85678\n150.03\n1.4457\n1.0765\n0.164665\n0.118228\n...\n7.00\n6.0\n7.5\n5.75\n3.50\n2.00\n8.5\n5.125\n3.75\n8.25\n\n\n2023-06-14\n1.5922\n5.2469\n1.4403\n0.9784\n0.85850\n150.62\n1.4467\n1.0793\n0.151799\n0.111484\n...\n7.00\n6.0\n7.5\n5.75\n3.50\n2.00\n8.5\n5.125\n3.75\n8.25\n\n\n2023-06-15\n1.5915\n5.2489\n1.4378\n0.9751\n0.85455\n151.21\n1.4499\n1.0809\n0.137266\n0.094035\n...\n7.00\n6.0\n7.5\n5.75\n3.50\n2.00\n8.5\n5.125\n3.75\n8.25\n\n\n\n\n5239 rows × 69 columns\n\n\n\n\nPipeline\nAugmentSDMX can also be part of a Pipeline object, which minimises operational errors during modelling and avoids using testing data during training:\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\n\n\npipeline = Pipeline([\n    ('augmentation', AugmentSDMX(sources={'BIS': 'WS_CBPOL_D'})),\n    ('imp', IterativeImputer(max_iter=10)),\n    ('forest', RandomForestRegressor())\n], verbose=True)\n\n\n\nTuning the data augmentation to enhance model performance\nAnd since AugmentSDMX can be included in a Pipeline, it can also be fine-tuned by parameter search techniques (such as grid search), further helping users make the best of available data to enhance performance of their models.\n\n\n\n\n\n\nTip\n\n\n\nUsers can cache the data augmentation step to avoid repeating potentially lengthy data downloads. See the memory argument in the sklearn.pipeline.Pipeline documentation.\n\n\n\ngrid = GridSearchCV(\n    estimator=pipeline,\n    param_grid={\n        'augmentation': ['passthrough', AugmentSDMX(sources={'ECB': 'CISS'})]\n    },\n    verbose=2,\n    cv=TimeSeriesSplit(n_splits=2)\n    )\n\ny_pred_grid = grid.fit(X_train, y_train).predict(X_test)\n\nFitting 2 folds for each of 2 candidates, totalling 4 fits\n[Pipeline] ...... (step 1 of 3) Processing augmentation, total=   0.0s\n[Pipeline] ............... (step 2 of 3) Processing imp, total=   0.0s\n[Pipeline] ............ (step 3 of 3) Processing forest, total=   0.7s\n[CV] END ...........................augmentation=passthrough; total time=   0.7s\n[Pipeline] ...... (step 1 of 3) Processing augmentation, total=   0.0s\n[Pipeline] ............... (step 2 of 3) Processing imp, total=   0.0s\n[Pipeline] ............ (step 3 of 3) Processing forest, total=   1.3s\n[CV] END ...........................augmentation=passthrough; total time=   1.3s\nQuerying data from ECB's dataflow 'CISS' - Composite Indicator of Systemic Stress...\n[Pipeline] ...... (step 1 of 3) Processing augmentation, total=   9.7s\n[Pipeline] ............... (step 2 of 3) Processing imp, total=   0.2s\n[Pipeline] ............ (step 3 of 3) Processing forest, total=   1.8s\nQuerying data from ECB's dataflow 'CISS' - Composite Indicator of Systemic Stress...\n[CV] END ..augmentation=AugmentSDMX(sources={'ECB': 'CISS'}); total time=  21.1s\nQuerying data from ECB's dataflow 'CISS' - Composite Indicator of Systemic Stress...\n[Pipeline] ...... (step 1 of 3) Processing augmentation, total=  18.3s\n[Pipeline] ............... (step 2 of 3) Processing imp, total=   0.4s\n[Pipeline] ............ (step 3 of 3) Processing forest, total=   4.4s\nQuerying data from ECB's dataflow 'CISS' - Composite Indicator of Systemic Stress...\n[CV] END ..augmentation=AugmentSDMX(sources={'ECB': 'CISS'}); total time=  41.7s\n[Pipeline] ...... (step 1 of 3) Processing augmentation, total=   0.0s\n[Pipeline] ............... (step 2 of 3) Processing imp, total=   0.0s\n[Pipeline] ............ (step 3 of 3) Processing forest, total=   2.0s\n\n\n2023-06-18 11:37:46,240 pandasdmx.reader.sdmxml - INFO: Use supplied dsd=… argument for non–structure-specific message\n2023-06-18 11:37:58,171 pandasdmx.reader.sdmxml - INFO: Use supplied dsd=… argument for non–structure-specific message\n2023-06-18 11:38:07,471 pandasdmx.reader.sdmxml - INFO: Use supplied dsd=… argument for non–structure-specific message\n2023-06-18 11:38:30,335 pandasdmx.reader.sdmxml - INFO: Use supplied dsd=… argument for non–structure-specific message\n\n\n\ngrid.best_params_\n\n{'augmentation': 'passthrough'}\n\n\n\nprint(f\"In this particular case, the best model was achieved by {'not ' if grid.best_params_['augmentation'] == 'passthrough' else ''}using the data augmentation.\")\n\nIn this particular case, the best model was achieved by not using the data augmentation.\n\n\n\nprint(f\"The last value in the training dataset was {y_train.tail(1).to_numpy()}. The predicted value was {y_pred_grid}, and the actual value was {y_test.to_numpy()}.\")\n\nThe last value in the training dataset was [5.2244]. The predicted value was [5.236705], and the actual value was [5.279]."
  },
  {
    "objectID": "dataset_transformation.html",
    "href": "dataset_transformation.html",
    "title": "Dataset transformation for uploading",
    "section": "",
    "text": "For more information on this data, consult the datasets page.\n\nimport pandas as pd\nfrom scipy import io\n\ngrowth_data = io.loadmat('data/GrowthData.mat')\ncolnames = [m[0].strip() for m in growth_data['Mnem'][0]]\ndf = pd.DataFrame(growth_data['data'], columns=colnames)\n\ndf.to_csv('gingado/dataset_BarroLee_1994.csv')\n\n\npd.set_option('display.max_rows', None)\ndf.describe().T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\ngdpsh465\n90.0\n7.702907\n0.896179\n5.762051\n7.131539\n7.725700\n8.441914\n9.229849\n\n\nbmp1l\n90.0\n0.168747\n0.249116\n0.000000\n0.000000\n0.063800\n0.274550\n1.637800\n\n\nfreeop\n90.0\n0.220102\n0.074861\n0.078488\n0.166044\n0.203972\n0.286425\n0.416234\n\n\nfreetar\n90.0\n0.028334\n0.021855\n0.000000\n0.011589\n0.025426\n0.039745\n0.109921\n\n\nh65\n90.0\n0.111556\n0.101361\n0.002000\n0.032250\n0.089000\n0.147500\n0.573000\n\n\nhm65\n90.0\n0.137156\n0.116826\n0.004000\n0.042250\n0.114500\n0.181000\n0.635000\n\n\nhf65\n90.0\n0.082233\n0.091549\n0.000000\n0.014000\n0.055000\n0.113000\n0.527000\n\n\np65\n90.0\n0.893333\n0.164938\n0.290000\n0.832500\n0.985000\n1.000000\n1.000000\n\n\npm65\n90.0\n0.919556\n0.134632\n0.370000\n0.882500\n1.000000\n1.000000\n1.000000\n\n\npf65\n90.0\n0.849556\n0.210899\n0.210000\n0.762500\n0.970000\n1.000000\n1.000000\n\n\ns65\n90.0\n0.406556\n0.247219\n0.020000\n0.182500\n0.395000\n0.555000\n0.910000\n\n\nsm65\n90.0\n0.436222\n0.246543\n0.030000\n0.220000\n0.420000\n0.607500\n0.950000\n\n\nsf65\n90.0\n0.379778\n0.265800\n0.010000\n0.140000\n0.345000\n0.530000\n0.920000\n\n\nfert65\n90.0\n4.742000\n1.974886\n1.450000\n2.845000\n5.060000\n6.560000\n8.000000\n\n\nmort65\n90.0\n0.070467\n0.047166\n0.009000\n0.024000\n0.064500\n0.110750\n0.183000\n\n\nlifee065\n90.0\n4.102025\n0.167895\n3.693867\n3.994061\n4.110874\n4.258443\n4.317488\n\n\ngpop1\n90.0\n0.021301\n0.009851\n0.002600\n0.013150\n0.022900\n0.029900\n0.039000\n\n\nfert1\n90.0\n4.962444\n1.936886\n1.742000\n2.906500\n5.394000\n6.680000\n8.000000\n\n\nmort1\n90.0\n0.087211\n0.051937\n0.015000\n0.031500\n0.085000\n0.131000\n0.204000\n\n\ninvsh41\n90.0\n0.196663\n0.086981\n0.027133\n0.129175\n0.191680\n0.254760\n0.475500\n\n\ngeetot1\n90.0\n0.035594\n0.014543\n0.011700\n0.024325\n0.033900\n0.046400\n0.077000\n\n\ngeerec1\n90.0\n0.029782\n0.012673\n0.004100\n0.020925\n0.028350\n0.038250\n0.068700\n\n\ngde1\n90.0\n0.032133\n0.033750\n0.005000\n0.015250\n0.020500\n0.039750\n0.251000\n\n\ngovwb1\n90.0\n0.126661\n0.045588\n0.061500\n0.096000\n0.120950\n0.145125\n0.352300\n\n\ngovsh41\n90.0\n0.155161\n0.061317\n0.035500\n0.118350\n0.147250\n0.183850\n0.385900\n\n\ngvxdxe41\n90.0\n0.094915\n0.053846\n0.010000\n0.058965\n0.086585\n0.122365\n0.310460\n\n\nhigh65\n90.0\n4.203000\n5.252551\n0.120000\n1.302500\n2.735000\n4.952500\n30.900000\n\n\nhighm65\n90.0\n5.531556\n5.908117\n0.230000\n1.890000\n3.990000\n6.525000\n33.400000\n\n\nhighf65\n90.0\n2.946889\n4.781800\n0.010000\n0.667500\n1.320000\n2.992500\n28.500000\n\n\nhighc65\n90.0\n2.455556\n2.487163\n0.090000\n1.007500\n1.695000\n3.135000\n15.630000\n\n\nhighcm65\n90.0\n3.434222\n3.111632\n0.180000\n1.347500\n2.445000\n4.517500\n18.830000\n\n\nhighcf65\n90.0\n1.529667\n2.066383\n0.010000\n0.432500\n0.765000\n1.925000\n12.770000\n\n\nhuman65\n90.0\n4.214878\n2.530420\n0.301000\n2.212000\n3.803500\n5.674000\n11.158000\n\n\nhumanm65\n90.0\n4.698267\n2.423311\n0.568000\n2.888000\n4.244500\n6.030250\n11.535000\n\n\nhumanf65\n90.0\n3.745967\n2.692534\n0.043000\n1.605250\n3.194000\n5.178250\n10.798000\n\n\nhyr65\n90.0\n0.133178\n0.151704\n0.004000\n0.046500\n0.087000\n0.161500\n0.829000\n\n\nhyrm65\n90.0\n0.179300\n0.177153\n0.008000\n0.066250\n0.125500\n0.215250\n0.960000\n\n\nhyrf65\n90.0\n0.089533\n0.133602\n0.000000\n0.021250\n0.043500\n0.095000\n0.712000\n\n\nno65\n90.0\n34.723111\n29.101051\n0.000000\n4.527500\n31.110000\n59.075000\n89.460000\n\n\nnom65\n90.0\n28.999889\n25.307305\n0.000000\n4.227500\n25.180000\n49.545000\n82.350000\n\n\nnof65\n90.0\n40.327111\n33.446850\n0.000000\n4.275000\n36.400000\n69.550000\n98.610000\n\n\npinstab1\n90.0\n0.114160\n0.214415\n0.000000\n0.000000\n0.000706\n0.103919\n1.068500\n\n\npop65\n90.0\n39825.300000\n87794.312844\n1482.000000\n4961.000000\n12275.000000\n42263.500000\n620701.000000\n\n\nworker65\n90.0\n0.369268\n0.069487\n0.215600\n0.311050\n0.369400\n0.410075\n0.526000\n\n\npop1565\n90.0\n0.375158\n0.093359\n0.201800\n0.280600\n0.421100\n0.456325\n0.515800\n\n\npop6565\n90.0\n0.059175\n0.037660\n0.021548\n0.031189\n0.036967\n0.086604\n0.151105\n\n\nsec65\n90.0\n15.318000\n13.039270\n0.450000\n5.700000\n11.200000\n18.865000\n57.100000\n\n\nsecm65\n90.0\n16.622111\n12.421347\n0.750000\n7.730000\n12.780000\n22.307500\n56.370000\n\n\nsecf65\n90.0\n14.024111\n14.121763\n0.170000\n3.747500\n8.865000\n16.840000\n57.800000\n\n\nsecc65\n90.0\n6.444111\n6.351368\n0.130000\n2.352500\n4.005000\n7.870000\n34.090000\n\n\nseccm65\n90.0\n6.702889\n6.182263\n0.150000\n2.635000\n4.735000\n8.910000\n31.270000\n\n\nseccf65\n90.0\n6.171778\n6.857261\n0.040000\n1.500000\n3.870000\n8.042500\n36.610000\n\n\nsyr65\n90.0\n0.912422\n0.823222\n0.033000\n0.357750\n0.678500\n1.130750\n4.211000\n\n\nsyrm65\n90.0\n1.045444\n0.831756\n0.057000\n0.467500\n0.770500\n1.246750\n4.227000\n\n\nsyrf65\n90.0\n0.783767\n0.839486\n0.010000\n0.215250\n0.512500\n1.002000\n4.198000\n\n\nteapri65\n90.0\n33.203333\n9.818516\n18.200000\n27.425000\n32.200000\n37.475000\n62.400000\n\n\nteasec65\n90.0\n19.412222\n6.384194\n7.200000\n15.350000\n18.400000\n22.800000\n37.100000\n\n\nex1\n90.0\n0.133981\n0.118708\n0.017800\n0.063450\n0.092300\n0.169225\n0.747000\n\n\nim1\n90.0\n0.144356\n0.120937\n0.022200\n0.070625\n0.116300\n0.181475\n0.848900\n\n\nxr65\n90.0\n42.663856\n119.335089\n0.003000\n1.099000\n4.762000\n19.619000\n652.850000\n\n\ntot1\n90.0\n0.009236\n0.059182\n-0.156878\n-0.016877\n0.004890\n0.018526\n0.207492\n\n\nOutcome\n90.0\n0.045349\n0.051314\n-0.100990\n0.021045\n0.046209\n0.074029\n0.185526"
  },
  {
    "objectID": "dataset_transformation.html#barro-and-lee-1994",
    "href": "dataset_transformation.html#barro-and-lee-1994",
    "title": "Dataset transformation for uploading",
    "section": "",
    "text": "For more information on this data, consult the datasets page.\n\nimport pandas as pd\nfrom scipy import io\n\ngrowth_data = io.loadmat('data/GrowthData.mat')\ncolnames = [m[0].strip() for m in growth_data['Mnem'][0]]\ndf = pd.DataFrame(growth_data['data'], columns=colnames)\n\ndf.to_csv('gingado/dataset_BarroLee_1994.csv')\n\n\npd.set_option('display.max_rows', None)\ndf.describe().T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\ngdpsh465\n90.0\n7.702907\n0.896179\n5.762051\n7.131539\n7.725700\n8.441914\n9.229849\n\n\nbmp1l\n90.0\n0.168747\n0.249116\n0.000000\n0.000000\n0.063800\n0.274550\n1.637800\n\n\nfreeop\n90.0\n0.220102\n0.074861\n0.078488\n0.166044\n0.203972\n0.286425\n0.416234\n\n\nfreetar\n90.0\n0.028334\n0.021855\n0.000000\n0.011589\n0.025426\n0.039745\n0.109921\n\n\nh65\n90.0\n0.111556\n0.101361\n0.002000\n0.032250\n0.089000\n0.147500\n0.573000\n\n\nhm65\n90.0\n0.137156\n0.116826\n0.004000\n0.042250\n0.114500\n0.181000\n0.635000\n\n\nhf65\n90.0\n0.082233\n0.091549\n0.000000\n0.014000\n0.055000\n0.113000\n0.527000\n\n\np65\n90.0\n0.893333\n0.164938\n0.290000\n0.832500\n0.985000\n1.000000\n1.000000\n\n\npm65\n90.0\n0.919556\n0.134632\n0.370000\n0.882500\n1.000000\n1.000000\n1.000000\n\n\npf65\n90.0\n0.849556\n0.210899\n0.210000\n0.762500\n0.970000\n1.000000\n1.000000\n\n\ns65\n90.0\n0.406556\n0.247219\n0.020000\n0.182500\n0.395000\n0.555000\n0.910000\n\n\nsm65\n90.0\n0.436222\n0.246543\n0.030000\n0.220000\n0.420000\n0.607500\n0.950000\n\n\nsf65\n90.0\n0.379778\n0.265800\n0.010000\n0.140000\n0.345000\n0.530000\n0.920000\n\n\nfert65\n90.0\n4.742000\n1.974886\n1.450000\n2.845000\n5.060000\n6.560000\n8.000000\n\n\nmort65\n90.0\n0.070467\n0.047166\n0.009000\n0.024000\n0.064500\n0.110750\n0.183000\n\n\nlifee065\n90.0\n4.102025\n0.167895\n3.693867\n3.994061\n4.110874\n4.258443\n4.317488\n\n\ngpop1\n90.0\n0.021301\n0.009851\n0.002600\n0.013150\n0.022900\n0.029900\n0.039000\n\n\nfert1\n90.0\n4.962444\n1.936886\n1.742000\n2.906500\n5.394000\n6.680000\n8.000000\n\n\nmort1\n90.0\n0.087211\n0.051937\n0.015000\n0.031500\n0.085000\n0.131000\n0.204000\n\n\ninvsh41\n90.0\n0.196663\n0.086981\n0.027133\n0.129175\n0.191680\n0.254760\n0.475500\n\n\ngeetot1\n90.0\n0.035594\n0.014543\n0.011700\n0.024325\n0.033900\n0.046400\n0.077000\n\n\ngeerec1\n90.0\n0.029782\n0.012673\n0.004100\n0.020925\n0.028350\n0.038250\n0.068700\n\n\ngde1\n90.0\n0.032133\n0.033750\n0.005000\n0.015250\n0.020500\n0.039750\n0.251000\n\n\ngovwb1\n90.0\n0.126661\n0.045588\n0.061500\n0.096000\n0.120950\n0.145125\n0.352300\n\n\ngovsh41\n90.0\n0.155161\n0.061317\n0.035500\n0.118350\n0.147250\n0.183850\n0.385900\n\n\ngvxdxe41\n90.0\n0.094915\n0.053846\n0.010000\n0.058965\n0.086585\n0.122365\n0.310460\n\n\nhigh65\n90.0\n4.203000\n5.252551\n0.120000\n1.302500\n2.735000\n4.952500\n30.900000\n\n\nhighm65\n90.0\n5.531556\n5.908117\n0.230000\n1.890000\n3.990000\n6.525000\n33.400000\n\n\nhighf65\n90.0\n2.946889\n4.781800\n0.010000\n0.667500\n1.320000\n2.992500\n28.500000\n\n\nhighc65\n90.0\n2.455556\n2.487163\n0.090000\n1.007500\n1.695000\n3.135000\n15.630000\n\n\nhighcm65\n90.0\n3.434222\n3.111632\n0.180000\n1.347500\n2.445000\n4.517500\n18.830000\n\n\nhighcf65\n90.0\n1.529667\n2.066383\n0.010000\n0.432500\n0.765000\n1.925000\n12.770000\n\n\nhuman65\n90.0\n4.214878\n2.530420\n0.301000\n2.212000\n3.803500\n5.674000\n11.158000\n\n\nhumanm65\n90.0\n4.698267\n2.423311\n0.568000\n2.888000\n4.244500\n6.030250\n11.535000\n\n\nhumanf65\n90.0\n3.745967\n2.692534\n0.043000\n1.605250\n3.194000\n5.178250\n10.798000\n\n\nhyr65\n90.0\n0.133178\n0.151704\n0.004000\n0.046500\n0.087000\n0.161500\n0.829000\n\n\nhyrm65\n90.0\n0.179300\n0.177153\n0.008000\n0.066250\n0.125500\n0.215250\n0.960000\n\n\nhyrf65\n90.0\n0.089533\n0.133602\n0.000000\n0.021250\n0.043500\n0.095000\n0.712000\n\n\nno65\n90.0\n34.723111\n29.101051\n0.000000\n4.527500\n31.110000\n59.075000\n89.460000\n\n\nnom65\n90.0\n28.999889\n25.307305\n0.000000\n4.227500\n25.180000\n49.545000\n82.350000\n\n\nnof65\n90.0\n40.327111\n33.446850\n0.000000\n4.275000\n36.400000\n69.550000\n98.610000\n\n\npinstab1\n90.0\n0.114160\n0.214415\n0.000000\n0.000000\n0.000706\n0.103919\n1.068500\n\n\npop65\n90.0\n39825.300000\n87794.312844\n1482.000000\n4961.000000\n12275.000000\n42263.500000\n620701.000000\n\n\nworker65\n90.0\n0.369268\n0.069487\n0.215600\n0.311050\n0.369400\n0.410075\n0.526000\n\n\npop1565\n90.0\n0.375158\n0.093359\n0.201800\n0.280600\n0.421100\n0.456325\n0.515800\n\n\npop6565\n90.0\n0.059175\n0.037660\n0.021548\n0.031189\n0.036967\n0.086604\n0.151105\n\n\nsec65\n90.0\n15.318000\n13.039270\n0.450000\n5.700000\n11.200000\n18.865000\n57.100000\n\n\nsecm65\n90.0\n16.622111\n12.421347\n0.750000\n7.730000\n12.780000\n22.307500\n56.370000\n\n\nsecf65\n90.0\n14.024111\n14.121763\n0.170000\n3.747500\n8.865000\n16.840000\n57.800000\n\n\nsecc65\n90.0\n6.444111\n6.351368\n0.130000\n2.352500\n4.005000\n7.870000\n34.090000\n\n\nseccm65\n90.0\n6.702889\n6.182263\n0.150000\n2.635000\n4.735000\n8.910000\n31.270000\n\n\nseccf65\n90.0\n6.171778\n6.857261\n0.040000\n1.500000\n3.870000\n8.042500\n36.610000\n\n\nsyr65\n90.0\n0.912422\n0.823222\n0.033000\n0.357750\n0.678500\n1.130750\n4.211000\n\n\nsyrm65\n90.0\n1.045444\n0.831756\n0.057000\n0.467500\n0.770500\n1.246750\n4.227000\n\n\nsyrf65\n90.0\n0.783767\n0.839486\n0.010000\n0.215250\n0.512500\n1.002000\n4.198000\n\n\nteapri65\n90.0\n33.203333\n9.818516\n18.200000\n27.425000\n32.200000\n37.475000\n62.400000\n\n\nteasec65\n90.0\n19.412222\n6.384194\n7.200000\n15.350000\n18.400000\n22.800000\n37.100000\n\n\nex1\n90.0\n0.133981\n0.118708\n0.017800\n0.063450\n0.092300\n0.169225\n0.747000\n\n\nim1\n90.0\n0.144356\n0.120937\n0.022200\n0.070625\n0.116300\n0.181475\n0.848900\n\n\nxr65\n90.0\n42.663856\n119.335089\n0.003000\n1.099000\n4.762000\n19.619000\n652.850000\n\n\ntot1\n90.0\n0.009236\n0.059182\n-0.156878\n-0.016877\n0.004890\n0.018526\n0.207492\n\n\nOutcome\n90.0\n0.045349\n0.051314\n-0.100990\n0.021045\n0.046209\n0.074029\n0.185526"
  },
  {
    "objectID": "benchmark.html",
    "href": "benchmark.html",
    "title": "Automatic benchmark model",
    "section": "",
    "text": "A Benchmark object has a similar API to a sciki-learn estimator: you build an instance with the desired arguments, and fit it to the data at a later moment. Benchmarks is a convenience wrapper for reading the training data, passing it through a simplified pipeline consisting of data imputation and a standard scalar, and then the benchmark function calibrated with a grid search.\nA gingado Benchmark object seeks to automatise a significant part of creating a benchmark model. Importantly, the Benchmark object also has a compare method that helps users evaluate if candidate models are better than the benchmark, and if one of them is, it becomes the new benchmark. This compare method takes as argument another fitted estimator (which could be itself a solo estimator or a whole pipeline) or a list of fitted estimators.\nBenchmarks start with default values that should perform reasonably well in most settings, but the user is also free to choose any of the benchmark’s components by passing as arguments the data split, pipeline, and/or a dictionary of parameters for the hyperparameter tuning."
  },
  {
    "objectID": "benchmark.html#scoring",
    "href": "benchmark.html#scoring",
    "title": "Automatic benchmark model",
    "section": "Scoring",
    "text": "Scoring\nClassificationBenchmark and RegressionBenchmark use the default scoring method for comparing model alternatives, both during estimation of the benchmark model and when comparing this benchmark with candidate models. Users are encouraged to consider if another scoring method is more suitable for their use case. More information on available scoring methods that are compatible with gingado Benchmark objects can be found here."
  },
  {
    "objectID": "benchmark.html#data-split",
    "href": "benchmark.html#data-split",
    "title": "Automatic benchmark model",
    "section": "Data split",
    "text": "Data split\ngingado benchmarks rely on hyperparameter tuning to discover the benchmark specification that is most likely to perform better with the user data. This tuning in turn depends on a data splitting strategy for the cross-validation. By default, gingado uses StratifiedShuffleSplit (in classification problems) or ShuffleSplit (in regression problems) if the data is not time series and TimeSeriesSplit otherwise.\nThe user may overrun these defaults either by directly setting the parameter cv or default_cv when instanciating the gingado benchmark class. The difference is that default_cv is only used after gingado checks that the data is not a time series (if a time dimension exists, then TimeSeriesSplit is used).\n\nX, y = make_classification()\nbm_cls = ClassificationBenchmark(cv=TimeSeriesSplit(n_splits=3)).fit(X, y)\nassert bm_cls.benchmark.n_splits_ == 3\n\nX, y = make_regression()\nbm_reg = RegressionBenchmark(default_cv=ShuffleSplit(n_splits=7)).fit(X, y)\nassert bm_reg.benchmark.n_splits_ == 7\n\nPlease refer to this page for more information on the different Splitter classes available on scikit-learn, and this page for practical advice on how to choose a splitter for data that are not time series. Any one of these objects (or a custom splitter that is compatible with them) can be passed to a Benchmark object.\nUsers that wish to use specific parameters should include the actual Splitter object as the parameter, as done with the n_splits parameter in the chunk above."
  },
  {
    "objectID": "benchmark.html#custom-benchmarks",
    "href": "benchmark.html#custom-benchmarks",
    "title": "Automatic benchmark model",
    "section": "Custom benchmarks",
    "text": "Custom benchmarks\ngingado provides users with two Benchmark objects out of the box: ClassificationBenchmark and RegressionBenchmark, to be used depending on the task at hand. Both classes derive from a base class ggdBenchmark, which implements methods that facilitate model comparison. Users that want to create a customised benchmark model for themselves have two options:\n\nthe simpler possibility is to train the estimator as usual, and then assign the fitted estimator to a Benchmark object.\nif the user wants more control over the fitting process of estimating the benchmark, they can create a class that subclasses from ggdBenchmark and either implements custom fit, predict and score methods, or also subclasses from scikit-learn’s BaseEstimator.\n\nIn any case, if the user wants the benchmark to automatically detect if the data is a time series and also to document the model right after fitting, the fit method should call self._fit on the data. Otherwise, the user can simply implement any consistent logic in fit as the user sees fit (pun intended)."
  },
  {
    "objectID": "barrolee1994.html",
    "href": "barrolee1994.html",
    "title": "Using gingado to understand economic growth",
    "section": "",
    "text": "This notebook showcases one possible use of gingado by estimating economic growth across countries, using the dataset studied by Barro and Lee (1994). You can run this notebook interactively, by clicking on the appropriate link above.\nThis dataset has been widely studied in economics. Belloni, Chernozhukov, and Hansen (2011) and Giannone, Lenza, and Primiceri (2021) are two studies of this dataset that are most related to machine learning.\nThis notebook will use gingado to compare quickly setup a well-performing machine learning model and use its results as evidence to support the conditional convergence hypothesis; compare different classes of models (and their combination in a single model), and use and document the best performing alternative.\nBecause the notebook is for pedagogical purposes only, please bear in mind some aspects of the machine learning workflow (such as carefully thinking about the cross-validation strategy) are glossed over in this notebook. Also, only the key academic references are cited; more references can be found in the papers mentioned in this example.\nFor a more thorough description of gingado, please refer to the package’s website and to the academic material about it."
  },
  {
    "objectID": "barrolee1994.html#setting-the-stage",
    "href": "barrolee1994.html#setting-the-stage",
    "title": "Using gingado to understand economic growth",
    "section": "Setting the stage",
    "text": "Setting the stage\nWe will import packages as the work progresses. This will help highlight the specific steps in the workflow that gingado can be helpful with.\n\n\nCode\nimport pandas as pd\n\n\nThe data is available in the online annex to Giannone, Lenza, and Primiceri (2021). In that paper, this dataset corresponds to what the authors call “macro2”. The original data, along with more information on the variables, can be found in this NBER website. A very helpful codebook is found in this repo.\n\n\nCode\nfrom gingado.datasets import load_BarroLee_1994\n\nX, y = load_BarroLee_1994()\n\n\nThe dataset contains explanatory variables representing per-capita growth between 1960 and 1985, for 90 countries.\n\n\nCode\nX.columns\n\n\nIndex(['Unnamed: 0', 'gdpsh465', 'bmp1l', 'freeop', 'freetar', 'h65', 'hm65',\n       'hf65', 'p65', 'pm65', 'pf65', 's65', 'sm65', 'sf65', 'fert65',\n       'mort65', 'lifee065', 'gpop1', 'fert1', 'mort1', 'invsh41', 'geetot1',\n       'geerec1', 'gde1', 'govwb1', 'govsh41', 'gvxdxe41', 'high65', 'highm65',\n       'highf65', 'highc65', 'highcm65', 'highcf65', 'human65', 'humanm65',\n       'humanf65', 'hyr65', 'hyrm65', 'hyrf65', 'no65', 'nom65', 'nof65',\n       'pinstab1', 'pop65', 'worker65', 'pop1565', 'pop6565', 'sec65',\n       'secm65', 'secf65', 'secc65', 'seccm65', 'seccf65', 'syr65', 'syrm65',\n       'syrf65', 'teapri65', 'teasec65', 'ex1', 'im1', 'xr65', 'tot1'],\n      dtype='object')\n\n\n\n\nCode\nX.head().T\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\nUnnamed: 0\n0.000000\n1.000000\n2.000000\n3.000000\n4.000000\n\n\ngdpsh465\n6.591674\n6.829794\n8.895082\n7.565275\n7.162397\n\n\nbmp1l\n0.283700\n0.614100\n0.000000\n0.199700\n0.174000\n\n\nfreeop\n0.153491\n0.313509\n0.204244\n0.248714\n0.299252\n\n\nfreetar\n0.043888\n0.061827\n0.009186\n0.036270\n0.037367\n\n\n...\n...\n...\n...\n...\n...\n\n\nteasec65\n17.300000\n18.000000\n20.700000\n22.700000\n17.600000\n\n\nex1\n0.072900\n0.094000\n0.174100\n0.126500\n0.121100\n\n\nim1\n0.066700\n0.143800\n0.175000\n0.149600\n0.130800\n\n\nxr65\n0.348000\n0.525000\n1.082000\n6.625000\n2.500000\n\n\ntot1\n-0.014727\n0.005750\n-0.010040\n-0.002195\n0.003283\n\n\n\n\n62 rows × 5 columns\n\n\n\nThe outcome variable is represented here:\n\n\nCode\ny.plot.hist(bins=90, title='GDP growth')\n\n\n&lt;AxesSubplot:title={'center':'GDP growth'}, ylabel='Frequency'&gt;"
  },
  {
    "objectID": "barrolee1994.html#establishing-a-benchmark-model",
    "href": "barrolee1994.html#establishing-a-benchmark-model",
    "title": "Using gingado to understand economic growth",
    "section": "Establishing a benchmark model",
    "text": "Establishing a benchmark model\nGenerally speaking, it is a good idea to establish a benchmark model at the first stages of development of the machine learning model. gingado offers a class of automatic benchmarks that can be used off-the-shelf depending on the task at hand: RegressionBenchmark and ClassificationBenchmark. It is also good to keep in mind that more advanced users can create their own benchmark on top of a base class provided by gingado: ggdBenchmark.\nFor this application, since we are interested in running a regression task, we will use RegressionBenchmark:\n\n\nCode\nfrom gingado.benchmark import RegressionBenchmark\n\n\nWhat this object does is the following:\n\nit creates a random forest\nthree different versions of the random forest are trained on the user data\nthe version that performs better is chosen as the benchmark\nright after it is trained, the benchmark is documented using gingado’s ModelCard documenter.\n\nThe user can easily change the parameters above. For example, instead of a random forest the user might prefer a neural network as the benchmark. Or, in lieu of the default parameters provided by gingado, users might have their own idea of what could be a reasonable parameter space to search.\nRandom forests are chosen as the go-to benchmark algorithm because of their reasonably good performance in a wide variety of settings, the fact that they don’t require much data transformation (ie, normalising the data to have zero mean and one standard deviation), and by virtue of their relatively transparency about the importance of each regressor.\nThe first step is to initialise the benchmark object. At this time, we pass some arguments about how we want it to behave. In this case, we set the verbosity level to produce output related to each alternative considered. Then we fit it to the data.\n\n\nCode\n#####\n#####\nfrom sklearn.ensemble import RandomForestRegressor\nrfr = RandomForestRegressor()\nrfr.fit(X, y)\n\n\nRandomForestRegressor()\n\n\n\n\nCode\nbenchmark = RegressionBenchmark(verbose_grid=2)\nbenchmark.fit(X, y)\n\n\nFitting 5 folds for each of 6 candidates, totalling 30 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.5s\n\n\nRegressionBenchmark(cv=StratifiedShuffleSplit(n_splits=10, random_state=None, test_size=None,\n            train_size=None),\n                    verbose_grid=2)\n\n\nAs we can see above, with a few lines we have trained a random forest on the dataset. In this case, the benchmark was the better of six versions of the random forest, according to the default hyperparameters: 100 and 250 estimators were alternated with models for which the maximum number of regressors analysed by individual trees changesd fom the maximum, a square root and a log of the number of regressors. They were each trained using a 5-fold cross-validation.\nLet’s see which one was the best performing in this case, and hence our benchmark model:\n\n\nCode\npd.DataFrame(benchmark.benchmark.cv_results_).T\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n\n\n\n\nmean_fit_time\n0.127842\n0.267661\n0.106398\n0.268515\n0.163344\n0.43746\n\n\nstd_fit_time\n0.011492\n0.015168\n0.004224\n0.01286\n0.009028\n0.006711\n\n\nmean_score_time\n0.007325\n0.013783\n0.005755\n0.014542\n0.005953\n0.015245\n\n\nstd_score_time\n0.001457\n0.000516\n0.000381\n0.001177\n0.000455\n0.001707\n\n\nparam_max_features\nsqrt\nsqrt\nlog2\nlog2\nNone\nNone\n\n\nparam_n_estimators\n100\n250\n100\n250\n100\n250\n\n\nparams\n{'max_features': 'sqrt', 'n_estimators': 100}\n{'max_features': 'sqrt', 'n_estimators': 250}\n{'max_features': 'log2', 'n_estimators': 100}\n{'max_features': 'log2', 'n_estimators': 250}\n{'max_features': None, 'n_estimators': 100}\n{'max_features': None, 'n_estimators': 250}\n\n\nsplit0_test_score\n-0.16933\n-0.092086\n-0.122926\n-0.211516\n-0.108522\n-0.160882\n\n\nsplit1_test_score\n-0.295742\n-0.277869\n-0.334904\n-0.267957\n-0.353692\n-0.329935\n\n\nsplit2_test_score\n0.204602\n0.085291\n-0.037415\n0.082589\n0.404614\n0.411464\n\n\nsplit3_test_score\n-0.728921\n-0.573613\n-0.623305\n-0.757371\n-0.471002\n-0.460872\n\n\nsplit4_test_score\n0.137501\n0.126744\n0.01719\n0.114779\n0.125886\n0.098748\n\n\nmean_test_score\n-0.170378\n-0.146307\n-0.220272\n-0.207895\n-0.080543\n-0.088295\n\n\nstd_test_score\n0.335585\n0.257306\n0.234468\n0.314339\n0.31807\n0.312159\n\n\nrank_test_score\n4\n3\n6\n5\n1\n2\n\n\n\n\n\n\n\nThe values above are calculated with \\(R^2\\), the default scoring function for a random forest from the scikit-learn package. Suppose that instead we would like a benchmark model that is optimised on the maximum error, ie a benchmark that minimises the worst deviation from prediction to ground truth for all the sample. These are the steps that we would take. Note that a more complete list of ready-made scoring parameters and how to create your own function can be found here.\n\n\nCode\nbenchmark_lower_worsterror = RegressionBenchmark(scoring='max_error', verbose_grid=2)\nbenchmark_lower_worsterror.fit(X, y)\n\n\nFitting 5 folds for each of 6 candidates, totalling 30 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n\n\nRegressionBenchmark(cv=StratifiedShuffleSplit(n_splits=10, random_state=None, test_size=None,\n            train_size=None),\n                    scoring='max_error', verbose_grid=2)\n\n\n\n\nCode\npd.DataFrame(benchmark_lower_worsterror.benchmark.cv_results_).T\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n\n\n\n\nmean_fit_time\n0.125901\n0.284121\n0.116043\n0.25507\n0.16534\n0.446526\n\n\nstd_fit_time\n0.010974\n0.013291\n0.003123\n0.002374\n0.006095\n0.029097\n\n\nmean_score_time\n0.006425\n0.013692\n0.006476\n0.013916\n0.005643\n0.015142\n\n\nstd_score_time\n0.000569\n0.000061\n0.000596\n0.00071\n0.000369\n0.001178\n\n\nparam_max_features\nsqrt\nsqrt\nlog2\nlog2\nNone\nNone\n\n\nparam_n_estimators\n100\n250\n100\n250\n100\n250\n\n\nparams\n{'max_features': 'sqrt', 'n_estimators': 100}\n{'max_features': 'sqrt', 'n_estimators': 250}\n{'max_features': 'log2', 'n_estimators': 100}\n{'max_features': 'log2', 'n_estimators': 250}\n{'max_features': None, 'n_estimators': 100}\n{'max_features': None, 'n_estimators': 250}\n\n\nsplit0_test_score\n-0.091383\n-0.09865\n-0.089205\n-0.089668\n-0.099687\n-0.100254\n\n\nsplit1_test_score\n-0.13291\n-0.131386\n-0.126901\n-0.131514\n-0.168115\n-0.166914\n\n\nsplit2_test_score\n-0.105365\n-0.103247\n-0.107508\n-0.110758\n-0.087416\n-0.089252\n\n\nsplit3_test_score\n-0.156452\n-0.154446\n-0.155642\n-0.158177\n-0.154062\n-0.154419\n\n\nsplit4_test_score\n-0.12944\n-0.122188\n-0.121306\n-0.128108\n-0.137235\n-0.123249\n\n\nmean_test_score\n-0.12311\n-0.121983\n-0.120112\n-0.123645\n-0.129303\n-0.126818\n\n\nstd_test_score\n0.022668\n0.020188\n0.022018\n0.022781\n0.031029\n0.029997\n\n\nrank_test_score\n3\n2\n1\n4\n6\n5\n\n\n\n\n\n\n\nNow we even have two benchmark models.\nWe could further tweak and adjust them, but one of the ideas behind having a benchmark is that it is simple and easy to set up.\nLet’s retain only the first benchmark, for simplicity, and now look at the predictions, comparing them to the original growth values.\n\n\nCode\ny_pred = benchmark.predict(X)\n\npd.DataFrame({\n    'y': y,\n    'y_pred': y_pred\n    }).plot.scatter(\n        x='y', y='y_pred',\n         grid=True, \n         title='Actual and predicted outcome',\n         xlabel='actual GDP growth',\n         ylabel='predicted GDP growth')\n\n\n/Users/douglasaraujo/Coding/.venv_gingado/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but RandomForestRegressor was fitted without feature names\n  warnings.warn(\n\n\n&lt;AxesSubplot:title={'center':'Actual and predicted outcome'}, xlabel='actual GDP growth', ylabel='predicted GDP growth'&gt;\n\n\n\n\n\nAnd now a histogram of the benchmark’s errors:\n\n\nCode\npd.DataFrame(y - y_pred).plot.hist(bins=30, title='Residual')\n\n\n&lt;AxesSubplot:title={'center':'Residual'}, ylabel='Frequency'&gt;\n\n\n\n\n\nSince the benchmark is a random forest model, we can see what are the most important regressors, measured as the average reduction in impurity across the trees in the random forest that actually use that particular regressor. They are scaled so that the sum for all features is one. Higher importance amounts indicate that that particular regressor is a more important contributor to the final prediction.\n\n\nCode\nregressor_importance = pd.DataFrame(\n    benchmark.benchmark.best_estimator_.feature_importances_, \n    index=X.columns, \n    columns=[\"Importance\"]\n    )\n\nregressor_importance.sort_values(by=\"Importance\", ascending=False) \\\n    .plot.bar(figsize=(20, 8), title='Regressor importance')\n\n\n&lt;AxesSubplot:title={'center':'Regressor importance'}&gt;\n\n\n\n\n\nFrom the graph above, we can see that the regressor bmp1l (black-market premium on foreign exchange) predominates. Interestingly, Belloni, Chernozhukov, and Hansen (2011) using squared-root lasso also find this regressor to be important."
  },
  {
    "objectID": "barrolee1994.html#testing-the-conditional-converge-hypothesis",
    "href": "barrolee1994.html#testing-the-conditional-converge-hypothesis",
    "title": "Using gingado to understand economic growth",
    "section": "Testing the conditional converge hypothesis",
    "text": "Testing the conditional converge hypothesis\nNow we can leverage our automatic benchmark model to test the conditional converge hypothesis - ie, the preposition that countries with lower starting GDP tend to grow faster than other comparable countries. In other words, this hypothesis predicts that when GDP growth is regressed on the level of past GDP and on an adequate set of covariates \\(X\\), the coefficient on past GDP levels are negative.\nSince we have the results for the importance of each regressor in separating countries by their growth result, we can compare the estimated coefficient for GDP levels in regressions that include different regressors in the vector \\(X\\). To maintain this example a simple exercise, the following three models are estimated:\n\n\\(X\\) contains the five most important regressors, as estimated by the benchmark model (see the graph above)\n\\(X\\) contains the five least important regressors, from the same estimation as above\n\\(X\\) is the empty set - in other words, this is a simple equation on GDP growth on GDP levels\n\nA result that would be consistent with the conditionality of the conditional convergence hypothesis is the first equation resulting in a negative coefficient for starting GDP, while the following two equations may not necessarily be successful in identifying a negative coefficient. This is because the least important regressors are not likely to have sufficient predictive power to separate countries into comparable groups.\nThe five more and less important regressors are:\n\n\nCode\ntop_five = regressor_importance.sort_values(by=\"Importance\", ascending=False).head(5)\nbottom_five = regressor_importance.sort_values(by=\"Importance\", ascending=True).head(5)\n\ntop_five, bottom_five\n\n\n(            Importance\n bmp1l         0.128438\n pop6565       0.120012\n teasec65      0.056266\n Unnamed: 0    0.048724\n gpop1         0.040157,\n          Importance\n human65    0.001123\n hyr65      0.001158\n high65     0.001531\n secc65     0.001928\n s65        0.002282)\n\n\n\n\nCode\nimport statsmodels.api as sm\n\n\n\n\nCode\ngdp_level = 'gdpsh465'\n\n\n\n\nCode\nX_topfive = X[[gdp_level] + list(top_five.index)]\nX_topfive = sm.add_constant(X_topfive)\nX_topfive.head()\n\n\n\n\n\n\n\n\n\nconst\ngdpsh465\nbmp1l\npop6565\nteasec65\nUnnamed: 0\ngpop1\n\n\n\n\n0\n1.0\n6.591674\n0.2837\n0.027591\n17.3\n0\n0.0203\n\n\n1\n1.0\n6.829794\n0.6141\n0.035637\n18.0\n1\n0.0185\n\n\n2\n1.0\n8.895082\n0.0000\n0.076685\n20.7\n2\n0.0188\n\n\n3\n1.0\n7.565275\n0.1997\n0.031039\n22.7\n3\n0.0345\n\n\n4\n1.0\n7.162397\n0.1740\n0.026281\n17.6\n4\n0.0310\n\n\n\n\n\n\n\n\n\nCode\nX_bottomfive = X[[gdp_level] + list(bottom_five.index)]\nX_bottomfive = sm.add_constant(X_bottomfive)\nX_bottomfive.head()\n\n\n\n\n\n\n\n\n\nconst\ngdpsh465\nhuman65\nhyr65\nhigh65\nsecc65\ns65\n\n\n\n\n0\n1.0\n6.591674\n0.301\n0.004\n0.12\n0.13\n0.04\n\n\n1\n1.0\n6.829794\n0.706\n0.027\n0.70\n1.36\n0.16\n\n\n2\n1.0\n8.895082\n8.317\n0.424\n16.67\n15.68\n0.56\n\n\n3\n1.0\n7.565275\n3.833\n0.104\n3.10\n2.76\n0.24\n\n\n4\n1.0\n7.162397\n1.900\n0.022\n0.67\n2.17\n0.17\n\n\n\n\n\n\n\n\n\nCode\nX_onlyGDPlevel = sm.add_constant(X[gdp_level])\nX_onlyGDPlevel.head()\n\n\n\n\n\n\n\n\n\nconst\ngdpsh465\n\n\n\n\n0\n1.0\n6.591674\n\n\n1\n1.0\n6.829794\n\n\n2\n1.0\n8.895082\n\n\n3\n1.0\n7.565275\n\n\n4\n1.0\n7.162397\n\n\n\n\n\n\n\n\n\nCode\nmodels = dict(\n    topfive = sm.OLS(y, X_topfive).fit(),\n    bottomfive = sm.OLS(y, X_bottomfive).fit(),\n    onlyGDPlevel = sm.OLS(y, X_onlyGDPlevel).fit()\n)\n\n\n\n\nCode\ncoefs = pd.DataFrame({name: model.conf_int().loc[gdp_level] for name, model in models.items()})\ncoefs.loc[0.5] = [model.params[gdp_level] for _, model in models.items()]\ncoefs = coefs.sort_index().reset_index(drop=True)\ncoefs.index = ['[0.025', 'coef on GDP levels', '0.975]']\ncoefs\n\n\n\n\n\n\n\n\n\ntopfive\nbottomfive\nonlyGDPlevel\n\n\n\n\n[0.025\n-0.033143\n-0.037257\n-0.010810\n\n\ncoef on GDP levels\n-0.016378\n-0.014432\n0.001317\n\n\n0.975]\n0.000387\n0.008394\n0.013444\n\n\n\n\n\n\n\nThe equation using the top five regressors in explanatory power yielded a coefficient that is statistically speaking negative under the usual confidence interval levels. In contrast, the regression using the bottom five regressors failed to maintain that level of statistical significance (although the coefficient point estimate was still negative). And finally the regression on GDP level solely resulted, as in the past literature, on a point estimate that is also statistically not different than zero.\nThese results above offer a different way to add evidence to the conditional convergence hypothesis. In particular, with the help of gingado’s RegressionBenchmark model, it is possible to identify which covariates can meaningfully serve as covariates in a growth equation from those that cannot. This is important because if the covariate selection for some reason included only variables with little explanatory power instead of the most relevant ones, an economist might erroneously reach a different conclusion."
  },
  {
    "objectID": "barrolee1994.html#model-documentation",
    "href": "barrolee1994.html#model-documentation",
    "title": "Using gingado to understand economic growth",
    "section": "Model documentation",
    "text": "Model documentation\nImportantly for model documentation, the benchmark already has some baseline documentation set up. If the user wishes, they can use that as a basis to document their model. Note that the output is in a raw format that is suitable for machine reading and writing. Intermediary and advanced users may wish to use that format to construct personalised forms, documents, etc.\n\n\nCode\nbenchmark.model_documentation.show_json()\n\n\n{'model_details': {'developer': 'Person or organisation developing the model',\n  'datetime': '2022-09-24 00:46:53 ',\n  'version': 'Model version',\n  'type': 'Model type',\n  'info': {'_estimator_type': 'regressor',\n   'best_estimator_': RandomForestRegressor(max_features=None, oob_score=True),\n   'best_index_': 4,\n   'best_params_': {'max_features': None, 'n_estimators': 100},\n   'best_score_': -0.08054342152726568,\n   'cv_results_': {'mean_fit_time': array([0.12784224, 0.26766114, 0.10639763, 0.26851478, 0.16334381,\n           0.43746042]),\n    'std_fit_time': array([0.01149181, 0.0151677 , 0.00422391, 0.01286013, 0.00902753,\n           0.00671144]),\n    'mean_score_time': array([0.00732503, 0.01378322, 0.00575457, 0.01454234, 0.00595331,\n           0.01524515]),\n    'std_score_time': array([0.00145713, 0.00051617, 0.00038052, 0.00117699, 0.00045451,\n           0.00170701]),\n    'param_max_features': masked_array(data=['sqrt', 'sqrt', 'log2', 'log2', None, None],\n                 mask=[False, False, False, False, False, False],\n           fill_value='?',\n                dtype=object),\n    'param_n_estimators': masked_array(data=[100, 250, 100, 250, 100, 250],\n                 mask=[False, False, False, False, False, False],\n           fill_value='?',\n                dtype=object),\n    'params': [{'max_features': 'sqrt', 'n_estimators': 100},\n     {'max_features': 'sqrt', 'n_estimators': 250},\n     {'max_features': 'log2', 'n_estimators': 100},\n     {'max_features': 'log2', 'n_estimators': 250},\n     {'max_features': None, 'n_estimators': 100},\n     {'max_features': None, 'n_estimators': 250}],\n    'split0_test_score': array([-0.16933021, -0.09208619, -0.12292621, -0.21151594, -0.10852241,\n           -0.16088226]),\n    'split1_test_score': array([-0.2957416 , -0.27786931, -0.33490436, -0.26795696, -0.35369235,\n           -0.32993453]),\n    'split2_test_score': array([ 0.20460155,  0.08529122, -0.03741542,  0.08258917,  0.40461419,\n            0.41146413]),\n    'split3_test_score': array([-0.72892109, -0.5736127 , -0.62330486, -0.75737116, -0.4710023 ,\n           -0.46087249]),\n    'split4_test_score': array([0.13750078, 0.12674359, 0.01719048, 0.11477866, 0.12588575,\n           0.09874811]),\n    'mean_test_score': array([-0.17037811, -0.14630668, -0.22027207, -0.20789524, -0.08054342,\n           -0.08829541]),\n    'std_test_score': array([0.33558501, 0.25730646, 0.23446843, 0.31433865, 0.31806987,\n           0.31215921]),\n    'rank_test_score': array([4, 3, 6, 5, 1, 2], dtype=int32)},\n   'multimetric_': False,\n   'n_features_in_': 62,\n   'n_splits_': 5,\n   'refit_time_': 0.21793103218078613,\n   'scorer_': &lt;function sklearn.metrics._scorer._passthrough_scorer(estimator, *args, **kwargs)&gt;},\n  'paper': 'Paper or other resource for more information',\n  'citation': 'Citation details',\n  'license': 'License',\n  'contact': 'Where to send questions or comments about the model'},\n 'intended_use': {'primary_uses': 'Primary intended uses',\n  'primary_users': 'Primary intended users',\n  'out_of_scope': 'Out-of-scope use cases'},\n 'factors': {'relevant': 'Relevant factors',\n  'evaluation': 'Evaluation factors'},\n 'metrics': {'performance_measures': 'Model performance measures',\n  'thresholds': 'Decision thresholds',\n  'variation_approaches': 'Variation approaches'},\n 'evaluation_data': {'datasets': 'Datasets',\n  'motivation': 'Motivation',\n  'preprocessing': 'Preprocessing'},\n 'training_data': {'training_data': 'Information on training data'},\n 'quant_analyses': {'unitary': 'Unitary results',\n  'intersectional': 'Intersectional results'},\n 'ethical_considerations': {'sensitive_data': 'Does the model use any sensitive data (e.g., protected classes)?',\n  'human_life': 'Is the model intended to inform decisions about mat- ters central to human life or flourishing - e.g., health or safety? Or could it be used in such a way?',\n  'mitigations': 'What risk mitigation strategies were used during model development?',\n  'risks_and_harms': '\\n            What risks may be present in model usage? Try to identify the potential recipients, likelihood, and magnitude of harms. \\n            If these cannot be determined, note that they were consid- ered but remain unknown',\n  'use_cases': 'Are there any known model use cases that are especially fraught?',\n  'additional_information': '\\n            If possible, this section should also include any additional ethical considerations that went into model development, \\n            for example, review by an external board, or testing with a specific community.'},\n 'caveats_recommendations': {'caveats': 'For example, did the results suggest any further testing? Were there any relevant groups that were not represented in the evaluation dataset?',\n  'recommendations': 'Are there additional recommendations for model use? What are the ideal characteristics of an evaluation dataset for this model?'}}\n\n\nSince there is some information in the model documentation that was automatically added, we might want to concentrate on the fields in the model card that are yet to be answered. Actually, this is the purpose of gingado’s automatic documentation: to afford users more time so they can invest, if they want, on model documentation.\n\n\nCode\nbenchmark.model_documentation.open_questions()\n\n\n['model_details__developer',\n 'model_details__version',\n 'model_details__type',\n 'model_details__paper',\n 'model_details__citation',\n 'model_details__license',\n 'model_details__contact',\n 'intended_use__primary_uses',\n 'intended_use__primary_users',\n 'intended_use__out_of_scope',\n 'factors__relevant',\n 'factors__evaluation',\n 'metrics__performance_measures',\n 'metrics__thresholds',\n 'metrics__variation_approaches',\n 'evaluation_data__datasets',\n 'evaluation_data__motivation',\n 'evaluation_data__preprocessing',\n 'training_data__training_data',\n 'quant_analyses__unitary',\n 'quant_analyses__intersectional',\n 'ethical_considerations__sensitive_data',\n 'ethical_considerations__human_life',\n 'ethical_considerations__mitigations',\n 'ethical_considerations__risks_and_harms',\n 'ethical_considerations__use_cases',\n 'ethical_considerations__additional_information',\n 'caveats_recommendations__caveats',\n 'caveats_recommendations__recommendations']\n\n\nLet’s fill some information:\n\n\nCode\nbenchmark.model_documentation.fill_info({\n    'intended_use': {\n        'primary_uses': 'This model is trained for pedagogical uses only.',\n        'primary_users': 'Everyone is welcome to follow the description showing the development of this benchmark.'\n    }\n})\n\n\nNote the format, based on a Python dictionary. In particular, the open_questions method results include keys divided by double underscores. As seen above, these should be interpreted as different levels of the documentation template, leading to a nested dictionary.\nNow when we confirm that the questions answered above are no longer “open questions”:\n\n\nCode\nbenchmark.model_documentation.open_questions()\n\n\n['model_details__developer',\n 'model_details__version',\n 'model_details__type',\n 'model_details__paper',\n 'model_details__citation',\n 'model_details__license',\n 'model_details__contact',\n 'intended_use__out_of_scope',\n 'factors__relevant',\n 'factors__evaluation',\n 'metrics__performance_measures',\n 'metrics__thresholds',\n 'metrics__variation_approaches',\n 'evaluation_data__datasets',\n 'evaluation_data__motivation',\n 'evaluation_data__preprocessing',\n 'training_data__training_data',\n 'quant_analyses__unitary',\n 'quant_analyses__intersectional',\n 'ethical_considerations__sensitive_data',\n 'ethical_considerations__human_life',\n 'ethical_considerations__mitigations',\n 'ethical_considerations__risks_and_harms',\n 'ethical_considerations__use_cases',\n 'ethical_considerations__additional_information',\n 'caveats_recommendations__caveats',\n 'caveats_recommendations__recommendations']\n\n\nIf we want, at any time we can save the documentation to a local JSON file, as well as read another document."
  },
  {
    "objectID": "barrolee1994.html#trying-out-model-alternatives",
    "href": "barrolee1994.html#trying-out-model-alternatives",
    "title": "Using gingado to understand economic growth",
    "section": "Trying out model alternatives",
    "text": "Trying out model alternatives\nThe benchmark model may be enough for some analyses, or maybe the user is interested in using the benchmark to explore the data and have an understanding of the importance of each regressor, to concentrate their work on data that can be meaningful for their purposes. But oftentimes a user will want to seek a machine learning model that performs as well as possible.\nFor users that want to manually create other models, gingado allows the possibility of comparing them with the benchmark. If the user model is better, it becomes the new benchmark!\nFor the following analyses, we will use K-fold as cross-validation, with 5 splits of the sample.\n\nFirst candidate: a gradient boosting tree\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n\n\n\nCode\nparam_grid = {\n    'learning_rate': [0.01, 0.1, 0.25],\n    'max_depth': [3, 6, 9]\n}\n\nreg_gradbooster = GradientBoostingRegressor()\n\ngradboosterg_grid = GridSearchCV(\n    reg_gradbooster,\n    param_grid,\n    n_jobs=-1,\n    verbose=2\n).fit(X, y)\n\n\nFitting 5 folds for each of 9 candidates, totalling 45 fits\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   0.3s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   0.3s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.1s\n\n\n\n\nCode\ny_pred = gradboosterg_grid.predict(X)\npd.DataFrame({\n    'y': y,\n    'y_pred': y_pred\n    }).plot.scatter(x='y', y='y_pred', grid=True)\n\n\n&lt;AxesSubplot:xlabel='y', ylabel='y_pred'&gt;\n\n\n\n\n\n\n\nCode\npd.DataFrame(y - y_pred).plot.hist(bins=30)\n\n\n&lt;AxesSubplot:ylabel='Frequency'&gt;\n\n\n\n\n\n\n\nSecond candidate: lasso\n\n\nCode\nfrom sklearn.linear_model import Lasso\n\n\n\n\nCode\nparam_grid = {\n    'alpha': [0.5, 1, 1.25],\n}\n\nreg_lasso = Lasso(fit_intercept=True)\n\nlasso_grid = GridSearchCV(\n    reg_lasso,\n    param_grid,\n    n_jobs=-1,\n    verbose=2\n).fit(X, y)\n\n\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n\n\n\n\nCode\ny_pred = lasso_grid.predict(X)\npd.DataFrame({\n    'y': y,\n    'y_pred': y_pred\n    }).plot.scatter(x='y', y='y_pred', grid=True)\n\n\n&lt;AxesSubplot:xlabel='y', ylabel='y_pred'&gt;\n\n\n\n\n\n\n\nCode\npd.DataFrame(y - y_pred).plot.hist(bins=30)\n\n\n&lt;AxesSubplot:ylabel='Frequency'&gt;"
  },
  {
    "objectID": "barrolee1994.html#comparing-the-models-with-the-benchmark",
    "href": "barrolee1994.html#comparing-the-models-with-the-benchmark",
    "title": "Using gingado to understand economic growth",
    "section": "Comparing the models with the benchmark",
    "text": "Comparing the models with the benchmark\ngingado allows users to compare different candidate models with the existing benchmark in a very simple way: using the compare method.\n\n\nCode\ncandidates = [gradboosterg_grid, lasso_grid]\nbenchmark.compare(X, y, candidates)\n\n\nFitting 5 folds for each of 4 candidates, totalling 20 fits\nFitting 5 folds for each of 6 candidates, totalling 30 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END candidate_estimator=GridSearchCV(estimator=RandomForestRegressor(oob_score=True),\n             param_grid={'max_features': ['sqrt', 'log2', None],\n                         'n_estimators': [100, 250]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=RandomForestRegressor(oob_score=True), candidate_estimator__estimator__bootstrap=True, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=squared_error, candidate_estimator__estimator__max_depth=None, candidate_estimator__estimator__max_features=1.0, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__max_samples=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_jobs=None, candidate_estimator__estimator__oob_score=True, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=None, candidate_estimator__param_grid={'n_estimators': [100, 250], 'max_features': ['sqrt', 'log2', None]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   7.0s\nFitting 5 folds for each of 6 candidates, totalling 30 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END candidate_estimator=GridSearchCV(estimator=RandomForestRegressor(oob_score=True),\n             param_grid={'max_features': ['sqrt', 'log2', None],\n                         'n_estimators': [100, 250]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=RandomForestRegressor(oob_score=True), candidate_estimator__estimator__bootstrap=True, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=squared_error, candidate_estimator__estimator__max_depth=None, candidate_estimator__estimator__max_features=1.0, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__max_samples=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_jobs=None, candidate_estimator__estimator__oob_score=True, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=None, candidate_estimator__param_grid={'n_estimators': [100, 250], 'max_features': ['sqrt', 'log2', None]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   6.9s\nFitting 5 folds for each of 6 candidates, totalling 30 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END candidate_estimator=GridSearchCV(estimator=RandomForestRegressor(oob_score=True),\n             param_grid={'max_features': ['sqrt', 'log2', None],\n                         'n_estimators': [100, 250]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=RandomForestRegressor(oob_score=True), candidate_estimator__estimator__bootstrap=True, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=squared_error, candidate_estimator__estimator__max_depth=None, candidate_estimator__estimator__max_features=1.0, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__max_samples=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_jobs=None, candidate_estimator__estimator__oob_score=True, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=None, candidate_estimator__param_grid={'n_estimators': [100, 250], 'max_features': ['sqrt', 'log2', None]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   6.4s\nFitting 5 folds for each of 6 candidates, totalling 30 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END candidate_estimator=GridSearchCV(estimator=RandomForestRegressor(oob_score=True),\n             param_grid={'max_features': ['sqrt', 'log2', None],\n                         'n_estimators': [100, 250]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=RandomForestRegressor(oob_score=True), candidate_estimator__estimator__bootstrap=True, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=squared_error, candidate_estimator__estimator__max_depth=None, candidate_estimator__estimator__max_features=1.0, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__max_samples=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_jobs=None, candidate_estimator__estimator__oob_score=True, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=None, candidate_estimator__param_grid={'n_estimators': [100, 250], 'max_features': ['sqrt', 'log2', None]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   6.7s\nFitting 5 folds for each of 6 candidates, totalling 30 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END candidate_estimator=GridSearchCV(estimator=RandomForestRegressor(oob_score=True),\n             param_grid={'max_features': ['sqrt', 'log2', None],\n                         'n_estimators': [100, 250]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=RandomForestRegressor(oob_score=True), candidate_estimator__estimator__bootstrap=True, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=squared_error, candidate_estimator__estimator__max_depth=None, candidate_estimator__estimator__max_features=1.0, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__max_samples=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_jobs=None, candidate_estimator__estimator__oob_score=True, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=None, candidate_estimator__param_grid={'n_estimators': [100, 250], 'max_features': ['sqrt', 'log2', None]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   6.7s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   0.2s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.1s\n[CV] END candidate_estimator=GridSearchCV(estimator=GradientBoostingRegressor(), n_jobs=-1,\n             param_grid={'learning_rate': [0.01, 0.1, 0.25],\n                         'max_depth': [3, 6, 9]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=GradientBoostingRegressor(), candidate_estimator__estimator__alpha=0.9, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=friedman_mse, candidate_estimator__estimator__init=None, candidate_estimator__estimator__learning_rate=0.1, candidate_estimator__estimator__loss=squared_error, candidate_estimator__estimator__max_depth=3, candidate_estimator__estimator__max_features=None, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_iter_no_change=None, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__subsample=1.0, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__validation_fraction=0.1, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'learning_rate': [0.01, 0.1, 0.25], 'max_depth': [3, 6, 9]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   0.6s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   0.2s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   0.2s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.1s\n[CV] END candidate_estimator=GridSearchCV(estimator=GradientBoostingRegressor(), n_jobs=-1,\n             param_grid={'learning_rate': [0.01, 0.1, 0.25],\n                         'max_depth': [3, 6, 9]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=GradientBoostingRegressor(), candidate_estimator__estimator__alpha=0.9, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=friedman_mse, candidate_estimator__estimator__init=None, candidate_estimator__estimator__learning_rate=0.1, candidate_estimator__estimator__loss=squared_error, candidate_estimator__estimator__max_depth=3, candidate_estimator__estimator__max_features=None, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_iter_no_change=None, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__subsample=1.0, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__validation_fraction=0.1, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'learning_rate': [0.01, 0.1, 0.25], 'max_depth': [3, 6, 9]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   0.5s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   0.2s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   0.2s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.1s\n[CV] END candidate_estimator=GridSearchCV(estimator=GradientBoostingRegressor(), n_jobs=-1,\n             param_grid={'learning_rate': [0.01, 0.1, 0.25],\n                         'max_depth': [3, 6, 9]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=GradientBoostingRegressor(), candidate_estimator__estimator__alpha=0.9, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=friedman_mse, candidate_estimator__estimator__init=None, candidate_estimator__estimator__learning_rate=0.1, candidate_estimator__estimator__loss=squared_error, candidate_estimator__estimator__max_depth=3, candidate_estimator__estimator__max_features=None, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_iter_no_change=None, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__subsample=1.0, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__validation_fraction=0.1, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'learning_rate': [0.01, 0.1, 0.25], 'max_depth': [3, 6, 9]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   0.6s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   0.2s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   0.2s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.1s\n[CV] END candidate_estimator=GridSearchCV(estimator=GradientBoostingRegressor(), n_jobs=-1,\n             param_grid={'learning_rate': [0.01, 0.1, 0.25],\n                         'max_depth': [3, 6, 9]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=GradientBoostingRegressor(), candidate_estimator__estimator__alpha=0.9, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=friedman_mse, candidate_estimator__estimator__init=None, candidate_estimator__estimator__learning_rate=0.1, candidate_estimator__estimator__loss=squared_error, candidate_estimator__estimator__max_depth=3, candidate_estimator__estimator__max_features=None, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_iter_no_change=None, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__subsample=1.0, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__validation_fraction=0.1, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'learning_rate': [0.01, 0.1, 0.25], 'max_depth': [3, 6, 9]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   0.5s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   0.2s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.1s\n[CV] END candidate_estimator=GridSearchCV(estimator=GradientBoostingRegressor(), n_jobs=-1,\n             param_grid={'learning_rate': [0.01, 0.1, 0.25],\n                         'max_depth': [3, 6, 9]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=GradientBoostingRegressor(), candidate_estimator__estimator__alpha=0.9, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=friedman_mse, candidate_estimator__estimator__init=None, candidate_estimator__estimator__learning_rate=0.1, candidate_estimator__estimator__loss=squared_error, candidate_estimator__estimator__max_depth=3, candidate_estimator__estimator__max_features=None, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_iter_no_change=None, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__subsample=1.0, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__validation_fraction=0.1, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'learning_rate': [0.01, 0.1, 0.25], 'max_depth': [3, 6, 9]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   0.6s\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END candidate_estimator=GridSearchCV(estimator=Lasso(), n_jobs=-1, param_grid={'alpha': [0.5, 1, 1.25]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=Lasso(), candidate_estimator__estimator__alpha=1.0, candidate_estimator__estimator__copy_X=True, candidate_estimator__estimator__fit_intercept=True, candidate_estimator__estimator__max_iter=1000, candidate_estimator__estimator__normalize=deprecated, candidate_estimator__estimator__positive=False, candidate_estimator__estimator__precompute=False, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__selection=cyclic, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'alpha': [0.5, 1, 1.25]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   0.0s\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END candidate_estimator=GridSearchCV(estimator=Lasso(), n_jobs=-1, param_grid={'alpha': [0.5, 1, 1.25]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=Lasso(), candidate_estimator__estimator__alpha=1.0, candidate_estimator__estimator__copy_X=True, candidate_estimator__estimator__fit_intercept=True, candidate_estimator__estimator__max_iter=1000, candidate_estimator__estimator__normalize=deprecated, candidate_estimator__estimator__positive=False, candidate_estimator__estimator__precompute=False, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__selection=cyclic, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'alpha': [0.5, 1, 1.25]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   0.0s\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END candidate_estimator=GridSearchCV(estimator=Lasso(), n_jobs=-1, param_grid={'alpha': [0.5, 1, 1.25]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=Lasso(), candidate_estimator__estimator__alpha=1.0, candidate_estimator__estimator__copy_X=True, candidate_estimator__estimator__fit_intercept=True, candidate_estimator__estimator__max_iter=1000, candidate_estimator__estimator__normalize=deprecated, candidate_estimator__estimator__positive=False, candidate_estimator__estimator__precompute=False, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__selection=cyclic, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'alpha': [0.5, 1, 1.25]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   0.0s\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END candidate_estimator=GridSearchCV(estimator=Lasso(), n_jobs=-1, param_grid={'alpha': [0.5, 1, 1.25]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=Lasso(), candidate_estimator__estimator__alpha=1.0, candidate_estimator__estimator__copy_X=True, candidate_estimator__estimator__fit_intercept=True, candidate_estimator__estimator__max_iter=1000, candidate_estimator__estimator__normalize=deprecated, candidate_estimator__estimator__positive=False, candidate_estimator__estimator__precompute=False, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__selection=cyclic, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'alpha': [0.5, 1, 1.25]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   0.0s\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s[CV] END .........................................alpha=1.25; total time=   0.0s\n\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END candidate_estimator=GridSearchCV(estimator=Lasso(), n_jobs=-1, param_grid={'alpha': [0.5, 1, 1.25]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=Lasso(), candidate_estimator__estimator__alpha=1.0, candidate_estimator__estimator__copy_X=True, candidate_estimator__estimator__fit_intercept=True, candidate_estimator__estimator__max_iter=1000, candidate_estimator__estimator__normalize=deprecated, candidate_estimator__estimator__positive=False, candidate_estimator__estimator__precompute=False, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__selection=cyclic, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'alpha': [0.5, 1, 1.25]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   0.0s\nFitting 5 folds for each of 6 candidates, totalling 30 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   0.2s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   0.2s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.1s\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END candidate_estimator=VotingRegressor(estimators=[('candidate_1',\n                             GridSearchCV(estimator=RandomForestRegressor(oob_score=True),\n                                          param_grid={'max_features': ['sqrt',\n                                                                       'log2',\n                                                                       None],\n                                                      'n_estimators': [100,\n                                                                       250]},\n                                          verbose=2)),\n                            ('candidate_2',\n                             GridSearchCV(estimator=GradientBoostingRegressor(),\n                                          n_jobs=-1,\n                                          param_grid={'learning_rate': [0.01,\n                                                                        0.1,\n                                                                        0.25],\n                                                      'max_depth': [3, 6, 9]},\n                                          verbose=2)),\n                            ('candidate_3',\n                             GridSearchCV(estimator=Lasso(), n_jobs=-1,\n                                          param_grid={'alpha': [0.5, 1, 1.25]},\n                                          verbose=2))]); total time=   7.0s\nFitting 5 folds for each of 6 candidates, totalling 30 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   0.3s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.3s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   0.4s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.3s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.3s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   0.4s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   0.4s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.3s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.3s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.3s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.3s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   0.4s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   0.4s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   0.4s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   0.4s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   0.4s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.1s\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END candidate_estimator=VotingRegressor(estimators=[('candidate_1',\n                             GridSearchCV(estimator=RandomForestRegressor(oob_score=True),\n                                          param_grid={'max_features': ['sqrt',\n                                                                       'log2',\n                                                                       None],\n                                                      'n_estimators': [100,\n                                                                       250]},\n                                          verbose=2)),\n                            ('candidate_2',\n                             GridSearchCV(estimator=GradientBoostingRegressor(),\n                                          n_jobs=-1,\n                                          param_grid={'learning_rate': [0.01,\n                                                                        0.1,\n                                                                        0.25],\n                                                      'max_depth': [3, 6, 9]},\n                                          verbose=2)),\n                            ('candidate_3',\n                             GridSearchCV(estimator=Lasso(), n_jobs=-1,\n                                          param_grid={'alpha': [0.5, 1, 1.25]},\n                                          verbose=2))]); total time=   7.7s\nFitting 5 folds for each of 6 candidates, totalling 30 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   0.2s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.1s\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END candidate_estimator=VotingRegressor(estimators=[('candidate_1',\n                             GridSearchCV(estimator=RandomForestRegressor(oob_score=True),\n                                          param_grid={'max_features': ['sqrt',\n                                                                       'log2',\n                                                                       None],\n                                                      'n_estimators': [100,\n                                                                       250]},\n                                          verbose=2)),\n                            ('candidate_2',\n                             GridSearchCV(estimator=GradientBoostingRegressor(),\n                                          n_jobs=-1,\n                                          param_grid={'learning_rate': [0.01,\n                                                                        0.1,\n                                                                        0.25],\n                                                      'max_depth': [3, 6, 9]},\n                                          verbose=2)),\n                            ('candidate_3',\n                             GridSearchCV(estimator=Lasso(), n_jobs=-1,\n                                          param_grid={'alpha': [0.5, 1, 1.25]},\n                                          verbose=2))]); total time=   7.9s\nFitting 5 folds for each of 6 candidates, totalling 30 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   0.2s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.1s\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END candidate_estimator=VotingRegressor(estimators=[('candidate_1',\n                             GridSearchCV(estimator=RandomForestRegressor(oob_score=True),\n                                          param_grid={'max_features': ['sqrt',\n                                                                       'log2',\n                                                                       None],\n                                                      'n_estimators': [100,\n                                                                       250]},\n                                          verbose=2)),\n                            ('candidate_2',\n                             GridSearchCV(estimator=GradientBoostingRegressor(),\n                                          n_jobs=-1,\n                                          param_grid={'learning_rate': [0.01,\n                                                                        0.1,\n                                                                        0.25],\n                                                      'max_depth': [3, 6, 9]},\n                                          verbose=2)),\n                            ('candidate_3',\n                             GridSearchCV(estimator=Lasso(), n_jobs=-1,\n                                          param_grid={'alpha': [0.5, 1, 1.25]},\n                                          verbose=2))]); total time=   8.4s\nFitting 5 folds for each of 6 candidates, totalling 30 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   0.2s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   0.2s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.1s\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END candidate_estimator=VotingRegressor(estimators=[('candidate_1',\n                             GridSearchCV(estimator=RandomForestRegressor(oob_score=True),\n                                          param_grid={'max_features': ['sqrt',\n                                                                       'log2',\n                                                                       None],\n                                                      'n_estimators': [100,\n                                                                       250]},\n                                          verbose=2)),\n                            ('candidate_2',\n                             GridSearchCV(estimator=GradientBoostingRegressor(),\n                                          n_jobs=-1,\n                                          param_grid={'learning_rate': [0.01,\n                                                                        0.1,\n                                                                        0.25],\n                                                      'max_depth': [3, 6, 9]},\n                                          verbose=2)),\n                            ('candidate_3',\n                             GridSearchCV(estimator=Lasso(), n_jobs=-1,\n                                          param_grid={'alpha': [0.5, 1, 1.25]},\n                                          verbose=2))]); total time=   8.1s\nFitting 5 folds for each of 6 candidates, totalling 30 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.5s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   0.3s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   0.3s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   0.3s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   0.3s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   0.3s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   0.2s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   0.3s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   0.3s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.1s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   0.2s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   0.3s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   0.3s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.1s[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.1s\n\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.1s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.1s\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\nBenchmark updated!\nNew benchmark:\nPipeline(steps=[('candidate_estimator',\n                 VotingRegressor(estimators=[('candidate_1',\n                                              GridSearchCV(estimator=RandomForestRegressor(oob_score=True),\n                                                           param_grid={'max_features': ['sqrt',\n                                                                                        'log2',\n                                                                                        None],\n                                                                       'n_estimators': [100,\n                                                                                        250]},\n                                                           verbose=2)),\n                                             ('candidate_2',\n                                              GridSearchCV(estimator=GradientBoostingRegressor(),\n                                                           n_jobs=-1,\n                                                           param_grid={'learning_rate': [0.01,\n                                                                                         0.1,\n                                                                                         0.25],\n                                                                       'max_depth': [3,\n                                                                                     6,\n                                                                                     9]},\n                                                           verbose=2)),\n                                             ('candidate_3',\n                                              GridSearchCV(estimator=Lasso(),\n                                                           n_jobs=-1,\n                                                           param_grid={'alpha': [0.5,\n                                                                                 1,\n                                                                                 1.25]},\n                                                           verbose=2))]))])\n\n\nThe output above clearly indicates that after evaluating the models - and their ensemble together with the existing benchmark - at least one of them was better than the current benchmark. Therefore, it will now be the new benchmark.\n\n\nCode\ny_pred = benchmark.predict(X)\npd.DataFrame({\n    'y': y,\n    'y_pred': y_pred\n    }).plot.scatter(x='y', y='y_pred', grid=True)\n\n\n&lt;AxesSubplot:xlabel='y', ylabel='y_pred'&gt;\n\n\n\n\n\n\n\nCode\npd.DataFrame(y - y_pred).plot.hist(bins=30)\n\n\n&lt;AxesSubplot:ylabel='Frequency'&gt;"
  },
  {
    "objectID": "barrolee1994.html#model-documentation-1",
    "href": "barrolee1994.html#model-documentation-1",
    "title": "Using gingado to understand economic growth",
    "section": "Model documentation",
    "text": "Model documentation\nAfter this process, we can now see how the model documentation was updated automatically:\n\n\nCode\nbenchmark.model_documentation.show_json()\n\n\n{'model_details': {'developer': 'Person or organisation developing the model',\n  'datetime': '2022-09-24 00:48:34 ',\n  'version': 'Model version',\n  'type': 'Model type',\n  'info': {'_estimator_type': 'regressor',\n   'best_estimator_': Pipeline(steps=[('candidate_estimator',\n                    VotingRegressor(estimators=[('candidate_1',\n                                                 GridSearchCV(estimator=RandomForestRegressor(oob_score=True),\n                                                              param_grid={'max_features': ['sqrt',\n                                                                                           'log2',\n                                                                                           None],\n                                                                          'n_estimators': [100,\n                                                                                           250]},\n                                                              verbose=2)),\n                                                ('candidate_2',\n                                                 GridSearchCV(estimator=GradientBoostingRegressor(),\n                                                              n_jobs=-1,\n                                                              param_grid={'learning_rate': [0.01,\n                                                                                            0.1,\n                                                                                            0.25],\n                                                                          'max_depth': [3,\n                                                                                        6,\n                                                                                        9]},\n                                                              verbose=2)),\n                                                ('candidate_3',\n                                                 GridSearchCV(estimator=Lasso(),\n                                                              n_jobs=-1,\n                                                              param_grid={'alpha': [0.5,\n                                                                                    1,\n                                                                                    1.25]},\n                                                              verbose=2))]))]),\n   'best_index_': 3,\n   'best_params_': {'candidate_estimator': VotingRegressor(estimators=[('candidate_1',\n                                 GridSearchCV(estimator=RandomForestRegressor(oob_score=True),\n                                              param_grid={'max_features': ['sqrt',\n                                                                           'log2',\n                                                                           None],\n                                                          'n_estimators': [100,\n                                                                           250]},\n                                              verbose=2)),\n                                ('candidate_2',\n                                 GridSearchCV(estimator=GradientBoostingRegressor(),\n                                              n_jobs=-1,\n                                              param_grid={'learning_rate': [0.01,\n                                                                            0.1,\n                                                                            0.25],\n                                                          'max_depth': [3, 6, 9]},\n                                              verbose=2)),\n                                ('candidate_3',\n                                 GridSearchCV(estimator=Lasso(), n_jobs=-1,\n                                              param_grid={'alpha': [0.5, 1, 1.25]},\n                                              verbose=2))])},\n   'best_score_': -0.18635669528402712,\n   'cv_results_': {'mean_fit_time': array([6.72996483, 0.53636065, 0.02441492, 7.80900722]),\n    'std_fit_time': array([0.19927051, 0.02714838, 0.00106719, 0.46355224]),\n    'mean_score_time': array([0.0114748 , 0.0014092 , 0.00136008, 0.01820731]),\n    'std_score_time': array([4.06425372e-03, 4.62592504e-05, 1.08190831e-04, 4.78124658e-03]),\n    'param_candidate_estimator': masked_array(data=[GridSearchCV(estimator=RandomForestRegressor(oob_score=True),\n                                    param_grid={'max_features': ['sqrt', 'log2', None],\n                                                'n_estimators': [100, 250]},\n                                    verbose=2)                                         ,\n                       GridSearchCV(estimator=GradientBoostingRegressor(), n_jobs=-1,\n                                    param_grid={'learning_rate': [0.01, 0.1, 0.25],\n                                                'max_depth': [3, 6, 9]},\n                                    verbose=2)                                       ,\n                       GridSearchCV(estimator=Lasso(), n_jobs=-1, param_grid={'alpha': [0.5, 1, 1.25]},\n                                    verbose=2)                                                         ,\n                       VotingRegressor(estimators=[('candidate_1',\n                                                    GridSearchCV(estimator=RandomForestRegressor(oob_score=True),\n                                                                 param_grid={'max_features': ['sqrt',\n                                                                                              'log2',\n                                                                                              None],\n                                                                             'n_estimators': [100,\n                                                                                              250]},\n                                                                 verbose=2)),\n                                                   ('candidate_2',\n                                                    GridSearchCV(estimator=GradientBoostingRegressor(),\n                                                                 n_jobs=-1,\n                                                                 param_grid={'learning_rate': [0.01,\n                                                                                               0.1,\n                                                                                               0.25],\n                                                                             'max_depth': [3, 6, 9]},\n                                                                 verbose=2)),\n                                                   ('candidate_3',\n                                                    GridSearchCV(estimator=Lasso(), n_jobs=-1,\n                                                                 param_grid={'alpha': [0.5, 1, 1.25]},\n                                                                 verbose=2))])                                   ],\n                 mask=[False, False, False, False],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__cv': masked_array(data=[None, None, None, --],\n                 mask=[False, False, False,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__error_score': masked_array(data=[nan, nan, nan, --],\n                 mask=[False, False, False,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator': masked_array(data=[RandomForestRegressor(oob_score=True),\n                       GradientBoostingRegressor(), Lasso(), --],\n                 mask=[False, False, False,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__bootstrap': masked_array(data=[True, --, --, --],\n                 mask=[False,  True,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__ccp_alpha': masked_array(data=[0.0, 0.0, --, --],\n                 mask=[False, False,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__criterion': masked_array(data=['squared_error', 'friedman_mse', --, --],\n                 mask=[False, False,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__max_depth': masked_array(data=[None, 3, --, --],\n                 mask=[False, False,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__max_features': masked_array(data=[1.0, None, --, --],\n                 mask=[False, False,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__max_leaf_nodes': masked_array(data=[None, None, --, --],\n                 mask=[False, False,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__max_samples': masked_array(data=[None, --, --, --],\n                 mask=[False,  True,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__min_impurity_decrease': masked_array(data=[0.0, 0.0, --, --],\n                 mask=[False, False,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__min_samples_leaf': masked_array(data=[1, 1, --, --],\n                 mask=[False, False,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__min_samples_split': masked_array(data=[2, 2, --, --],\n                 mask=[False, False,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__min_weight_fraction_leaf': masked_array(data=[0.0, 0.0, --, --],\n                 mask=[False, False,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__n_estimators': masked_array(data=[100, 100, --, --],\n                 mask=[False, False,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__n_jobs': masked_array(data=[None, --, --, --],\n                 mask=[False,  True,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__oob_score': masked_array(data=[True, --, --, --],\n                 mask=[False,  True,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__random_state': masked_array(data=[None, None, None, --],\n                 mask=[False, False, False,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__verbose': masked_array(data=[0, 0, --, --],\n                 mask=[False, False,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__warm_start': masked_array(data=[False, False, False, --],\n                 mask=[False, False, False,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__n_jobs': masked_array(data=[None, -1, -1, --],\n                 mask=[False, False, False,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__param_grid': masked_array(data=[{'n_estimators': [100, 250], 'max_features': ['sqrt', 'log2', None]},\n                       {'learning_rate': [0.01, 0.1, 0.25], 'max_depth': [3, 6, 9]},\n                       {'alpha': [0.5, 1, 1.25]}, --],\n                 mask=[False, False, False,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__pre_dispatch': masked_array(data=['2*n_jobs', '2*n_jobs', '2*n_jobs', --],\n                 mask=[False, False, False,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__refit': masked_array(data=[True, True, True, --],\n                 mask=[False, False, False,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__return_train_score': masked_array(data=[False, False, False, --],\n                 mask=[False, False, False,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__scoring': masked_array(data=[None, None, None, --],\n                 mask=[False, False, False,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__verbose': masked_array(data=[2, 2, 2, --],\n                 mask=[False, False, False,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__alpha': masked_array(data=[--, 0.9, 1.0, --],\n                 mask=[ True, False, False,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__init': masked_array(data=[--, None, --, --],\n                 mask=[ True, False,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__learning_rate': masked_array(data=[--, 0.1, --, --],\n                 mask=[ True, False,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__loss': masked_array(data=[--, 'squared_error', --, --],\n                 mask=[ True, False,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__n_iter_no_change': masked_array(data=[--, None, --, --],\n                 mask=[ True, False,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__subsample': masked_array(data=[--, 1.0, --, --],\n                 mask=[ True, False,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__tol': masked_array(data=[--, 0.0001, 0.0001, --],\n                 mask=[ True, False, False,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__validation_fraction': masked_array(data=[--, 0.1, --, --],\n                 mask=[ True, False,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__copy_X': masked_array(data=[--, --, True, --],\n                 mask=[ True,  True, False,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__fit_intercept': masked_array(data=[--, --, True, --],\n                 mask=[ True,  True, False,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__max_iter': masked_array(data=[--, --, 1000, --],\n                 mask=[ True,  True, False,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__normalize': masked_array(data=[--, --, 'deprecated', --],\n                 mask=[ True,  True, False,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__positive': masked_array(data=[--, --, False, --],\n                 mask=[ True,  True, False,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__precompute': masked_array(data=[--, --, False, --],\n                 mask=[ True,  True, False,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__selection': masked_array(data=[--, --, 'cyclic', --],\n                 mask=[ True,  True, False,  True],\n           fill_value='?',\n                dtype=object),\n    'params': [{'candidate_estimator': GridSearchCV(estimator=RandomForestRegressor(oob_score=True),\n                   param_grid={'max_features': ['sqrt', 'log2', None],\n                               'n_estimators': [100, 250]},\n                   verbose=2),\n      'candidate_estimator__cv': None,\n      'candidate_estimator__error_score': nan,\n      'candidate_estimator__estimator': RandomForestRegressor(oob_score=True),\n      'candidate_estimator__estimator__bootstrap': True,\n      'candidate_estimator__estimator__ccp_alpha': 0.0,\n      'candidate_estimator__estimator__criterion': 'squared_error',\n      'candidate_estimator__estimator__max_depth': None,\n      'candidate_estimator__estimator__max_features': 1.0,\n      'candidate_estimator__estimator__max_leaf_nodes': None,\n      'candidate_estimator__estimator__max_samples': None,\n      'candidate_estimator__estimator__min_impurity_decrease': 0.0,\n      'candidate_estimator__estimator__min_samples_leaf': 1,\n      'candidate_estimator__estimator__min_samples_split': 2,\n      'candidate_estimator__estimator__min_weight_fraction_leaf': 0.0,\n      'candidate_estimator__estimator__n_estimators': 100,\n      'candidate_estimator__estimator__n_jobs': None,\n      'candidate_estimator__estimator__oob_score': True,\n      'candidate_estimator__estimator__random_state': None,\n      'candidate_estimator__estimator__verbose': 0,\n      'candidate_estimator__estimator__warm_start': False,\n      'candidate_estimator__n_jobs': None,\n      'candidate_estimator__param_grid': {'n_estimators': [100, 250],\n       'max_features': ['sqrt', 'log2', None]},\n      'candidate_estimator__pre_dispatch': '2*n_jobs',\n      'candidate_estimator__refit': True,\n      'candidate_estimator__return_train_score': False,\n      'candidate_estimator__scoring': None,\n      'candidate_estimator__verbose': 2},\n     {'candidate_estimator': GridSearchCV(estimator=GradientBoostingRegressor(), n_jobs=-1,\n                   param_grid={'learning_rate': [0.01, 0.1, 0.25],\n                               'max_depth': [3, 6, 9]},\n                   verbose=2),\n      'candidate_estimator__cv': None,\n      'candidate_estimator__error_score': nan,\n      'candidate_estimator__estimator': GradientBoostingRegressor(),\n      'candidate_estimator__estimator__alpha': 0.9,\n      'candidate_estimator__estimator__ccp_alpha': 0.0,\n      'candidate_estimator__estimator__criterion': 'friedman_mse',\n      'candidate_estimator__estimator__init': None,\n      'candidate_estimator__estimator__learning_rate': 0.1,\n      'candidate_estimator__estimator__loss': 'squared_error',\n      'candidate_estimator__estimator__max_depth': 3,\n      'candidate_estimator__estimator__max_features': None,\n      'candidate_estimator__estimator__max_leaf_nodes': None,\n      'candidate_estimator__estimator__min_impurity_decrease': 0.0,\n      'candidate_estimator__estimator__min_samples_leaf': 1,\n      'candidate_estimator__estimator__min_samples_split': 2,\n      'candidate_estimator__estimator__min_weight_fraction_leaf': 0.0,\n      'candidate_estimator__estimator__n_estimators': 100,\n      'candidate_estimator__estimator__n_iter_no_change': None,\n      'candidate_estimator__estimator__random_state': None,\n      'candidate_estimator__estimator__subsample': 1.0,\n      'candidate_estimator__estimator__tol': 0.0001,\n      'candidate_estimator__estimator__validation_fraction': 0.1,\n      'candidate_estimator__estimator__verbose': 0,\n      'candidate_estimator__estimator__warm_start': False,\n      'candidate_estimator__n_jobs': -1,\n      'candidate_estimator__param_grid': {'learning_rate': [0.01, 0.1, 0.25],\n       'max_depth': [3, 6, 9]},\n      'candidate_estimator__pre_dispatch': '2*n_jobs',\n      'candidate_estimator__refit': True,\n      'candidate_estimator__return_train_score': False,\n      'candidate_estimator__scoring': None,\n      'candidate_estimator__verbose': 2},\n     {'candidate_estimator': GridSearchCV(estimator=Lasso(), n_jobs=-1, param_grid={'alpha': [0.5, 1, 1.25]},\n                   verbose=2),\n      'candidate_estimator__cv': None,\n      'candidate_estimator__error_score': nan,\n      'candidate_estimator__estimator': Lasso(),\n      'candidate_estimator__estimator__alpha': 1.0,\n      'candidate_estimator__estimator__copy_X': True,\n      'candidate_estimator__estimator__fit_intercept': True,\n      'candidate_estimator__estimator__max_iter': 1000,\n      'candidate_estimator__estimator__normalize': 'deprecated',\n      'candidate_estimator__estimator__positive': False,\n      'candidate_estimator__estimator__precompute': False,\n      'candidate_estimator__estimator__random_state': None,\n      'candidate_estimator__estimator__selection': 'cyclic',\n      'candidate_estimator__estimator__tol': 0.0001,\n      'candidate_estimator__estimator__warm_start': False,\n      'candidate_estimator__n_jobs': -1,\n      'candidate_estimator__param_grid': {'alpha': [0.5, 1, 1.25]},\n      'candidate_estimator__pre_dispatch': '2*n_jobs',\n      'candidate_estimator__refit': True,\n      'candidate_estimator__return_train_score': False,\n      'candidate_estimator__scoring': None,\n      'candidate_estimator__verbose': 2},\n     {'candidate_estimator': VotingRegressor(estimators=[('candidate_1',\n                                   GridSearchCV(estimator=RandomForestRegressor(oob_score=True),\n                                                param_grid={'max_features': ['sqrt',\n                                                                             'log2',\n                                                                             None],\n                                                            'n_estimators': [100,\n                                                                             250]},\n                                                verbose=2)),\n                                  ('candidate_2',\n                                   GridSearchCV(estimator=GradientBoostingRegressor(),\n                                                n_jobs=-1,\n                                                param_grid={'learning_rate': [0.01,\n                                                                              0.1,\n                                                                              0.25],\n                                                            'max_depth': [3, 6, 9]},\n                                                verbose=2)),\n                                  ('candidate_3',\n                                   GridSearchCV(estimator=Lasso(), n_jobs=-1,\n                                                param_grid={'alpha': [0.5, 1, 1.25]},\n                                                verbose=2))])}],\n    'split0_test_score': array([-0.14223923, -1.12352886, -0.05987803, -0.1483543 ]),\n    'split1_test_score': array([-0.29097524, -0.37422845, -0.6591801 , -0.37153239]),\n    'split2_test_score': array([ 0.00159444,  0.3266288 , -0.1660621 ,  0.18563444]),\n    'split3_test_score': array([-0.48230221, -0.65713081, -0.69195534, -0.57313168]),\n    'split4_test_score': array([-0.02551751, -0.23009924, -0.09628362, -0.02439955]),\n    'mean_test_score': array([-0.18788795, -0.41167171, -0.33467184, -0.1863567 ]),\n    'std_test_score': array([0.17981006, 0.47884214, 0.28061608, 0.26455118]),\n    'rank_test_score': array([2, 4, 3, 1], dtype=int32)},\n   'feature_names_in_': array(['Unnamed: 0', 'gdpsh465', 'bmp1l', 'freeop', 'freetar', 'h65',\n          'hm65', 'hf65', 'p65', 'pm65', 'pf65', 's65', 'sm65', 'sf65',\n          'fert65', 'mort65', 'lifee065', 'gpop1', 'fert1', 'mort1',\n          'invsh41', 'geetot1', 'geerec1', 'gde1', 'govwb1', 'govsh41',\n          'gvxdxe41', 'high65', 'highm65', 'highf65', 'highc65', 'highcm65',\n          'highcf65', 'human65', 'humanm65', 'humanf65', 'hyr65', 'hyrm65',\n          'hyrf65', 'no65', 'nom65', 'nof65', 'pinstab1', 'pop65',\n          'worker65', 'pop1565', 'pop6565', 'sec65', 'secm65', 'secf65',\n          'secc65', 'seccm65', 'seccf65', 'syr65', 'syrm65', 'syrf65',\n          'teapri65', 'teasec65', 'ex1', 'im1', 'xr65', 'tot1'], dtype=object),\n   'multimetric_': False,\n   'n_features_in_': 62,\n   'n_splits_': 5,\n   'refit_time_': 8.925978183746338,\n   'scorer_': &lt;function sklearn.metrics._scorer._passthrough_scorer(estimator, *args, **kwargs)&gt;},\n  'paper': 'Paper or other resource for more information',\n  'citation': 'Citation details',\n  'license': 'License',\n  'contact': 'Where to send questions or comments about the model'},\n 'intended_use': {'primary_uses': 'Primary intended uses',\n  'primary_users': 'Primary intended users',\n  'out_of_scope': 'Out-of-scope use cases'},\n 'factors': {'relevant': 'Relevant factors',\n  'evaluation': 'Evaluation factors'},\n 'metrics': {'performance_measures': 'Model performance measures',\n  'thresholds': 'Decision thresholds',\n  'variation_approaches': 'Variation approaches'},\n 'evaluation_data': {'datasets': 'Datasets',\n  'motivation': 'Motivation',\n  'preprocessing': 'Preprocessing'},\n 'training_data': {'training_data': 'Information on training data'},\n 'quant_analyses': {'unitary': 'Unitary results',\n  'intersectional': 'Intersectional results'},\n 'ethical_considerations': {'sensitive_data': 'Does the model use any sensitive data (e.g., protected classes)?',\n  'human_life': 'Is the model intended to inform decisions about mat- ters central to human life or flourishing - e.g., health or safety? Or could it be used in such a way?',\n  'mitigations': 'What risk mitigation strategies were used during model development?',\n  'risks_and_harms': '\\n            What risks may be present in model usage? Try to identify the potential recipients, likelihood, and magnitude of harms. \\n            If these cannot be determined, note that they were consid- ered but remain unknown',\n  'use_cases': 'Are there any known model use cases that are especially fraught?',\n  'additional_information': '\\n            If possible, this section should also include any additional ethical considerations that went into model development, \\n            for example, review by an external board, or testing with a specific community.'},\n 'caveats_recommendations': {'caveats': 'For example, did the results suggest any further testing? Were there any relevant groups that were not represented in the evaluation dataset?',\n  'recommendations': 'Are there additional recommendations for model use? What are the ideal characteristics of an evaluation dataset for this model?'}}\n\n\nAnd as before, any remaining open questions can be viewed and answered using the same methods as above."
  },
  {
    "objectID": "barrolee1994.html#references",
    "href": "barrolee1994.html#references",
    "title": "Using gingado to understand economic growth",
    "section": "References",
    "text": "References\n\n\nBarro, Robert J., and Jong-Wha Lee. 1994. “Sources of Economic Growth.” Carnegie-Rochester Conference Series on Public Policy 40: 1–46. https://doi.org/10.1016/0167-2231(94)90002-7.\n\n\nBelloni, Alexandre, Victor Chernozhukov, and Christian Hansen. 2011. “Inference for High-Dimensional Sparse Econometric Models.” arXiv Preprint arXiv:1201.0220.\n\n\nGiannone, Domenico, Michele Lenza, and Giorgio E Primiceri. 2021. “Economic Predictions with Big Data: The Illusion of Sparsity.” Econometrica 89 (5): 2409–37."
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "Utils",
    "section": "",
    "text": "source\n\n\n\n get_datetime ()\n\nReturns the time now\n\nd = get_datetime()\nassert isinstance(d, str)\nassert len(d) &gt; 0\n\n\nsource\n\n\n\n\n read_attr (obj)\n\nRead object type and values of attributes from fitted object\n\n\n\n\nDetails\n\n\n\n\nobj\nObject from which to attributes will be read\n\n\n\nFunction read_attr helps gingado Documenters to read the object behind the scenes.\nIt collects the type of estimator, and any attributes resulting from fitting an object (in ie, those that end in “_” without being double underscores).\nFor example, the attributes of an untrained and a trained random forest are, in sequence:\n\nfrom sklearn.ensemble import RandomForestRegressor\n\n\nrf_unfit = RandomForestRegressor(n_estimators=3)\nrf_fit = RandomForestRegressor(n_estimators=3)\\\n    .fit([[1, 0], [0, 1]], [[0.5], [0.5]]) # random numbers\nlist(read_attr(rf_unfit)), list(read_attr(rf_fit))\n\n/var/folders/b9/p8z57lqd55xfk68xz34dg0s40000gn/T/ipykernel_45335/3975710638.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  .fit([[1, 0], [0, 1]], [[0.5], [0.5]]) # random numbers\n/Users/douglasaraujo/Coding/.venv_gingado/lib/python3.10/site-packages/sklearn/utils/deprecation.py:103: FutureWarning: Attribute `n_features_` was deprecated in version 1.0 and will be removed in 1.2. Use `n_features_in_` instead.\n  warnings.warn(msg, category=FutureWarning)\n\n\n([{'_estimator_type': 'regressor'}],\n [{'_estimator_type': 'regressor'},\n  {'base_estimator_': DecisionTreeRegressor()},\n  {'estimators_': [DecisionTreeRegressor(max_features=1.0, random_state=1632148864),\n    DecisionTreeRegressor(max_features=1.0, random_state=1616501356),\n    DecisionTreeRegressor(max_features=1.0, random_state=2109419996)]},\n  {'feature_importances_': array([0., 0.])},\n  {'n_features_': 2},\n  {'n_features_in_': 2},\n  {'n_outputs_': 1}])"
  },
  {
    "objectID": "utils.html#support-for-model-documentation",
    "href": "utils.html#support-for-model-documentation",
    "title": "Utils",
    "section": "",
    "text": "source\n\n\n\n get_datetime ()\n\nReturns the time now\n\nd = get_datetime()\nassert isinstance(d, str)\nassert len(d) &gt; 0\n\n\nsource\n\n\n\n\n read_attr (obj)\n\nRead object type and values of attributes from fitted object\n\n\n\n\nDetails\n\n\n\n\nobj\nObject from which to attributes will be read\n\n\n\nFunction read_attr helps gingado Documenters to read the object behind the scenes.\nIt collects the type of estimator, and any attributes resulting from fitting an object (in ie, those that end in “_” without being double underscores).\nFor example, the attributes of an untrained and a trained random forest are, in sequence:\n\nfrom sklearn.ensemble import RandomForestRegressor\n\n\nrf_unfit = RandomForestRegressor(n_estimators=3)\nrf_fit = RandomForestRegressor(n_estimators=3)\\\n    .fit([[1, 0], [0, 1]], [[0.5], [0.5]]) # random numbers\nlist(read_attr(rf_unfit)), list(read_attr(rf_fit))\n\n/var/folders/b9/p8z57lqd55xfk68xz34dg0s40000gn/T/ipykernel_45335/3975710638.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  .fit([[1, 0], [0, 1]], [[0.5], [0.5]]) # random numbers\n/Users/douglasaraujo/Coding/.venv_gingado/lib/python3.10/site-packages/sklearn/utils/deprecation.py:103: FutureWarning: Attribute `n_features_` was deprecated in version 1.0 and will be removed in 1.2. Use `n_features_in_` instead.\n  warnings.warn(msg, category=FutureWarning)\n\n\n([{'_estimator_type': 'regressor'}],\n [{'_estimator_type': 'regressor'},\n  {'base_estimator_': DecisionTreeRegressor()},\n  {'estimators_': [DecisionTreeRegressor(max_features=1.0, random_state=1632148864),\n    DecisionTreeRegressor(max_features=1.0, random_state=1616501356),\n    DecisionTreeRegressor(max_features=1.0, random_state=2109419996)]},\n  {'feature_importances_': array([0., 0.])},\n  {'n_features_': 2},\n  {'n_features_in_': 2},\n  {'n_outputs_': 1}])"
  },
  {
    "objectID": "utils.html#support-for-time-series",
    "href": "utils.html#support-for-time-series",
    "title": "Utils",
    "section": "Support for time series",
    "text": "Support for time series\nObjects of the class Lag are similar to scikit-learn’s transformers.\n\nsource\n\nLag\n\n Lag (lags=1, jump=0, keep_contemporaneous_X=False)\n\nA transformer that lags variables\n\nsource\n\n\nLag.fit\n\n Lag.fit (X:numpy.ndarray, y=None)\n\nFit the Lag transformer\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nX\nndarray\n\nArray-like data of shape (n_samples, n_features)\n\n\ny\nNoneType\nNone\nArray-like data of shape (n_samples,) or (n_samples, n_targets) or None\n\n\n\n\nsource\n\n\nLag.transform\n\n Lag.transform (X:numpy.ndarray)\n\nLag the dataset X\n\n\n\n\nType\nDetails\n\n\n\n\nX\nndarray\nArray-like data of shape (n_samples, n_features)\n\n\n\n\n\n\nTransformerMixin.fit_transform\n\n TransformerMixin.fit_transform (X, y=None, **fit_params)\n\nFit to data, then transform it.\nFits transformer to X and y with optional parameters fit_params and returns a transformed version of X.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nX\narray-like of shape (n_samples, n_features)\n\nInput samples.\n\n\ny\nNoneType\nNone\nTarget values (None for unsupervised transformations).\n\n\nfit_params\n\n\n\n\n\nReturns\nndarray array of shape (n_samples, n_features_new)\n\nTransformed array.\n\n\n\nThe code below demonstrates how Lag works in practice. Note in particular that, because Lag is a transformer, it can be used as part of a scikit-learn’s Pipeline.\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n\nrandomX = np.random.rand(15, 2)\nrandomY = np.random.rand(15)\n\nlags = 3\njump = 2\n\npipe = Pipeline([\n    ('scaler', StandardScaler()),\n    ('lagger', Lag(lags=lags, jump=jump, keep_contemporaneous_X=False))\n]).fit_transform(randomX, randomY)\n\nBelow we confirm that the lagger removes the correct number of rows corresponding to the lagged observations:\n\nassert randomX.shape[0] - lags - jump == pipe.shape[0]\n\nAnd because Lag is a transformer, its parameters (lags and jump) can be calibrated using hyperparameter tuning to achieve the best performance for a model."
  },
  {
    "objectID": "utils.html#support-for-data-augmentation-with-sdmx",
    "href": "utils.html#support-for-data-augmentation-with-sdmx",
    "title": "Utils",
    "section": "Support for data augmentation with SDMX",
    "text": "Support for data augmentation with SDMX\n\n\n\n\n\n\nNote\n\n\n\nplease note that working with SDMX may take some minutes depending on the amount of information you are downloading.\n\n\n\nsource\n\nlist_SDMX_sources\n\n list_SDMX_sources ()\n\nFetch the list of SDMX sources\n\nsources = list_SDMX_sources()\nprint(sources)\n\nassert len(sources) &gt; 0\n# all elements are of type 'str'\nassert sum([isinstance(src, str) for src in sources]) == len(sources)\n\n['ABS', 'ABS_XML', 'BBK', 'BIS', 'CD2030', 'ECB', 'ESTAT', 'ILO', 'IMF', 'INEGI', 'INSEE', 'ISTAT', 'LSD', 'NB', 'NBB', 'OECD', 'SGR', 'SPC', 'STAT_EE', 'UNICEF', 'UNSD', 'WB', 'WB_WDI']\n\n\n\nsource\n\n\nlist_all_dataflows\n\n list_all_dataflows (codes_only:bool=False, return_pandas:bool=True)\n\nList all SDMX dataflows. Note: When using as a parameter to an AugmentSDMX object or to the load_SDMX_data function, set codes_only=True\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncodes_only\nbool\nFalse\nWhether to return only the dataflow codes\n\n\nreturn_pandas\nbool\nTrue\nWhether to return the result in a pandas DataFrame format\n\n\n\n\ndflows = list_all_dataflows(return_pandas=False)\n\nassert isinstance(dflows, dict)\nall_sources = list_SDMX_sources()\nassert len([s for s in dflows.keys() if s in all_sources]) == len(dflows.keys())\n\n2023-09-16 00:49:48,202 pandasdmx.reader.sdmxml - DEBUG: Truncate sub-microsecond time in &lt;Prepared&gt;\n2023-09-16 00:50:09,352 pandasdmx.reader.sdmxml - DEBUG: Truncate sub-microsecond time in &lt;Prepared&gt;\n2023-09-16 00:50:10,173 pandasdmx.reader.sdmxml - DEBUG: Truncate sub-microsecond time in &lt;Prepared&gt;\n2023-09-16 00:50:19,614 pandasdmx.reader.sdmxml - DEBUG: Truncate sub-microsecond time in &lt;Prepared&gt;\n2023-09-16 00:50:20,660 pandasdmx.reader.sdmxml - DEBUG: Truncate sub-microsecond time in &lt;Prepared&gt;\n\n\nlist_all_dataflows returns by default a pandas Series, facilitating data discovery by users like so:\n\ndflows = list_all_dataflows(return_pandas=True)\nassert type(dflows) == pd.core.series.Series\n\ndflows\n\n2023-09-16 00:50:44,400 pandasdmx.reader.sdmxml - DEBUG: Truncate sub-microsecond time in &lt;Prepared&gt;\n2023-09-16 00:51:09,450 pandasdmx.reader.sdmxml - DEBUG: Truncate sub-microsecond time in &lt;Prepared&gt;\n2023-09-16 00:51:10,058 pandasdmx.reader.sdmxml - DEBUG: Truncate sub-microsecond time in &lt;Prepared&gt;\n2023-09-16 00:51:14,175 pandasdmx.reader.sdmxml - DEBUG: Truncate sub-microsecond time in &lt;Prepared&gt;\n2023-09-16 00:51:19,057 pandasdmx.reader.sdmxml - DEBUG: Truncate sub-microsecond time in &lt;Prepared&gt;\n\n\nABS_XML  ABORIGINAL_POP_PROJ                 Projected population, Aboriginal and Torres St...\n         ABORIGINAL_POP_PROJ_REMOTE          Projected population, Aboriginal and Torres St...\n         ABS_ABORIGINAL_POPPROJ_INDREGION    Projected population, Aboriginal and Torres St...\n         ABS_ACLD_LFSTATUS                   Australian Census Longitudinal Dataset (ACLD):...\n         ABS_ACLD_TENURE                     Australian Census Longitudinal Dataset (ACLD):...\n                                                                   ...                        \nUNSD     DF_UNData_UNFCC                                                       SDMX_GHG_UNDATA\nWB       DF_WITS_Tariff_TRAINS                                WITS - UNCTAD TRAINS Tariff Data\n         DF_WITS_TradeStats_Development                             WITS TradeStats Devlopment\n         DF_WITS_TradeStats_Tariff                                      WITS TradeStats Tariff\n         DF_WITS_TradeStats_Trade                                        WITS TradeStats Trade\nName: dataflow, Length: 3290, dtype: object\n\n\nThis format allows for more easily searching dflows by source:\n\nlist_all_dataflows(codes_only=True, return_pandas=True)\n\n2023-09-16 00:51:51,419 pandasdmx.reader.sdmxml - DEBUG: Truncate sub-microsecond time in &lt;Prepared&gt;\n2023-09-16 00:51:57,339 pandasdmx.reader.sdmxml - DEBUG: Truncate sub-microsecond time in &lt;Prepared&gt;\n2023-09-16 00:52:15,569 pandasdmx.reader.sdmxml - DEBUG: Truncate sub-microsecond time in &lt;Prepared&gt;\n2023-09-16 00:52:16,277 pandasdmx.reader.sdmxml - DEBUG: Truncate sub-microsecond time in &lt;Prepared&gt;\n2023-09-16 00:52:18,956 pandasdmx.reader.sdmxml - DEBUG: Truncate sub-microsecond time in &lt;Prepared&gt;\n\n\nABS_XML  0                 ABORIGINAL_POP_PROJ\n         1          ABORIGINAL_POP_PROJ_REMOTE\n         2    ABS_ABORIGINAL_POPPROJ_INDREGION\n         3                   ABS_ACLD_LFSTATUS\n         4                     ABS_ACLD_TENURE\n                            ...               \nUNSD     5                     DF_UNData_UNFCC\nWB       0               DF_WITS_Tariff_TRAINS\n         1      DF_WITS_TradeStats_Development\n         2           DF_WITS_TradeStats_Tariff\n         3            DF_WITS_TradeStats_Trade\nName: dataflow, Length: 3290, dtype: object\n\n\n\ndflows['BIS']\n\nWS_CBPOL_D                                    Policy rates daily\nWS_CBPOL_M                                  Policy rates monthly\nWS_CBS_PUB                              BIS consolidated banking\nWS_CPMI_CASHLESS                   CPMI cashless payments (T5-6)\nWS_CPMI_CT1                       CPMI comparative tables type 1\nWS_CPMI_CT2                       CPMI comparative tables type 2\nWS_CPMI_DEVICES                             CPMI payment devices\nWS_CPMI_INSTITUTIONS                           CPMI institutions\nWS_CPMI_MACRO                                         CPMI Macro\nWS_CPMI_PARTICIPANTS                           CPMI participants\nWS_CPMI_SYSTEMS         CPMI systems (T8-9-11-13-14-16-17-18-19)\nWS_CREDIT_GAP                             BIS credit-to-GDP gaps\nWS_DEBT_SEC2_PUB                             BIS debt securities\nWS_DER_OTC_TOV                          OTC derivatives turnover\nWS_DSR                                    BIS debt service ratio\nWS_EER_D                      BIS effective exchange rates daily\nWS_EER_M                    BIS effective exchange rates monthly\nWS_GLI                               Global liquidity indicators\nWS_LBS_D_PUB                              BIS locational banking\nWS_LONG_CPI                             BIS long consumer prices\nWS_OTC_DERIV2                        OTC derivatives outstanding\nWS_SPP                      BIS property prices: selected series\nWS_TC                            BIS long series on total credit\nWS_XRU                           US dollar exchange rates, m,q,a\nWS_XRU_D                         US dollar exchange rates, daily\nWS_XTD_DERIV                         Exchange traded derivatives\nName: dataflow, dtype: object\n\n\nOr the user can search dataflows by their human-readable name instead of their code. For example, this is one way to see if any dataflow has information on interest rates:\n\ndflows[dflows.str.contains('Interest rates', case=False)]\n\nBBK  BBSDI       Discount interest rates pursuant to section 25...\nECB  RIR                                     Retail Interest Rates\nIMF  6SR         M&B: Interest Rates and Share Prices (6SR) for...\n     INR                                            Interest rates\n     INR_NSTD                          Interest rates_Non-Standard\nName: dataflow, dtype: object\n\n\nThe function load_SDMX_data is a convenience function that downloads data from SDMX sources (and any specific dataflows passed as arguments) if they match the key and parameters set by the user.\n\nsource\n\n\nload_SDMX_data\n\n load_SDMX_data (sources:dict, keys:dict, params:dict, verbose:bool=True)\n\nLoads datasets from SDMX.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsources\ndict\n\nA dictionary with the sources and dataflows per source\n\n\nkeys\ndict\n\nThe keys to be used in the SDMX query\n\n\nparams\ndict\n\nThe parameters to be used in the SDMX query\n\n\nverbose\nbool\nTrue\nWhether to communicate download steps to the user\n\n\n\n\ndf = load_SDMX_data(sources={'ECB': 'CISS', 'BIS': 'WS_CBPOL_D'}, keys={'FREQ': 'D'}, params={'startPeriod': 2003})\n\nassert type(df) == pd.DataFrame\nassert df.shape[0] &gt; 0\nassert df.shape[1] &gt; 0\n\nQuerying data from ECB's dataflow 'CISS' - Composite Indicator of Systemic Stress...\nQuerying data from BIS's dataflow 'WS_CBPOL_D' - Policy rates daily...\n\n\n2023-09-16 00:52:42,940 pandasdmx.reader.sdmxml - INFO: Use supplied dsd=… argument for non–structure-specific message"
  },
  {
    "objectID": "machine_controls.html",
    "href": "machine_controls.html",
    "title": "Machine controls",
    "section": "",
    "text": "This notebook illustrates the use of MachineControl, the gingado estimator that calculates a synthetic control model with machine learning techniques."
  },
  {
    "objectID": "machine_controls.html#setting",
    "href": "machine_controls.html#setting",
    "title": "Machine controls",
    "section": "Setting",
    "text": "Setting\nUse of the MachineControl estimator is illustrated with an admittedly simplistic estimation of the impact of softening labour regulation on output per worker, measured in constant 2017 international US dollars PPP.\nMore specifically, the example below focuses on Brazil’s 2017 labour reforms (Law No 13,467/2017). The reform markedly deregulated labour markets, with the purpose of increasing productivity and thereby unlocking growth. Some of its main points are:\n\nprominence of collective bargaining between firms and employees over statutory “blanket” provisions\nlower costs for employers of employment termination without just cause\ndiscouraging of labour litigation by employees, previously diagnosed as being excessive and contributing to clogging the judicial system\n\nThe reform was enacted in July 2017 and went into effect in November of the same year."
  },
  {
    "objectID": "machine_controls.html#using-machine-learning",
    "href": "machine_controls.html#using-machine-learning",
    "title": "Machine controls",
    "section": "Using machine learning",
    "text": "Using machine learning\nMachineControl does the following:\n\nautomatically select a group of countries from a global list to form a smaller set of control countries\nestimate a GDP value for “synthetic Brazil” using pre-enactment data on the outcome of interest\ncheck the statistical quality of the synthetic control\ncalculates the difference between post-reforms actual Brazilian GDP growth to synthetic Brazil’s to measure the effect of the labour reform.\n\n\nNote: There are many other variables that would be interesting for this study as well, such as various labour market indicators.\n\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom gingado.utils import list_all_dataflows, load_SDMX_data\nfrom gingado.estimators import MachineControl\nfrom sklearn.cluster import AffinityPropagation\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.manifold import TSNE"
  },
  {
    "objectID": "machine_controls.html#downloading-the-data",
    "href": "machine_controls.html#downloading-the-data",
    "title": "Machine controls",
    "section": "Downloading the data",
    "text": "Downloading the data\nFirst, we list all dataflows obtainable with SDMX.\n\n\nCode\ndflows = list_all_dataflows(return_pandas=True)\n\n\n2023-09-14 22:42:31,803 pandasdmx.reader.sdmxml - DEBUG: Truncate sub-microsecond time in &lt;Prepared&gt;\n2023-09-14 22:42:51,049 pandasdmx.reader.sdmxml - DEBUG: Truncate sub-microsecond time in &lt;Prepared&gt;\n2023-09-14 22:42:51,959 pandasdmx.reader.sdmxml - DEBUG: Truncate sub-microsecond time in &lt;Prepared&gt;\n2023-09-14 22:42:55,113 pandasdmx.reader.sdmxml - DEBUG: Truncate sub-microsecond time in &lt;Prepared&gt;\n2023-09-14 22:42:56,250 pandasdmx.reader.sdmxml - DEBUG: Truncate sub-microsecond time in &lt;Prepared&gt;\n\n\n\n\nCode\ndflows[dflows.str.contains('per worker', case=False)]\n\n\nILO  DF_GDP_205U_NOC_NB    Output per worker (GDP constant 2015 US $) -- ...\n     DF_GDP_211P_NOC_NB    Output per worker (GDP constant 2017 internati...\n     DF_SDG_A821_NOC_RT    SDG indicator 8.2.1 - Annual growth rate of ou...\n     DF_SDG_B821_NOC_RT    SDG indicator 8.2.1 - Annual growth rate of ou...\nName: dataflow, dtype: object\n\n\nLet’s get the data on output per worker provided by the International Labour Organisation (ILO):\n\n\nCode\noutcome_var = load_SDMX_data(\n    sources={'ILO': 'DF_GDP_211P_NOC_NB'}, \n    keys={'FREQ': 'A'}, \n    params={'startPeriod': 2000, 'endPeriod': 2022}\n)\n\n\n2023-09-14 22:42:58,334 pandasdmx.reader.sdmxml - DEBUG: Truncate sub-microsecond time in &lt;Prepared&gt;\n2023-09-14 22:42:59,461 pandasdmx.reader.sdmxml - INFO: Use supplied dsd=… argument for non–structure-specific message\n\n\nQuerying data from ILO's dataflow 'DF_GDP_211P_NOC_NB' - Output per worker (GDP constant 2017 international $ at PPP) -- ILO modelled estimates, Nov. 2022...\n\n\nThis dataflow provides one series for each country, as seen below.\n\n\nCode\ncol_fields = pd.DataFrame([c.split(\"__\") for c in outcome_var.columns])\nfor col in col_fields.columns:\n    print(\"No of unique values for column No\", col, \": \", col_fields[col].nunique())\nprint(\"\\nFirst five rows:\")\nprint(col_fields.head())\n\n\nNo of unique values for column No 0 :  1\nNo of unique values for column No 1 :  271\nNo of unique values for column No 2 :  1\nNo of unique values for column No 3 :  1\n\nFirst five rows:\n     0                       1  2            3\n0  ILO  DF_GDP_211P_NOC_NB_AFG  A  GDP_211P_NB\n1  ILO  DF_GDP_211P_NOC_NB_AGO  A  GDP_211P_NB\n2  ILO  DF_GDP_211P_NOC_NB_ALB  A  GDP_211P_NB\n3  ILO  DF_GDP_211P_NOC_NB_ARE  A  GDP_211P_NB\n4  ILO  DF_GDP_211P_NOC_NB_ARG  A  GDP_211P_NB\n\n\nBecause 271 is much higher than the number of countries that usually report international statistics (close to 200), it is likely some 70 columns or more correspond to aggregations, typically used in statistics for convenience (eg, one code that encompasses all countries in the European Union, etc). We need to take them out, in case Brazil is a constituent of any of those aggregations.\nOne common way of spotting these aggregations in international statistics is finding the country codes that begin with “X”.\n\n\nCode\nlen([c for c in outcome_var.columns if \"DF_GDP_211P_NOC_NB_X\" in c])\n\n\n82\n\n\nAlso, we will take out other countries that have undergone labour reforms more or less around the same time. From Serra, Bottega, and Sanches (2022), I am aware of Argentina 🇦🇷, Costa Rica 🇨🇷, Paraguay 🇵🇾, and Uruguay 🇺🇾.\nIt is important to note, though, that these countries above were named by Serra, Bottega, and Sanches (2022) because their synthetic control donor pool consistent of geographically close countries. Other countries might have also enacted labour reforms in that period. For expositional purposes, we can assume no further country needs to be taken out of the same.\n\n\nCode\ncol_filter = [\n    c for c in outcome_var.columns\n    if \"DF_GDP_211P_NOC_NB_X\" not in c\n    and \"DF_GDP_211P_NOC_NB_ARG\" not in c\n    and \"DF_GDP_211P_NOC_NB_CRI\" not in c\n    and \"DF_GDP_211P_NOC_NB_PAR\" not in c\n    and \"DF_GDP_211P_NOC_NB_URY\" not in c\n]\n\nX = outcome_var[col_filter]\nX = X.dropna(axis=1)\n\n# cleaning out the name to remove the constant portions across all countries\nX.columns = [c.replace(\"ILO__DF_GDP_211P_NOC_NB_\", \"\") \\\n    .replace(\"__A__GDP_211P_NB\", \"\") for c in X.columns]\n\ncol_BRA = [c for c in X.columns if c == \"BRA\"]\ny = X.pop(col_BRA[0])\n\nassert X.shape[0] == y.shape[0]\n\n\nThis is what the series of annual output per worker in Brazil looks like. A vertical line marking November 2017, when the labour reforms entered into force.\n\n\nCode\nlaw_date = '2017-11-11'\nylabel = 'PPP 2017 US$ per worker'\n\nax = y.plot(legend=False)\nax.axvline(x=law_date, color='r', linestyle='--')\nplt.ylabel(ylabel)\nplt.xlabel('')\nplt.title('Labour productivity in Brazil')\n\n\nText(0.5, 1.0, 'Labour productivity in Brazil')\n\n\n\n\n\nBefore creating the MachineControl object, a final comment on the intervention date.\nSince the reforms were enacted and entered into force in the same year of 2017, we can be conservative and consider: - pre-intervention data up to end-2016 - post-intervention data from 2018 onwards\n\n\nCode\nX_pre, y_pre = X[:'2016-12-31'], y[:'2016-12-31']\n\nassert X_pre.shape[0] == y_pre.shape[0]\nX_pre.shape, y_pre.shape\n\n\n((17, 181), (17,))"
  },
  {
    "objectID": "machine_controls.html#using-the-machinecontrol-object",
    "href": "machine_controls.html#using-the-machinecontrol-object",
    "title": "Machine controls",
    "section": "Using the MachineControl object",
    "text": "Using the MachineControl object\nThe code chunk below shows how a MachineControl object can be created.\nAs illustrated below, users can not only choose the clustering, estimator and manifold learning algorithms that best suit their needs, but also pass specific arguments to each of these elements.\n\n\nCode\nsynth_BR = MachineControl(\n    cluster_alg=AffinityPropagation(max_iter=10_000),\n    estimator=RandomForestRegressor(),\n    manifold=TSNE(perplexity=5)\n)\n\n\nLet’s train and then inspect the MachineControl object:\n\n\nCode\nsynth_BR.fit(X_pre, y_pre)\n\n\n/Users/douglasaraujo/Coding/.venv_gingado/lib/python3.10/site-packages/sklearn/manifold/_t_sne.py:800: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n  warnings.warn(\n/Users/douglasaraujo/Coding/.venv_gingado/lib/python3.10/site-packages/sklearn/manifold/_t_sne.py:810: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n  warnings.warn(\n\n\nMachineControl(cluster_alg=AffinityPropagation(max_iter=10000),\n               estimator=RandomForestRegressor(), manifold=TSNE(perplexity=5))In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MachineControlMachineControl(cluster_alg=AffinityPropagation(max_iter=10000),\n               estimator=RandomForestRegressor(), manifold=TSNE(perplexity=5))cluster_alg: AffinityPropagationAffinityPropagation(max_iter=10000)AffinityPropagationAffinityPropagation(max_iter=10000)estimator: RandomForestRegressorRandomForestRegressor()RandomForestRegressorRandomForestRegressor()manifold: TSNETSNE(perplexity=5)TSNETSNE(perplexity=5)\n\n\nAs shown above, the machine control estimator comprises:\n\nA clustering algorithm that selects the donor pool from the larger population (affinity propagation)\nA supervised learning algorithm that will use the donor pool to estimate contemporanous values for Brazil (random forest)\nA manifold learning algorithm that summarises the different time series in a 2-dimensional embedding space, enabling easier comparison of Brazil with each other country and with the synthetic control (t-SNE)\n\nAt this stage, we can extract the list of donor countries, ie, those that will be used in the control.\n\n\nCode\n\" \".join(synth_BR.donor_pool_)\n\n\n'ALB BGR BIH BLR BRB BWA COL CUB DOM DZA EGY FJI GUY IRQ KAZ LCA MKD MUS NAM PSE SRB SWZ TUN VCT YEM ZAF'\n\n\nThese countries are: Albania 🇦🇱, Bulgaria 🇧🇬, Bosnia and Herzegovina 🇧🇦, Belarus 🇧🇾, Barbados 🇧🇧, Botswana 🇧🇼, Colombia 🇨🇴, Cuba 🇨🇺, Dominican Republic 🇩🇴, Algeria 🇩🇿, Egypt 🇪🇬, Fiji 🇫🇯, Guyana 🇬🇾, Iraq 🇮🇶, Kazakhstan 🇰🇿, Saint Lucia 🇱🇨, North Macedonia 🇲🇰, Mauritius 🇲🇺, Namibia 🇳🇦, State of Palestine 🇵🇸, Serbia 🇷🇸, Eswatini 🇸🇿, Tunisia 🇹🇳, Saint Vincent and the Grenadines 🇻🇨, Yemen 🇾🇪 and South Africa 🇿🇦.\n\nNote: This list does not imply all countries contribute equally to estimating the synthetic version of Brazil. In fact, some might even end up not even contributing in the estimating equation. The selected list also does not imply a causal explanation from any of those countries into the Brazilian dynamics. They are merely closest to Brazil in this clustering exercise, and as such, likely to be a good predictor at the same time period.\n\nAnd here is how the outcome variable for these countries (grey) compares with Brazil’s (red).\n\n\nCode\nax = X_pre[synth_BR.donor_pool_].plot(legend=False, color=\"grey\", linewidth=0.5)\ny_pre.plot(ax=ax, color=\"red\", linewidth=2.5)\n\n\n&lt;AxesSubplot:xlabel='TIME_PERIOD'&gt;\n\n\n\n\n\nPlotting the result of the manifold learning underscores whether the synthetic control seems indeed to come from a similar space in the data distribution as the entity of interest.\n\n\nCode\ncolors = [(0.5, 0.5, 0.5, 0.35)] * X.shape[1] + ['red', 'blue']\n\nfig, ax = plt.subplots()\nax.scatter(\n    synth_BR.manifold_embed_[:, 0], synth_BR.manifold_embed_[:, 1],\n    color=colors,\n    )\nax.legend()\nplt.show()\n\n\n/Users/douglasaraujo/Coding/.venv_gingado/lib/python3.10/site-packages/matplotlib/cbook/__init__.py:1026: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n  x = np.asanyarray(x)\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n\n\n\n\n\n\n\nCode\nsynth_BR.control_quality_test_ / 100\n\n\n0.019426995735370444\n\n\nAs can be seen in the graph above, the actual Brazil 🇧🇷 (blue) and the machine controls (red) are almost juxtaposed.\nTo confirm objectively that the control is good, the Euclidean distance between the embeddings for the control and the actual outcome are one of the lowest, more specifically at the 2% percentile.\nTogether, both point to a strong signal that the control managed to replicate the pre-intervention outcome.\nSo now it’s time to look at the results in the years following the event."
  },
  {
    "objectID": "machine_controls.html#results",
    "href": "machine_controls.html#results",
    "title": "Machine controls",
    "section": "Results",
    "text": "Results\n\n\nCode\nax = synth_BR.predict(X, y).plot(legend=False)\nax.axvline(x='2017-11-11', color='r', linestyle='--')\nplt.ylabel('PPP 2017 US$ per worker')\nplt.xlabel('')\nplt.title(\"Labour productivity in Brazil\")\nplt.show()\n\n\n/Users/douglasaraujo/Coding/.venv_gingado/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but RandomForestRegressor was fitted without feature names\n  warnings.warn(\n\n\n\n\n\n\nTreatment effect\nThe main result is shown below, corresponding to the difference between actual and estimated value for the whole time series (from 2000). If the red line (Brazil) stays close to the other ones - placebo estimations.\n\n\nCode\nall_diffs = pd.concat([\n    synth_BR.placebo_diff_,\n    synth_BR.diff_\n], axis=1)\ncolors = ['grey'] * synth_BR.placebo_diff_.shape[1] + ['red']\ny_label = 'PPP 2017 US$ per worker'\neffect_title = \"Effect of deregulation on labour productivity in Brazil\"\n\nax = all_diffs.plot(legend=False, color=colors)\nax.axvline(x='2016-12-31', color='black', linestyle='solid')\nax.axvline(x='2017-11-11', color='red', linestyle='--')\nax.axhline(y=0, color='black', linewidth=1.2)\nplt.ylabel(y_label)\nplt.xlabel('')\nplt.title(effect_title)\nplt.show()\n\n\n\n\n\nIt seems there is some outlier effect with one of the control countries.\n\n\nCode\nsynth_BR.placebo_diff_.loc['2022-01-01'].sort_values(ascending=False).index[0]\n\n\n'GUY'\n\n\nAccording to the code above, it is Guyana 🇬🇾. Now plotting without this country:\n\n\n\nCode\ncolors = ['grey'] * (synth_BR.placebo_diff_.shape[1] - 1)+ ['red']\nax = all_diffs[[c for c in all_diffs.columns if c != \"GUY\"]].plot(legend=False, color=colors)\nax.axvline(x='2016-12-31', color='black', linestyle='solid')\nax.axvline(x='2017-11-11', color='red', linestyle='--')\nax.axhline(y=0, color='black', linewidth=1.2)\nplt.ylabel(y_label)\nplt.xlabel('')\nplt.title(effect_title)\nplt.show()"
  },
  {
    "objectID": "machine_controls.html#interpretation",
    "href": "machine_controls.html#interpretation",
    "title": "Machine controls",
    "section": "Interpretation",
    "text": "Interpretation\nThe above results hint at a null effect of the reforms on labour productivity after training cutoff date (black vertical line). If anything, the effect squarely close to zero throughout the years following the reform (red vertical line), even as the productivity for the majority of control countries actually went up in the years before the pandemic."
  },
  {
    "objectID": "estimators.html",
    "href": "estimators.html",
    "title": "Estimators",
    "section": "",
    "text": "In many instances, economists are interested in using machine learning models for specific purposes that go beyond their ability to predict variables to a good accuracy. For example:\nThe gingado.estimators module contains machine learning algorithms adapted to enable the types of analyses described above. More estimators can be expected over time.\nFor more academic discussions of machine learning methods in economics covering a broad range of topics, see Athey and Imbens (2019)."
  },
  {
    "objectID": "estimators.html#clustering",
    "href": "estimators.html#clustering",
    "title": "Estimators",
    "section": "Clustering",
    "text": "Clustering\nThe clustering algorithms used below are not themselves adapted from the general use methods. Rather, the functions offer convenience functionalities to find and retain the other variables in the same cluster.\nThese variables are usually entities (individuals, countries, stocks, etc) in a larger population.\nThe gingado clustering routines are designed to allow users standalone usage, or a seamless integration as part of a pipeline.\nThere are three levels of sophistication that users can choose from:\n\nusing the off-the-shelf clustering routines provided by gingado, which were selected to be applied cross various use cases;\nselecting an existing clustering routine from the scikit-learn.cluster module; or\ndesigning their own clustering algorithm.\n\n\nsource\n\nFindCluster\n\n FindCluster\n              (cluster_alg:[BaseEstimator,ClusterMixin]=AffinityPropagatio\n              n(), auto_document:ggdModelDocumentation=&lt;class\n              'gingado.model_documentation.ModelCard'&gt;,\n              random_state:int|None=None)\n\nRetain only the columns of X that are in the same cluster as y.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncluster_alg\n[BaseEstimator, ClusterMixin]\nAffinityPropagation()\nAn instance of the clustering algorithm to use\n\n\nauto_document\nggdModelDocumentation\nModelCard\ngingado Documenter template to facilitate model documentation\n\n\nrandom_state\nint | None\nNone\nThe random seed to be used by the algorithm, if relevant\n\n\n\n\nsource\n\nfit\n\n fit (X, y)\n\nFit FindCluster\n\n\n\n\nDetails\n\n\n\n\nX\nThe population of entities, organised in columns\n\n\ny\nThe entity of interest\n\n\n\n\nsource\n\n\ntransform\n\n transform (X)\n\nKeep only the entities in X that belong to the same cluster as y\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nX\n\nThe population of entities, organised in columns\n\n\nReturns\nnp.array\nColumns of X that are in the same cluster as y\n\n\n\n\nsource\n\n\nfit_transform\n\n fit_transform (X, y)\n\nFit a FindCluster object and keep only the entities in X that belong to the same cluster as y\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nX\n\nThe population of entities, organised in columns\n\n\ny\n\nThe entity of interest\n\n\nReturns\nnp.array\nColumns of X that are in the same cluster as y\n\n\n\n\nsource\n\n\ndocument\n\n document (documenter:Optional[gingado.model_documentation.ggdModelDocumen\n           tation]=None)\n\nDocument the FindCluster model using the template in documenter\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndocumenter\nggdModelDocumentation | None\nNone\nA gingado Documenter or the documenter set in auto_document if None.\n\n\n\n\n\nExample: finding similar countries\nThe Barro and Lee (1994) dataset is used to illustrate the use of FindCluster. It is a country-level dataset. Let’s use it to answer the following question: for some specific country, what other countries are the closest to it considering the data available?\nFirst, we import the data:\n\nfrom gingado.datasets import load_BarroLee_1994\n\nThe data is organized by rows: each row is a different country, and the variables are organised in columns.\nThe dataset is originally organised for a regression of GDP growth (here denoted y) on the covariates (X). This is not what we want to do in this case. So instead of keeping GDP as a separate variable, the next step is to include it in the X DataFrame.\n\nX, y = load_BarroLee_1994()\nX['gdp'] = y\nX.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\ngdpsh465\nbmp1l\nfreeop\nfreetar\nh65\nhm65\nhf65\np65\npm65\n...\nsyr65\nsyrm65\nsyrf65\nteapri65\nteasec65\nex1\nim1\nxr65\ntot1\ngdp\n\n\n\n\n0\n0\n6.591674\n0.2837\n0.153491\n0.043888\n0.007\n0.013\n0.001\n0.29\n0.37\n...\n0.033\n0.057\n0.010\n47.6\n17.3\n0.0729\n0.0667\n0.348\n-0.014727\n-0.024336\n\n\n1\n1\n6.829794\n0.6141\n0.313509\n0.061827\n0.019\n0.032\n0.007\n0.91\n1.00\n...\n0.173\n0.274\n0.067\n57.1\n18.0\n0.0940\n0.1438\n0.525\n0.005750\n0.100473\n\n\n2\n2\n8.895082\n0.0000\n0.204244\n0.009186\n0.260\n0.325\n0.201\n1.00\n1.00\n...\n2.573\n2.478\n2.667\n26.5\n20.7\n0.1741\n0.1750\n1.082\n-0.010040\n0.067051\n\n\n3\n3\n7.565275\n0.1997\n0.248714\n0.036270\n0.061\n0.070\n0.051\n1.00\n1.00\n...\n0.438\n0.453\n0.424\n27.8\n22.7\n0.1265\n0.1496\n6.625\n-0.002195\n0.064089\n\n\n4\n4\n7.162397\n0.1740\n0.299252\n0.037367\n0.017\n0.027\n0.007\n0.82\n0.85\n...\n0.257\n0.287\n0.229\n34.5\n17.6\n0.1211\n0.1308\n2.500\n0.003283\n0.027930\n\n\n\n\n5 rows × 63 columns\n\n\n\nNow we remove the first column (an identifier) and transpose the DataFrame, so that countries are organized in columns.\nEach country is identified by a number: 0, 1, …\n\nX = X.iloc[:, 1:]\ncountries = X.T\ncountries.columns = ['country_' + str(c) for c in countries.columns]\ncountries.head()\n\n\n\n\n\n\n\n\ncountry_0\ncountry_1\ncountry_2\ncountry_3\ncountry_4\ncountry_5\ncountry_6\ncountry_7\ncountry_8\ncountry_9\n...\ncountry_80\ncountry_81\ncountry_82\ncountry_83\ncountry_84\ncountry_85\ncountry_86\ncountry_87\ncountry_88\ncountry_89\n\n\n\n\ngdpsh465\n6.591674\n6.829794\n8.895082\n7.565275\n7.162397\n7.218910\n7.853605\n7.703910\n9.063463\n8.151910\n...\n9.030974\n8.995537\n8.234830\n8.332549\n8.645586\n8.991064\n8.025189\n9.030137\n8.865312\n8.912339\n\n\nbmp1l\n0.283700\n0.614100\n0.000000\n0.199700\n0.174000\n0.000000\n0.000000\n0.277600\n0.000000\n0.148400\n...\n0.000000\n0.000000\n0.036300\n0.000000\n0.000000\n0.000000\n0.005000\n0.000000\n0.000000\n0.000000\n\n\nfreeop\n0.153491\n0.313509\n0.204244\n0.248714\n0.299252\n0.258865\n0.182525\n0.215275\n0.109614\n0.110885\n...\n0.293138\n0.304720\n0.288405\n0.345485\n0.288440\n0.371898\n0.296437\n0.265778\n0.282939\n0.150366\n\n\nfreetar\n0.043888\n0.061827\n0.009186\n0.036270\n0.037367\n0.020880\n0.014385\n0.029713\n0.002171\n0.028579\n...\n0.005517\n0.011658\n0.011589\n0.006503\n0.005995\n0.014586\n0.013615\n0.008629\n0.005048\n0.024377\n\n\nh65\n0.007000\n0.019000\n0.260000\n0.061000\n0.017000\n0.023000\n0.039000\n0.024000\n0.402000\n0.145000\n...\n0.245000\n0.246000\n0.183000\n0.188000\n0.256000\n0.255000\n0.108000\n0.288000\n0.188000\n0.257000\n\n\n\n\n5 rows × 90 columns\n\n\n\nSuppose we are interested in country No 13. What other countries are similar to it?\nFirst, country No 13 needs to be carved out of the DataFrame with the other countries.\nSecond, we can now pass the larger DataFrame and country 13’s data separately to an instance of FindCluster.\n\ncountry_of_interest = countries.pop('country_13')\n\n\nsimilar = FindCluster(AffinityPropagation(convergence_iter=5000))\nsimilar\n\nFindCluster(cluster_alg=AffinityPropagation(convergence_iter=5000))\n\n\n\nsame_cluster = similar.fit_transform(X=countries, y=country_of_interest)\n\nassert same_cluster.equals(similar.fit(X=countries, y=country_of_interest).transform(X=countries))\n\nsame_cluster\n\n/Users/douglasaraujo/Coding/.venv_gingado/lib/python3.10/site-packages/sklearn/cluster/_affinity_propagation.py:236: ConvergenceWarning: Affinity propagation did not converge, this model may return degenerate cluster centers and labels.\n  warnings.warn(\n/Users/douglasaraujo/Coding/.venv_gingado/lib/python3.10/site-packages/sklearn/cluster/_affinity_propagation.py:236: ConvergenceWarning: Affinity propagation did not converge, this model may return degenerate cluster centers and labels.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\ncountry_2\ncountry_9\ncountry_41\ncountry_48\ncountry_49\ncountry_52\ncountry_60\ncountry_64\ncountry_66\n\n\n\n\ngdpsh465\n8.895082\n8.151910\n7.360740\n6.469250\n5.762051\n9.224933\n8.346168\n7.655864\n7.830028\n\n\nbmp1l\n0.000000\n0.148400\n0.418100\n0.538800\n0.600500\n0.000000\n0.319900\n0.134500\n0.488000\n\n\nfreeop\n0.204244\n0.110885\n0.218471\n0.153491\n0.151848\n0.204244\n0.110885\n0.164598\n0.136287\n\n\nfreetar\n0.009186\n0.028579\n0.027087\n0.043888\n0.024100\n0.009186\n0.028579\n0.044446\n0.046730\n\n\nh65\n0.260000\n0.145000\n0.032000\n0.015000\n0.002000\n0.393000\n0.272000\n0.080000\n0.146000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nex1\n0.174100\n0.052400\n0.190500\n0.069200\n0.148400\n0.255800\n0.062500\n0.052500\n0.076400\n\n\nim1\n0.175000\n0.052300\n0.225700\n0.074800\n0.186400\n0.241200\n0.057800\n0.057200\n0.086600\n\n\nxr65\n1.082000\n2.119000\n3.949000\n0.348000\n7.367000\n1.017000\n36.603000\n30.929000\n40.500000\n\n\ntot1\n-0.010040\n0.007584\n0.205768\n0.035226\n0.007548\n0.018636\n0.014286\n-0.004592\n-0.007018\n\n\ngdp\n0.067051\n0.039147\n0.016775\n-0.048712\n0.024477\n0.050757\n-0.034045\n0.046010\n-0.011384\n\n\n\n\n62 rows × 9 columns\n\n\n\nThe default clustering algorithm used by FindCluster is affinity propagation (Frey and Dueck 2007). It is the algorithm of choice because of it combines several desireable characteristics, in particular: - the number of clusters is data-driven instad of set by the user, - the number of entities in each cluster is also chosen by the model, - all entities are part of a cluster, and - each cluster might have a different number of entities.\nHowever, we may want to try different clustering algorithms. Let’s compare the result above with the same analyses using DBSCAN (Ester et al. 1996).\n\nfrom sklearn.cluster import DBSCAN\n\n\nsimilar_dbscan = FindCluster(cluster_alg=DBSCAN())\nsimilar_dbscan\n\nFindCluster(cluster_alg=DBSCAN())\n\n\n\nsame_cluster_dbscan = similar_dbscan.fit_transform(X=countries, y=country_of_interest)\n\nassert same_cluster_dbscan.equals(similar_dbscan.fit(X=countries, y=country_of_interest).transform(X=countries))\n\nsame_cluster_dbscan\n\n\n\n\n\n\n\n\ncountry_0\ncountry_1\ncountry_2\ncountry_3\ncountry_4\ncountry_5\ncountry_6\ncountry_7\ncountry_8\ncountry_9\n...\ncountry_80\ncountry_81\ncountry_82\ncountry_83\ncountry_84\ncountry_85\ncountry_86\ncountry_87\ncountry_88\ncountry_89\n\n\n\n\ngdpsh465\n6.591674\n6.829794\n8.895082\n7.565275\n7.162397\n7.218910\n7.853605\n7.703910\n9.063463\n8.151910\n...\n9.030974\n8.995537\n8.234830\n8.332549\n8.645586\n8.991064\n8.025189\n9.030137\n8.865312\n8.912339\n\n\nbmp1l\n0.283700\n0.614100\n0.000000\n0.199700\n0.174000\n0.000000\n0.000000\n0.277600\n0.000000\n0.148400\n...\n0.000000\n0.000000\n0.036300\n0.000000\n0.000000\n0.000000\n0.005000\n0.000000\n0.000000\n0.000000\n\n\nfreeop\n0.153491\n0.313509\n0.204244\n0.248714\n0.299252\n0.258865\n0.182525\n0.215275\n0.109614\n0.110885\n...\n0.293138\n0.304720\n0.288405\n0.345485\n0.288440\n0.371898\n0.296437\n0.265778\n0.282939\n0.150366\n\n\nfreetar\n0.043888\n0.061827\n0.009186\n0.036270\n0.037367\n0.020880\n0.014385\n0.029713\n0.002171\n0.028579\n...\n0.005517\n0.011658\n0.011589\n0.006503\n0.005995\n0.014586\n0.013615\n0.008629\n0.005048\n0.024377\n\n\nh65\n0.007000\n0.019000\n0.260000\n0.061000\n0.017000\n0.023000\n0.039000\n0.024000\n0.402000\n0.145000\n...\n0.245000\n0.246000\n0.183000\n0.188000\n0.256000\n0.255000\n0.108000\n0.288000\n0.188000\n0.257000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nex1\n0.072900\n0.094000\n0.174100\n0.126500\n0.121100\n0.063400\n0.034200\n0.086400\n0.059400\n0.052400\n...\n0.166200\n0.259700\n0.104400\n0.286600\n0.129600\n0.440700\n0.166900\n0.323800\n0.184500\n0.187600\n\n\nim1\n0.066700\n0.143800\n0.175000\n0.149600\n0.130800\n0.076200\n0.042800\n0.093100\n0.046000\n0.052300\n...\n0.161700\n0.228800\n0.179600\n0.350000\n0.145800\n0.425700\n0.220100\n0.313400\n0.194000\n0.200700\n\n\nxr65\n0.348000\n0.525000\n1.082000\n6.625000\n2.500000\n1.000000\n12.499000\n7.000000\n1.000000\n2.119000\n...\n4.286000\n2.460000\n32.051000\n0.452000\n652.850000\n2.529000\n25.553000\n4.152000\n0.452000\n0.886000\n\n\ntot1\n-0.014727\n0.005750\n-0.010040\n-0.002195\n0.003283\n-0.001747\n0.009092\n0.011630\n0.008169\n0.007584\n...\n-0.006642\n-0.003241\n-0.034352\n-0.001660\n-0.046278\n-0.011883\n-0.039080\n0.005175\n-0.029551\n-0.036482\n\n\ngdp\n-0.024336\n0.100473\n0.067051\n0.064089\n0.027930\n0.046407\n0.067332\n0.020978\n0.033551\n0.039147\n...\n0.038095\n0.034213\n0.052759\n0.038416\n0.031895\n0.031196\n0.034096\n0.046900\n0.039773\n0.040642\n\n\n\n\n62 rows × 89 columns\n\n\n\nAs illustrated above, the results can be quite different. In this case, affinity propagation converged to more tightly defined clusters, while DBSCAN selected a cluster that contains almost all other countries (therefore, not useful in this particular case).\nNote that model documentation is already jumpstarted when the cluster is fit. A glimpse of the current template, including the questions in the documentation template that have been automatically filled, are shown below.\n\nsimilar.model_documentation.show_json()\n\n{'model_details': {'developer': 'Person or organisation developing the model',\n  'datetime': '2023-09-16 01:43:45 ',\n  'version': 'Model version',\n  'type': 'Model type',\n  'info': {'_estimator_type': 'clusterer',\n   'affinity_matrix_': array([[-0.00000000e+00, -5.97375771e+07, -5.35974361e+07, ...,\n           -1.92434215e+09, -8.60822083e+07, -3.77976931e+07],\n          [-5.97375771e+07, -0.00000000e+00, -2.26471602e+08, ...,\n           -2.66217555e+09, -2.43057326e+06, -1.92555486e+08],\n          [-5.35974361e+07, -2.26471602e+08, -0.00000000e+00, ...,\n           -1.33575671e+09, -2.75395788e+08, -1.37934978e+06],\n          ...,\n          [-1.92434215e+09, -2.66217555e+09, -1.33575671e+09, ...,\n           -0.00000000e+00, -2.82418157e+09, -1.42280304e+09],\n          [-8.60822083e+07, -2.43057326e+06, -2.75395788e+08, ...,\n           -2.82418157e+09, -0.00000000e+00, -2.37881124e+08],\n          [-3.77976931e+07, -1.92555486e+08, -1.37934978e+06, ...,\n           -1.42280304e+09, -2.37881124e+08, -0.00000000e+00]]),\n   'cluster_centers_': array([[ 6.82979374e+00,  6.14100000e-01,  3.13509000e-01, ...,\n            5.25000000e-01,  5.75000000e-03,  1.00472567e-01],\n          [ 8.89508153e+00,  0.00000000e+00,  2.04244000e-01, ...,\n            1.08200000e+00, -1.00400000e-02,  6.70514822e-02],\n          [ 7.56527528e+00,  1.99700000e-01,  2.48714000e-01, ...,\n            6.62500000e+00, -2.19500000e-03,  6.40891662e-02],\n          ...,\n          [ 8.33254894e+00,  0.00000000e+00,  3.45485000e-01, ...,\n            4.52000000e-01, -1.66000000e-03,  3.84156381e-02],\n          [ 8.86531163e+00,  0.00000000e+00,  2.82939000e-01, ...,\n            4.52000000e-01, -2.95510000e-02,  3.97733722e-02],\n          [ 8.91233857e+00,  0.00000000e+00,  1.50366000e-01, ...,\n            8.86000000e-01, -3.64820000e-02,  4.06415381e-02]]),\n   'cluster_centers_indices_': array([ 1,  2,  3,  4,  5,  7,  8, 10, 13, 14, 16, 18, 19, 25, 27, 32, 35,\n          39, 42, 45, 46, 49, 50, 52, 53, 55, 57, 58, 60, 62, 67, 68, 69, 71,\n          76, 82, 87, 88]),\n   'feature_names_in_': array(['gdpsh465', 'bmp1l', 'freeop', 'freetar', 'h65', 'hm65', 'hf65',\n          'p65', 'pm65', 'pf65', 's65', 'sm65', 'sf65', 'fert65', 'mort65',\n          'lifee065', 'gpop1', 'fert1', 'mort1', 'invsh41', 'geetot1',\n          'geerec1', 'gde1', 'govwb1', 'govsh41', 'gvxdxe41', 'high65',\n          'highm65', 'highf65', 'highc65', 'highcm65', 'highcf65', 'human65',\n          'humanm65', 'humanf65', 'hyr65', 'hyrm65', 'hyrf65', 'no65',\n          'nom65', 'nof65', 'pinstab1', 'pop65', 'worker65', 'pop1565',\n          'pop6565', 'sec65', 'secm65', 'secf65', 'secc65', 'seccm65',\n          'seccf65', 'syr65', 'syrm65', 'syrf65', 'teapri65', 'teasec65',\n          'ex1', 'im1', 'xr65', 'tot1', 'gdp'], dtype=object),\n   'labels_': array([29,  0,  1,  2,  3,  4, 18,  5,  6,  1,  7, 30, 14,  8,  9, 29, 10,\n          29, 11, 12, 12, 18, 29, 36, 18, 13, 18, 14, 29, 36, 36, 14, 15, 36,\n          29, 16, 18, 14, 36, 17,  1, 14, 18, 29, 29, 19, 20,  1,  1, 21, 22,\n           1, 23, 24, 21, 25, 36, 26, 27,  1, 28, 12, 29,  1, 14,  1, 29, 30,\n          31, 32, 12, 33, 18, 29, 30, 18, 34, 14, 18, 36, 36, 29, 35, 36, 29,\n          29, 14, 36, 37,  1]),\n   'n_features_in_': 62,\n   'n_iter_': 200},\n  'paper': 'Paper or other resource for more information',\n  'citation': 'Citation details',\n  'license': 'License',\n  'contact': 'Where to send questions or comments about the model'},\n 'intended_use': {'primary_uses': 'Primary intended uses',\n  'primary_users': 'Primary intended users',\n  'out_of_scope': 'Out-of-scope use cases'},\n 'factors': {'relevant': 'Relevant factors',\n  'evaluation': 'Evaluation factors'},\n 'metrics': {'performance_measures': 'Model performance measures',\n  'thresholds': 'Decision thresholds',\n  'variation_approaches': 'Variation approaches'},\n 'evaluation_data': {'datasets': 'Datasets',\n  'motivation': 'Motivation',\n  'preprocessing': 'Preprocessing'},\n 'training_data': {'training_data': 'Information on training data'},\n 'quant_analyses': {'unitary': 'Unitary results',\n  'intersectional': 'Intersectional results'},\n 'ethical_considerations': {'sensitive_data': 'Does the model use any sensitive data (e.g., protected classes)?',\n  'human_life': 'Is the model intended to inform decisions about matters central to human life or flourishing - e.g., health or safety? Or could it be used in such a way?',\n  'mitigations': 'What risk mitigation strategies were used during model development?',\n  'risks_and_harms': 'What risks may be present in model usage? Try to identify the potential recipients,likelihood, and magnitude of harms. If these cannot be determined, note that they were considered but remain unknown',\n  'use_cases': 'Are there any known model use cases that are especially fraught?',\n  'additional_information': 'If possible, this section should also include any additional ethical considerations that went into model development, for example, review by an external board, or testing with a specific community.'},\n 'caveats_recommendations': {'caveats': 'For example, did the results suggest any further testing? Were there any relevant groups that were not represented in the evaluation dataset?',\n  'recommendations': 'Are there additional recommendations for model use? What are the ideal characteristics of an evaluation dataset for this model?'}}\n\n\nFindCluster can also be used as part of a pipeline. In this case, only the entities in the same cluster as the entity of interest will continue on to the next steps of the estimation.\n\nfrom gingado.benchmark import RegressionBenchmark\nfrom sklearn.pipeline import Pipeline\n\n\npipe = Pipeline([\n    ('cluster', FindCluster(AffinityPropagation(convergence_iter=5000))),\n    ('rf', RegressionBenchmark())\n])\n\n\npipe.fit(X=countries, y=country_of_interest)\n\n/Users/douglasaraujo/Coding/.venv_gingado/lib/python3.10/site-packages/sklearn/cluster/_affinity_propagation.py:236: ConvergenceWarning: Affinity propagation did not converge, this model may return degenerate cluster centers and labels.\n  warnings.warn(\n\n\nPipeline(steps=[('cluster',\n                 FindCluster(cluster_alg=AffinityPropagation(convergence_iter=5000))),\n                ('rf',\n                 RegressionBenchmark(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None)))])"
  },
  {
    "objectID": "estimators.html#comparative-case-studies",
    "href": "estimators.html#comparative-case-studies",
    "title": "Estimators",
    "section": "Comparative case studies",
    "text": "Comparative case studies\n\nsource\n\nMachineControl\n\n MachineControl\n                 (cluster_alg:[BaseEstimator,ClusterMixin]|None=AffinityPr\n                 opagation(),\n                 estimator:BaseEstimator=RegressionBenchmark(),\n                 manifold:BaseEstimator=TSNE(), with_placebo:bool=True,\n                 auto_document:ggdModelDocumentation=&lt;class\n                 'gingado.model_documentation.ModelCard'&gt;,\n                 random_state:int|None=None)\n\nSynthetic controls with machine learning methods\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncluster_alg\n[BaseEstimator, ClusterMixin] | None\nAffinityPropagation()\nAn instance of the clustering algorithm to use, or None to retain all entities\n\n\nestimator\nBaseEstimator\nRegressionBenchmark()\nMethod to weight the control entities\n\n\nmanifold\nBaseEstimator\nTSNE()\nAlgorithm for manifold learning\n\n\nwith_placebo\nbool\nTrue\nInclude placebo estimations during prediction?\n\n\nauto_document\nggdModelDocumentation\nModelCard\ngingado Documenter template to facilitate model documentation\n\n\nrandom_state\nint | None\nNone\nThe random seed to be used by the algorithm, if relevant\n\n\n\n\nsource\n\nfit\n\n fit (X:pandas.core.frame.DataFrame,\n      y:Union[pandas.core.frame.DataFrame,pandas.core.series.Series])\n\nFit the MachineControl model\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nX\npd.DataFrame\nA pandas DataFrame with pre-intervention data of shape (n_samples, n_control_entites)\n\n\ny\npd.DataFrame | pd.Series\nA pandas DataFrame or Series with pre-intervention data of shape (n_samples,)\n\n\n\n\nsource\n\n\npredict\n\n predict (X:pandas.core.frame.DataFrame,\n          y:Union[pandas.core.frame.DataFrame,pandas.core.series.Series])\n\nCalculate the model predictions before and after the intervention\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nX\npd.DataFrame\nA pandas DataFrame with complete time series (pre- and post-intervention) of shape (n_samples, n_control_entites)\n\n\ny\npd.DataFrame | pd.Series\nA pandas DataFrame or Series with complete time series of shape (n_samples,)\n\n\n\n\nsource\n\n\nget_controls\n\n get_controls ()\n\nGet the list of control entities\n\nsource\n\n\ndocument\n\n document (documenter:Optional[gingado.model_documentation.ggdModelDocumen\n           tation]=None)\n\nDocument the MachineControl model using the template in documenter\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndocumenter\nggdModelDocumentation | None\nNone\nA gingado Documenter or the documenter set in auto_document if None.\n\n\n\n\n\nBrief econometric description\nThe goal of MachineControl is to estimate:\n\\[\n\\tau_t = Y_{1, t}^{I} - Y_{1, t}^{N}, t &gt; T0\n\\]\nwhere:\n\n\\(\\tau\\) is the effect on entity \\(i=1\\) of the intervention of interest\nwithout loss of generality, \\(i=1\\) is an entity that has undergone the intervention of interest, amongst \\(N\\) total entities\ntime period \\(T0\\) is a date in which the intervention occurred\nsuperscript \\(I\\) in an outcome variable denotes the occurence of the intervention, whereas superscript \\(N\\) is absence of intervention\nfor \\(t &gt; T0\\), \\(Y_{i, t}^{I}\\) is observed while \\(Y_{i, t}^{N}\\) must be estimated because it is a counterfacual.\n\n\\(Y_{i, t}^{N}\\) is calculated from the values of the other entities, \\(i \\neq 1\\). Collect this data in a vector \\(\\mathbb{Y}_{-1, t}^{N}\\). Then, following Doudchenko and Imbens (2016):\n\\[\n\\hat{Y}_{i, t}^{N} = f^*(\\mathbb{Y}_{-1, t}^{N}),\n\\]\nwith the star (\\(*\\)) superscript on the function \\(f(\\cdot)\\) representing that it was trained only with data up until the intervention date. The exact form of \\(f(\\cdot)\\) depends on the argument estimator. A general use estimator is the random forest (Breiman 2001).\nThe panel data itself might be the whole population in the data, or a subset when using the whole population might be too cumbersome to run analyses (eg, if the data contains too many entities). One way to select this subsample of control units without including subjective judgment in the data is quantitatilve. The control units are selected through a clustering algorithm (argument cluster_arg). One cluster algorithm that can be used is affinity propagation (Frey and Dueck 2007).\nTo finalise, the quality of the synthetic control can be assessed in many ways. One fully data-driven way to achieve this is by using manifold learning: lower-dimensional embeddings of a higher-dimensional data. A preferred manifold learning algorithm is t-SNE (Van der Maaten and Hinton 2008).\nThe relative distance between embeddings and the target centre, as well as the control and the target, represent the chance that a better feasible control (either from real or combined) will materialise. The intuition behind this test is:\n\nlet \\(d_{i,j}\\) be the Euclidean distance between the embeddings (2d points) of entities \\(i\\) and \\(j\\)\nif only a very small percentage of \\(d_{1, j \\in (2, ..., N)}\\) are lower than \\(d_{1, \\text{Synthetic control}}\\), than the synthetic control produced with \\(f(\\cdot)\\) is indeed a formula that provides one of the best alternative.\n\nMain references:\n\nAbadie and Gardeazabal (2003)\nAbadie, Diamond, and Hainmueller (2010)\nAbadie, Diamond, and Hainmueller (2015)\nDoudchenko and Imbens (2016)\nAbadie (2021)\n\n\n\nExample: impact of labour reform on productivity\nSee Machine controls: Synthetic controls with machine learning."
  },
  {
    "objectID": "datasets.html",
    "href": "datasets.html",
    "title": "Datasets for economic research",
    "section": "",
    "text": "source\n\n\n\n load_BarroLee_1994 (return_tuple:bool=True)\n\nDataset used in R Barro and J-W Lee’s Sources of Economic Growth (1994)\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nreturn_tuple\nbool\nTrue\nWhether to return the data in a tuple or jointly in a single pandas DataFrame\n\n\n\nRobert Barro and Jong-Wha Lee’s (1994) dataset has been used over time by other economists, such as by Belloni, Chernozhukov, and Hansen (2011) and Giannone, Lenza, and Primiceri (2021). This function uses the version available in their online annex. In that paper, this dataset corresponds to what the authors call “macro2”.\nThe original data, along with more information on the variables, can be found in this NBER website. A very helpful codebook is found in this repo.\n\nX, y = load_BarroLee_1994()\nX.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\ngdpsh465\nbmp1l\nfreeop\nfreetar\nh65\nhm65\nhf65\np65\npm65\n...\nseccf65\nsyr65\nsyrm65\nsyrf65\nteapri65\nteasec65\nex1\nim1\nxr65\ntot1\n\n\n\n\n0\n0\n6.591674\n0.2837\n0.153491\n0.043888\n0.007\n0.013\n0.001\n0.29\n0.37\n...\n0.04\n0.033\n0.057\n0.010\n47.6\n17.3\n0.0729\n0.0667\n0.348\n-0.014727\n\n\n1\n1\n6.829794\n0.6141\n0.313509\n0.061827\n0.019\n0.032\n0.007\n0.91\n1.00\n...\n0.64\n0.173\n0.274\n0.067\n57.1\n18.0\n0.0940\n0.1438\n0.525\n0.005750\n\n\n2\n2\n8.895082\n0.0000\n0.204244\n0.009186\n0.260\n0.325\n0.201\n1.00\n1.00\n...\n18.14\n2.573\n2.478\n2.667\n26.5\n20.7\n0.1741\n0.1750\n1.082\n-0.010040\n\n\n3\n3\n7.565275\n0.1997\n0.248714\n0.036270\n0.061\n0.070\n0.051\n1.00\n1.00\n...\n2.63\n0.438\n0.453\n0.424\n27.8\n22.7\n0.1265\n0.1496\n6.625\n-0.002195\n\n\n4\n4\n7.162397\n0.1740\n0.299252\n0.037367\n0.017\n0.027\n0.007\n0.82\n0.85\n...\n2.11\n0.257\n0.287\n0.229\n34.5\n17.6\n0.1211\n0.1308\n2.500\n0.003283\n\n\n\n\n5 rows × 62 columns\n\n\n\n\ny.plot.hist(title='GDP growth', bins=30)\n\n&lt;AxesSubplot:title={'center':'GDP growth'}, ylabel='Frequency'&gt;"
  },
  {
    "objectID": "datasets.html#real-datasets",
    "href": "datasets.html#real-datasets",
    "title": "Datasets for economic research",
    "section": "",
    "text": "source\n\n\n\n load_BarroLee_1994 (return_tuple:bool=True)\n\nDataset used in R Barro and J-W Lee’s Sources of Economic Growth (1994)\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nreturn_tuple\nbool\nTrue\nWhether to return the data in a tuple or jointly in a single pandas DataFrame\n\n\n\nRobert Barro and Jong-Wha Lee’s (1994) dataset has been used over time by other economists, such as by Belloni, Chernozhukov, and Hansen (2011) and Giannone, Lenza, and Primiceri (2021). This function uses the version available in their online annex. In that paper, this dataset corresponds to what the authors call “macro2”.\nThe original data, along with more information on the variables, can be found in this NBER website. A very helpful codebook is found in this repo.\n\nX, y = load_BarroLee_1994()\nX.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\ngdpsh465\nbmp1l\nfreeop\nfreetar\nh65\nhm65\nhf65\np65\npm65\n...\nseccf65\nsyr65\nsyrm65\nsyrf65\nteapri65\nteasec65\nex1\nim1\nxr65\ntot1\n\n\n\n\n0\n0\n6.591674\n0.2837\n0.153491\n0.043888\n0.007\n0.013\n0.001\n0.29\n0.37\n...\n0.04\n0.033\n0.057\n0.010\n47.6\n17.3\n0.0729\n0.0667\n0.348\n-0.014727\n\n\n1\n1\n6.829794\n0.6141\n0.313509\n0.061827\n0.019\n0.032\n0.007\n0.91\n1.00\n...\n0.64\n0.173\n0.274\n0.067\n57.1\n18.0\n0.0940\n0.1438\n0.525\n0.005750\n\n\n2\n2\n8.895082\n0.0000\n0.204244\n0.009186\n0.260\n0.325\n0.201\n1.00\n1.00\n...\n18.14\n2.573\n2.478\n2.667\n26.5\n20.7\n0.1741\n0.1750\n1.082\n-0.010040\n\n\n3\n3\n7.565275\n0.1997\n0.248714\n0.036270\n0.061\n0.070\n0.051\n1.00\n1.00\n...\n2.63\n0.438\n0.453\n0.424\n27.8\n22.7\n0.1265\n0.1496\n6.625\n-0.002195\n\n\n4\n4\n7.162397\n0.1740\n0.299252\n0.037367\n0.017\n0.027\n0.007\n0.82\n0.85\n...\n2.11\n0.257\n0.287\n0.229\n34.5\n17.6\n0.1211\n0.1308\n2.500\n0.003283\n\n\n\n\n5 rows × 62 columns\n\n\n\n\ny.plot.hist(title='GDP growth', bins=30)\n\n&lt;AxesSubplot:title={'center':'GDP growth'}, ylabel='Frequency'&gt;"
  },
  {
    "objectID": "datasets.html#simulated-datasets",
    "href": "datasets.html#simulated-datasets",
    "title": "Datasets for economic research",
    "section": "Simulated datasets",
    "text": "Simulated datasets\n\n\n\n\n\n\nNote\n\n\n\nAll of the functions creating simulated datasets have a parameter random_state that allow for reproducible random numbers.\n\n\n\nsource\n\nmake_causal_effect\n\n make_causal_effect (n_samples:int=100, n_features:int=100,\n                     pretreatment_outcome=&lt;function &lt;lambda&gt;&gt;,\n                     treatment_propensity=&lt;function &lt;lambda&gt;&gt;,\n                     treatment_assignment=&lt;function &lt;lambda&gt;&gt;,\n                     treatment=&lt;function &lt;lambda&gt;&gt;,\n                     treatment_effect=&lt;function &lt;lambda&gt;&gt;, bias:float=0,\n                     noise:float=0, random_state=None,\n                     return_propensity:bool=False,\n                     return_assignment:bool=False,\n                     return_treatment_value:bool=False,\n                     return_treatment_effect:bool=True,\n                     return_pretreatment_y:bool=False,\n                     return_as_dict:bool=False)\n\nSimulated dataset with causal effects from treatment\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_samples\nint\n100\nNumber of observations\n\n\nn_features\nint\n100\nNumber of covariates\n\n\npretreatment_outcome\nfunction\n\nFunction that generates the outcome variable before any treatment effects\n\n\ntreatment_propensity\nfunction\n\nNumber between 0 and 1, or function that generates a treatment propensity for each observation\n\n\ntreatment_assignment\nfunction\n\nFunction that controls how treatment propensities actually result in observations being treated\n\n\ntreatment\nfunction\n\nFunction that determines the magnitude of the treatment for each observation, conditional on assignment\n\n\ntreatment_effect\nfunction\n\nFunction that calculates the effect of a treatment to each treated observation\n\n\nbias\nfloat\n0\nThe value of the constant\n\n\nnoise\nfloat\n0\nIf 0, the pretreatment value of the overview does not include a random term. If &gt; 0, a random draw of the normal distribution with scale noise is drawn\n\n\nrandom_state\nNoneType\nNone\nSeed for the random number generator\n\n\nreturn_propensity\nbool\nFalse\nWhether the treatment propensity of each observation is to be retuned\n\n\nreturn_assignment\nbool\nFalse\nWhether the treatment assignment status of each observation is to be retuned\n\n\nreturn_treatment_value\nbool\nFalse\nWhether the treatment value of each observation is to be retuned\n\n\nreturn_treatment_effect\nbool\nTrue\nWhether the treatment effect of each observation is to be retuned\n\n\nreturn_pretreatment_y\nbool\nFalse\nWhether the outcome variable of each observation before the inclusion of treatment effects is to be retuned\n\n\nreturn_as_dict\nbool\nFalse\nWhether the results are returned as a list (False) or as a dictionary (True)\n\n\n\nmake_causal_effect creates a dataset for when the question of interest is related to the causal effects of a treatment. For example, for a simulated dataset, we can check that \\(Y_i\\) corresponds to the sum of the treatment effects plus the component that does not depend on the treatment:\n\ncausal_sim = make_causal_effect(\n    n_samples=2000,\n    n_features=100,\n    return_propensity=True,\n    return_treatment_effect=True, \n    return_pretreatment_y=True, \n    return_as_dict=True)\n\n assert not np.any(np.round(causal_sim['y'] - causal_sim['pretreatment_y'] - causal_sim['treatment_effect'], decimals=13))\n\n\nPre-treatment outcome\nThe pre-treatment outcome \\(Y_i|X_i\\) (the part of the outcome variable that is not dependent on the treatment) might be defined by the user. This corresponds to the value of the outcome for any untreated observations. The function should always take at least two arguments: X and bias, even if one of them is unused; bias is the constant. The argument is zero by default but can be set by the user to be another value.\n\ncausal_sim = make_causal_effect(\n    bias=0.123,\n    pretreatment_outcome=lambda X, bias: bias,\n    return_assignment=True,\n    return_as_dict=True\n)\n\nassert all(causal_sim['y'][causal_sim['treatment_assignment'] == 0] == 0.123)\n\nIf the outcome depends on specific columns of \\(X\\), this can be implemented as shown below.\n\ncausal_sim = make_causal_effect(\n    pretreatment_outcome=lambda X, bias: X[:, 1] + np.maximum(X[:,2], 0) + X[:,3] * X[:,4] + bias\n)\n\nAnd of course, the outcome might also have a random component.\nIn these cases (and in other parts of this function), when the user wants to use the same random number generator as the other parts of the function, the function must have an argment rng for the NumPy random number generator used in other parts of the function.\n\ncausal_sim_1 = make_causal_effect(\n    pretreatment_outcome=lambda X, bias, rng: X[:, 1] + np.maximum(X[:,2], 0) + X[:,3] * X[:,4] + bias + rng.standard_normal(size=X.shape[0]),\n    random_state=42,\n    return_pretreatment_y=True,\n    return_as_dict=True\n)\n\ncausal_sim_2 = make_causal_effect(\n    pretreatment_outcome=lambda X, bias, rng: X[:, 1] + np.maximum(X[:,2], 0) + X[:,3] * X[:,4] + bias + rng.standard_normal(size=X.shape[0]),\n    random_state=42,\n    return_pretreatment_y=True,\n    return_as_dict=True\n)\n\nassert all(causal_sim_1['X'].reshape(-1, 1) == causal_sim_2['X'].reshape(-1, 1))\nassert all(causal_sim_1['y'] == causal_sim_2['y'])\nassert all(causal_sim_1['pretreatment_y'] == causal_sim_2['pretreatment_y'])\n\n\n\nTreatment propensity\nThe treatment propensity of observations may all be the same, in which case treatment_propensity is a floating number between 0 and 1.\n\nsame_propensity_sim = make_causal_effect(\n    n_samples=485,\n    treatment_propensity=0.3,\n    return_propensity=True,\n    return_as_dict=True\n)\n\nassert np.unique(same_propensity_sim['propensity']) == 0.3\nassert len(same_propensity_sim['propensity']) == 485\n\nOr it might depend on the observation’s covariates, with the user passing a function with an argument ‘X’.\n\nheterogenous_propensities_sim = make_causal_effect(\n    n_samples=1000,\n    treatment_propensity=lambda X: 0.3 + (X[:, 0] &gt; 0) * 0.2,\n    return_propensity=True,\n    return_as_dict=True\n)\n\nplt.title(\"Heterogenously distributed propensities\")\nplt.xlabel(\"Propensity\")\nplt.ylabel(\"No of observations\")\nplt.hist(heterogenous_propensities_sim['propensity'], bins=100)\nplt.show()\n\n\n\n\nThe propensity can also be randomly allocated, together with covariate dependence or not. Note that even if the propensity is completely random and does not depend on covariates, the function must still use the argument X to calculate a random vector with the appropriate size.\n\nrandom_propensities_sim = make_causal_effect(\n    n_samples=50000,\n    treatment_propensity=lambda X: np.random.uniform(size=X.shape[0]),\n    return_propensity=True,\n    return_as_dict=True\n)\n\nplt.title(\"Randomly distributed propensities\")\nplt.xlabel(\"Propensity\")\nplt.ylabel(\"No of observations\")\nplt.hist(random_propensities_sim['propensity'], bins=100)\nplt.show()\n\n\n\n\n\n\nTreatment assignment\nAs seen above, every observation has a given treatment propensity - the chance that they are treated. Users can define how this propensity translates into actual treatment with the argument treatment_assignment. This argument takes a function, which must have an argument called propensity.\nThe default value for this argument is a function returning 1s with probability propensity and 0s otherwise. Any other function should always return either 0s or 1s for the data simulator to work as expected.\n\ncausal_sim = make_causal_effect(\n    treatment_assignment=lambda propensity: np.random.binomial(1, propensity)\n)\n\nWhile the case above is likely to be the most useful in practice, this argument accepts more complex relationships between an observation’s propensity and the actual treatment assignment.\nFor example, if treatment is subject to rationing, then one could simulate data with 10 observations where only the samples with the highest (say, 3) propensity scores get treated, as below:\n\nrationed_treatment_sim = make_causal_effect(\n    n_samples=10,\n    treatment_propensity=lambda X: np.random.uniform(size=X.shape[0]),\n    treatment_assignment=lambda propensity: propensity &gt;= propensity[np.argsort(propensity)][-3],\n    return_propensity=True,\n    return_assignment=True,\n    return_as_dict=True\n)\n\n\nrationed_treatment = pd.DataFrame(\n    np.column_stack((rationed_treatment_sim['propensity'], rationed_treatment_sim['treatment_assignment'])),\n    columns = ['propensity', 'assignment']\n    )\n\n\nrationed_treatment.sort_values('propensity')\n\n\n\n\n\n\n\n\npropensity\nassignment\n\n\n\n\n5\n0.090696\n0.0\n\n\n0\n0.142949\n0.0\n\n\n7\n0.184452\n0.0\n\n\n8\n0.209229\n0.0\n\n\n6\n0.237856\n0.0\n\n\n3\n0.289326\n0.0\n\n\n9\n0.471596\n0.0\n\n\n4\n0.861327\n1.0\n\n\n1\n0.969388\n1.0\n\n\n2\n0.982423\n1.0\n\n\n\n\n\n\n\n\n\nTreatment value\nThe treatment argument indicates the magnitude of the treatment for each observation assigned for treatment. Its value is always a function that must have an argument called assignment, as in the first example below.\nIn the simplest case, the treatment is a binary variable indicating whether or not a variable was treated. In other words, the treatment is the same as the assignment, as in the default value.\nBut users can also simulate data with heterogenous treatment, conditional on assignment. This is done by including a pararemeter X in the function, as shown in the second example below.\n\nbinary_treatment_sim = make_causal_effect(\n    n_samples=15,\n    treatment=lambda assignment: assignment,\n    return_assignment=True,\n    return_treatment_value=True,\n    return_as_dict=True\n)\n\nassert sum(binary_treatment_sim['treatment_assignment'] - binary_treatment_sim['treatment_value'][0]) == 0\n\nHeterogenous treatments may occur in settings where treatment intensity, conditional on assignment, varies across observations. Please note the following:\n\nthe heterogenous treatment amount may or may not depend on covariates, but either way, if treatment values are heterogenous, then X needs to be an argument of the function passed to treatment, if nothing else to make sure the shapes match; and\nif treatments are heterogenous, then it is important to multiply the treatment value with the assignment argument to ensure that observations that are not assigned to be treated are indeed not treated (the function will return an AssertionError otherwise).\n\n\nhetereogenous_treatment_sim = make_causal_effect(\n    n_samples=15,\n    treatment=lambda assignment, X: assignment * np.random.uniform(size=X.shape[0]),\n    return_assignment=True,\n    return_treatment_value=True,\n    return_as_dict=True\n)\n\nIn contrast to the function above, in the chunk below the function make_causal_effect fails because a treatment value is also assigned to observations that were not assigned for treatment.\n\ntest_fail(\n    make_causal_effect, \n    kwargs=dict(treatment=lambda assignment, X: assignment + np.random.uniform(size=X.shape[0]))\n)\n\n\n\nTreatment effect\nThe treatment effect can be homogenous, ie, is doesn’t depend on any other characteristic of the individual observations (in other words, does not depend on \\(X_i\\)), or heterogenous (where the treatment effect on \\(Y_i\\) does depend on each observation’s \\(X_i\\)). This can be done by specifying the causal relationship through a lambda function, as below:\n\nhomogenous_effects_sim = make_causal_effect(\n        treatment_effect=lambda treatment_value: treatment_value,\n        return_treatment_value=True,\n        return_as_dict=True\n)\n\nassert (homogenous_effects_sim['treatment_effect'] == homogenous_effects_sim['treatment_value']).all()\n\nheterogenous_effects_sim = make_causal_effect(\n        treatment_effect=lambda treatment_value, X: np.maximum(X[:, 1], 0) * treatment_value,\n        return_treatment_value=True,\n        return_as_dict=True\n)\n\nassert (heterogenous_effects_sim['treatment_effect'] != heterogenous_effects_sim['treatment_value']).any()"
  },
  {
    "objectID": "datasets.html#references",
    "href": "datasets.html#references",
    "title": "Datasets for economic research",
    "section": "References",
    "text": "References\n\n\nBarro, Robert J., and Jong-Wha Lee. 1994. “Sources of Economic Growth.” Carnegie-Rochester Conference Series on Public Policy 40: 1–46. https://doi.org/10.1016/0167-2231(94)90002-7.\n\n\nBelloni, Alexandre, Victor Chernozhukov, and Christian Hansen. 2011. “Inference for High-Dimensional Sparse Econometric Models.” arXiv Preprint arXiv:1201.0220.\n\n\nGiannone, Domenico, Michele Lenza, and Giorgio E Primiceri. 2021. “Economic Predictions with Big Data: The Illusion of Sparsity.” Econometrica 89 (5): 2409–37."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to gingado!",
    "section": "",
    "text": "gingado seeks to facilitate the use of machine learning in economic and finance use cases, while promoting good practices. This package aims to be suitable for beginners and advanced users alike. Use cases may range from simple data retrievals to experimentation with machine learning algorithms to more complex model pipelines used in production."
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Welcome to gingado!",
    "section": "Overview",
    "text": "Overview\ngingado is a free, open source library built different functionalities:\n\ndata augmentation, to add more data from official sources, improving the machine models being trained by the user;\nrelevant datasets, both real and simulamed, to allow for easier model development and comparison;\nautomatic benchmark model, to assess candidate models against a reasonably well-performant model;\n(new!) machine learning-based estimators, to help answer questions of academic or practical importance;\nsupport for model documentation, to embed documentation and ethical considerations in the model development phase; and\nutilities, including tools to allow for lagging variables in a straightforward way.\n\nEach of these functionalities builds on top of the previous one. They can be used on a stand-alone basis, together, or even as part of a larger pipeline from data input to model training to documentation!\n\n\n\n\n\n\nTip\n\n\n\nNew functionalities are planned over time, so consider checking frequently on gingado for the latest toolsets."
  },
  {
    "objectID": "index.html#design-principles",
    "href": "index.html#design-principles",
    "title": "Welcome to gingado!",
    "section": "Design principles",
    "text": "Design principles\nThe choices made during development of gingado derive from the following principles, in no particular order:\n\nflexibility: users can use gingado out of the box or build custom processes on top of it;\ncompatibility: gingado works well with other widely used libraries in machine learning, such as scikit-learn and pandas; and\nresponsibility: gingado facilitates and promotes model documentation, including ethical considerations, as part of the machine learning development workflow."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Welcome to gingado!",
    "section": "Acknowledgements",
    "text": "Acknowledgements\ngingado’s API is inspired on the following libraries:\n\nscikit-learn (Buitinck et al. 2013)\nkeras (website here and also, this essay)\nfastai (Howard and Gugger 2020)\n\nIn addition, gingado is developed and maintained using nbdev."
  },
  {
    "objectID": "index.html#presentations-talks-papers",
    "href": "index.html#presentations-talks-papers",
    "title": "Welcome to gingado!",
    "section": "Presentations, talks, papers",
    "text": "Presentations, talks, papers\nThe most current version of the paper describing gingado is here. The paper and other material about gingado (ie, slide decks, papers) in this dedicated repository. Interested users are welcome to visit the repository and comment on the drafts or slide decks, preferably by opening an issue. I also store in this repository suggestions I receive as issues, so users can see what others commented (anonymously unless requested) and comment along as well!"
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "Welcome to gingado!",
    "section": "Install",
    "text": "Install\nTo install gingado, simply run the following code on the terminal:\n$ pip install gingado\nIf you use this package in your work, please cite it as below:\nAraujo, Douglas KG (2023): “gingado: a machine learning library focused on economics and finance”, BIS Working Paper No 1122.\n@techreport{gingado,\n    author = {Araujo, Douglas KG},\n    title = {gingado: a machine learning library focused on economics and finance},\n    series = {BIS Working Paper},\n    type = {Working Paper},\n    institution = {Bank for International Settlements},\n    year = {2023},\n    number = {1122}\n}"
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Welcome to gingado!",
    "section": "References",
    "text": "References\n\n\nBuitinck, Lars, Gilles Louppe, Mathieu Blondel, Fabian Pedregosa, Andreas Mueller, Olivier Grisel, Vlad Niculae, et al. 2013. “API Design for Machine Learning Software: Experiences from the Scikit-Learn Project.” CoRR abs/1309.0238. http://arxiv.org/abs/1309.0238.\n\n\nHoward, Jeremy, and Sylvain Gugger. 2020. “Fastai: A Layered API for Deep Learning.” Information 11 (2). https://doi.org/10.3390/info11020108."
  },
  {
    "objectID": "forecast.html",
    "href": "forecast.html",
    "title": "Using gingado to forecast financial series",
    "section": "",
    "text": "This notebook illustrates the use of gingado to build models for forecasting, using foreign exchange (FX) rate movements as an example. Please note that the results or the model should not be taken as investment advice.\nForecasting exchange rates is notoriously difficult (Rossi (2013) and references therein).\nThis exercise will illustrate various functionalities provided by gingado:\nUnlike most scripts that concentrate the package imports at the beginning, this walkthrough will import as needed, to better highlight where each contribution of gingado is used in the workflow.\nFirst, we will use gingado to run a simple example with the following characteristics:"
  },
  {
    "objectID": "forecast.html#downloading-fx-rates",
    "href": "forecast.html#downloading-fx-rates",
    "title": "Using gingado to forecast financial series",
    "section": "Downloading FX rates",
    "text": "Downloading FX rates\nIn this exercise, we will concentrate on the bilateral FX rates between the 🇺🇸 US Dollar (USD) and the 🇧🇷 Brazilian Real (BRL), 🇨🇦 Canadian Dollar (CAD), 🇨🇭 Swiss Franc (CHF), 🇪🇺 Euro (EUR), 🇬🇧 British Pound (GBP), 🇯🇵 Japanese Yen (JPY) and 🇲🇽 Mexican Peso (MXN).\nThe rates are standardised to measure the units in foreign currency bought by one USD. Therefore, positive returns represent USD is more valued compared to the other currency, and vice-versa.\n\n\nCode\nfrom gingado.utils import load_SDMX_data\n\n\n\n\nCode\ndf = load_SDMX_data(\n    sources={'BIS': 'WS_XRU_D'},\n    keys={\n        'FREQ': 'D', \n        'CURRENCY': ['BRL', 'CAD', 'CHF', 'EUR', 'GBP', 'JPY', 'MXN'],\n        'REF_AREA': ['BR', 'CA', 'CH', 'XM', 'GB', 'JP', 'MX']\n        },\n    params={'startPeriod': 2003}\n)\n\n\nQuerying data from BIS's dataflow 'WS_XRU' - US dollar exchange rates, m,q,a...\nthis dataflow does not have data in the desired frequency and time period.\nQuerying data from BIS's dataflow 'WS_XRU_D' - US dollar exchange rates, daily...\n\n\nThe code below simplifies the column names by removing the identification of the SDMX sources, dataflows and keys and replacing it with the usual code for the bilateral exchange rates.\n\n\nCode\nprint(\"Original column names:\")\nprint(df.columns)\n\ndf.columns = ['USD' + col.split('_')[9] for col in df.columns]\n\nprint(\"New column names:\")\nprint(df.columns)\n\n\nOriginal column names:\nIndex(['BIS__WS_XRU_D_D__BR__BRL__A', 'BIS__WS_XRU_D_D__CA__CAD__A',\n       'BIS__WS_XRU_D_D__CH__CHF__A', 'BIS__WS_XRU_D_D__GB__GBP__A',\n       'BIS__WS_XRU_D_D__JP__JPY__A', 'BIS__WS_XRU_D_D__MX__MXN__A',\n       'BIS__WS_XRU_D_D__XM__EUR__A'],\n      dtype='object')\nNew column names:\nIndex(['USDBRL', 'USDCAD', 'USDCHF', 'USDGBP', 'USDJPY', 'USDMXN', 'USDEUR'], dtype='object')\n\n\nThe dataset looks like this so far (most recent 5 rows displayed only):\n\n\nCode\ndf.tail()\n\n\n\n\n\n\n\n\n\nUSDBRL\nUSDCAD\nUSDCHF\nUSDGBP\nUSDJPY\nUSDMXN\nUSDEUR\n\n\nTIME_PERIOD\n\n\n\n\n\n\n\n\n\n\n\n2022-09-13\n5.087371\n1.297297\n0.950270\n0.853002\n142.014742\n19.814742\n0.982801\n\n\n2022-09-14\n5.187888\n1.319019\n0.962162\n0.865846\n143.223223\n20.048048\n1.001001\n\n\n2022-09-15\n5.187850\n1.318255\n0.957966\n0.870036\n143.544836\n20.018114\n1.000801\n\n\n2022-09-16\n5.252059\n1.328712\n0.962327\n0.878039\n143.188668\n20.095238\n1.004621\n\n\n2022-09-19\n5.293894\n1.330731\n0.966767\n0.878729\n143.563564\n20.133133\n1.001001\n\n\n\n\n\n\n\nWe are interested in the percentage change from the previous day.\n\n\nCode\nFX_rate_changes = df.pct_change()\nFX_rate_changes.dropna(inplace=True)\n\n\n\n\nCode\nFX_rate_changes.plot(subplots=True, layout=(4, 2), figsize=(15, 15), sharex=True, title='Selected daily FX rate changes')\n\n\narray([[&lt;AxesSubplot:xlabel='TIME_PERIOD'&gt;,\n        &lt;AxesSubplot:xlabel='TIME_PERIOD'&gt;],\n       [&lt;AxesSubplot:xlabel='TIME_PERIOD'&gt;,\n        &lt;AxesSubplot:xlabel='TIME_PERIOD'&gt;],\n       [&lt;AxesSubplot:xlabel='TIME_PERIOD'&gt;,\n        &lt;AxesSubplot:xlabel='TIME_PERIOD'&gt;],\n       [&lt;AxesSubplot:xlabel='TIME_PERIOD'&gt;,\n        &lt;AxesSubplot:xlabel='TIME_PERIOD'&gt;]], dtype=object)"
  },
  {
    "objectID": "forecast.html#augmenting-the-dataset",
    "href": "forecast.html#augmenting-the-dataset",
    "title": "Using gingado to forecast financial series",
    "section": "Augmenting the dataset",
    "text": "Augmenting the dataset\nWe will complement the FX rates data with two other datasets:\n\ndaily central bank policy rates from the Bank for International Settlements (BIS) (2017), and\nthe daily Composite Indicator of Systemic Stress (CISS), created by Hollo, Kremer, and Lo Duca (2012) and updated by the European Central Bank (ECB).\n\n\n\nCode\nfrom gingado.augmentation import AugmentSDMX\n\n\n\n\nCode\nX = AugmentSDMX(sources={'BIS': 'WS_CBPOL_D', 'ECB': 'CISS'}).fit_transform(FX_rate_changes)\n\n\nQuerying data from BIS's dataflow 'WS_CBPOL_D' - Policy rates daily...\nQuerying data from ECB's dataflow 'CISS' - Composite Indicator of Systemic Stress...\n\n\n2022-09-24 00:51:08,339 pandasdmx.reader.sdmxml - INFO: Use supplied dsd=… argument for non–structure-specific message\n\n\n\n\n\n\n\n\nNote\n\n\n\nit is acceptable in gingado to pass the variable of interest (the “y”, or in this case, FX_rate_changes) as the X argument in fit_transform. This is because this series will also be merged with the additional, augmented data and subsequently lagged along with it.\n\n\nYou can see below that the column names for the newly added columns reflect the source (BIS or ECB), the dataflow (separated from the source by a double underline), and then the specific keys to the series, which are specific to each dataflow.\n\n\nCode\nX.columns\n\n\nIndex(['USDBRL', 'USDCAD', 'USDCHF', 'USDGBP', 'USDJPY', 'USDMXN', 'USDEUR',\n       'BIS__WS_CBPOL_D_D__AR', 'BIS__WS_CBPOL_D_D__AU',\n       'BIS__WS_CBPOL_D_D__BR', 'BIS__WS_CBPOL_D_D__CA',\n       'BIS__WS_CBPOL_D_D__CH', 'BIS__WS_CBPOL_D_D__CL',\n       'BIS__WS_CBPOL_D_D__CN', 'BIS__WS_CBPOL_D_D__CO',\n       'BIS__WS_CBPOL_D_D__CZ', 'BIS__WS_CBPOL_D_D__DK',\n       'BIS__WS_CBPOL_D_D__GB', 'BIS__WS_CBPOL_D_D__HK',\n       'BIS__WS_CBPOL_D_D__HR', 'BIS__WS_CBPOL_D_D__HU',\n       'BIS__WS_CBPOL_D_D__ID', 'BIS__WS_CBPOL_D_D__IL',\n       'BIS__WS_CBPOL_D_D__IN', 'BIS__WS_CBPOL_D_D__IS',\n       'BIS__WS_CBPOL_D_D__JP', 'BIS__WS_CBPOL_D_D__KR',\n       'BIS__WS_CBPOL_D_D__MK', 'BIS__WS_CBPOL_D_D__MX',\n       'BIS__WS_CBPOL_D_D__MY', 'BIS__WS_CBPOL_D_D__NO',\n       'BIS__WS_CBPOL_D_D__NZ', 'BIS__WS_CBPOL_D_D__PE',\n       'BIS__WS_CBPOL_D_D__PH', 'BIS__WS_CBPOL_D_D__PL',\n       'BIS__WS_CBPOL_D_D__RO', 'BIS__WS_CBPOL_D_D__RS',\n       'BIS__WS_CBPOL_D_D__RU', 'BIS__WS_CBPOL_D_D__SA',\n       'BIS__WS_CBPOL_D_D__SE', 'BIS__WS_CBPOL_D_D__TH',\n       'BIS__WS_CBPOL_D_D__TR', 'BIS__WS_CBPOL_D_D__US',\n       'BIS__WS_CBPOL_D_D__XM', 'BIS__WS_CBPOL_D_D__ZA',\n       'ECB__CISS_D__AT__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__BE__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__CN__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__DE__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__ES__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__FI__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__FR__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__GB__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__IE__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__IT__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__NL__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__PT__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_BM__CON',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_CI__IDX',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_CO__CON',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_EM__CON',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_FI__CON',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_FX__CON',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_MM__CON',\n       'ECB__CISS_D__US__Z0Z__4F__EC__SS_CI__IDX',\n       'ECB__CISS_D__US__Z0Z__4F__EC__SS_CIN__IDX'],\n      dtype='object')\n\n\nBefore proceeding, we also include a differentiated version of the central bank policy data. It will be sparse, since these changes occur infrequently for most central banks, but it can help the model uncover how FX rate changes respond to central bank policy changes.\n\n\nCode\nimport pandas as pd\n\n\n\n\nCode\nX_diff = X.loc[:, X.columns.str.contains(\"BIS__WS_CBPOL_D\", case=False)].diff()\nX_diff.columns = [col + \"_diff\" for col in X_diff.columns]\nX = pd.concat([X, X_diff], axis=1)\n\n\nThis is how the data looks like now. Note that the names of the added columns reflect the source, dataflow and keys, all separated by underlines (the source is separated from the dataflow by two underlines at all cases). For example, the last key is the jurisdiction of the central bank.\nWe will keep all the newly added variables - even those that are from countries not in the currency list. This is because the model may uncover any relationship of interest between central bank policies from other countries and each particular currency pair.\n\n\nCode\nX.describe().transpose()\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nUSDBRL\n5098.0\n0.000136\n0.010597\n-0.080226\n-0.005774\n-0.000018\n0.005402\n0.120503\n\n\nUSDCAD\n5098.0\n-0.000016\n0.005844\n-0.043367\n-0.003222\n-0.000138\n0.003036\n0.036864\n\n\nUSDCHF\n5098.0\n-0.000050\n0.006434\n-0.139149\n-0.003241\n0.000000\n0.003257\n0.085326\n\n\nUSDGBP\n5098.0\n0.000085\n0.005998\n-0.038140\n-0.003307\n0.000000\n0.003241\n0.085019\n\n\nUSDJPY\n5098.0\n0.000054\n0.005970\n-0.041963\n-0.003007\n0.000116\n0.003183\n0.032901\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nBIS__WS_CBPOL_D_D__TH_diff\n5097.0\n-0.000196\n0.033590\n-1.000000\n0.000000\n0.000000\n0.000000\n0.500000\n\n\nBIS__WS_CBPOL_D_D__TR_diff\n5097.0\n-0.006082\n0.252368\n-4.250000\n0.000000\n0.000000\n0.000000\n8.500000\n\n\nBIS__WS_CBPOL_D_D__US_diff\n5097.0\n0.000221\n0.038721\n-1.000000\n0.000000\n0.000000\n0.000000\n0.750000\n\n\nBIS__WS_CBPOL_D_D__XM_diff\n5097.0\n-0.000294\n0.027431\n-0.750000\n0.000000\n0.000000\n0.000000\n0.750000\n\n\nBIS__WS_CBPOL_D_D__ZA_diff\n5097.0\n-0.001570\n0.062037\n-1.500000\n0.000000\n0.000000\n0.000000\n0.750000\n\n\n\n\n105 rows × 8 columns\n\n\n\nThe policy rates for some central banks have less observations than the others, as seen above.\nBecause some data are missing, we will impute data for the missing dates, by simply propagating the last valid observation, and when that is not possible, replacing the missing information with a “0”.\n\n\nCode\nX.fillna(method='pad', inplace=True)\nX.fillna(value=0, inplace=True)\n\n\nNow is a good time to start the model documentation. For this, we can use the standard model card that already comes with gingado.\nThe goal is to facilitate economists who want to make model documentation a part of their normal workflow.\n\n\nCode\nfrom gingado.model_documentation import ModelCard\n\n\n\n\nCode\nmodel_doc = ModelCard()\nmodel_doc.open_questions()\n\n\n['model_details__developer',\n 'model_details__version',\n 'model_details__type',\n 'model_details__info',\n 'model_details__paper',\n 'model_details__citation',\n 'model_details__license',\n 'model_details__contact',\n 'intended_use__primary_uses',\n 'intended_use__primary_users',\n 'intended_use__out_of_scope',\n 'factors__relevant',\n 'factors__evaluation',\n 'metrics__performance_measures',\n 'metrics__thresholds',\n 'metrics__variation_approaches',\n 'evaluation_data__datasets',\n 'evaluation_data__motivation',\n 'evaluation_data__preprocessing',\n 'training_data__training_data',\n 'quant_analyses__unitary',\n 'quant_analyses__intersectional',\n 'ethical_considerations__sensitive_data',\n 'ethical_considerations__human_life',\n 'ethical_considerations__mitigations',\n 'ethical_considerations__risks_and_harms',\n 'ethical_considerations__use_cases',\n 'ethical_considerations__additional_information',\n 'caveats_recommendations__caveats',\n 'caveats_recommendations__recommendations']\n\n\nAs an example, we can add the following information to the model:\n\n\nCode\nmodel_doc.fill_info({\n    'intended_use': {\n        'primary_uses': 'These models are simplified toy models made to illustrate the use of gingado',\n        'out_of_scope': 'These models were not constructed for decision-making and as such their use as predictors in real life decisions is strongly discouraged and out of scope.'\n    },\n    'metrics': {\n        'performance_measures': 'Consistent with most papers reviewed by Rossi (2013), these models were evaluated by their root mean squared error.'\n    },\n    'ethical_considerations': {\n        'sensitive_data': 'These models were not trained with sensitive data.',\n        'human_life': 'The models do not involve the collection or use of individual-level data, and have no foreseen impact on human life.'\n    },\n    \n})"
  },
  {
    "objectID": "forecast.html#lagging-the-regressors",
    "href": "forecast.html#lagging-the-regressors",
    "title": "Using gingado to forecast financial series",
    "section": "Lagging the regressors",
    "text": "Lagging the regressors\nThis model will not include any contemporaneous variable. Therefore, all regresors must be lagged.\nFor illustration purposes, we use 5 lags in this exercise.\n\n\nCode\nfrom gingado.utils import Lag\n\n\n\n\nCode\nn_lags = 5\n\nX_lagged = Lag(lags=n_lags).fit_transform(X)\nX_lagged\n\ny = FX_rate_changes[n_lags:]\n\n\nNow is a good opportunity to check by how much we have increased our regressor space:\n\n\nCode\npd.Series({\n    \"FX rates only\": y.shape[1],\n    \"... with augmentation_\": X.shape[1],\n    \"... lagged\": X_lagged.shape[1]\n})\n\n\nFX rates only               7\n... with augmentation_    105\n... lagged                525\ndtype: int64"
  },
  {
    "objectID": "forecast.html#training-the-models",
    "href": "forecast.html#training-the-models",
    "title": "Using gingado to forecast financial series",
    "section": "Training the models",
    "text": "Training the models\nOur dataset is now complete. Before using it to train the models, we hold out the most recent data to serve as our testing dataset, so we can compare our models with real out-of-sample information.\nWe can choose, say, 1st January 2022.\n\n\nCode\ncutoff = '2020-01-01'\n\nX_train, X_test = X_lagged[:cutoff], X_lagged[cutoff:]\ny_train, y_test = y[:cutoff], y[cutoff:]\n\n\n\n\nCode\nmodel_doc.fill_info({\n    'training_data': \n    {'training_data': \n        \"\"\"\n        The training data comprise time series obtained from official sources (BIS and ECB) on:\n        * foreign exchange rates\n        * central bank policy rates\n        * an estimated indicator for systemic stress\n        The training and evaluation datasets are the same time series, only different windows in time.\"\"\"\n    }\n})\n\n\nThe current status of the documentation is:\n\n\nCode\npd.Series(model_doc.show_json())\n\n\nmodel_details              {'developer': 'Person or organisation developi...\nintended_use               {'primary_uses': 'These models are simplified ...\nfactors                    {'relevant': 'Relevant factors', 'evaluation':...\nmetrics                    {'performance_measures': 'Consistent with most...\nevaluation_data            {'datasets': 'Datasets', 'motivation': 'Motiva...\ntraining_data              {'training_data': '\n        The training data ...\nquant_analyses             {'unitary': 'Unitary results', 'intersectional...\nethical_considerations     {'sensitive_data': 'These models were not trai...\ncaveats_recommendations    {'caveats': 'For example, did the results sugg...\ndtype: object\n\n\n\nCreating a random walk benchmark\nRossi (2013) highlights that few predictors beat the random walk without drift model. This is a good opportunity to showcase how we can use gingado’s in-built base class ggdBenchmark to build our customised benchmark model, in this case a random walk.\nThe calculation of the random walk benchmark is very simple. Still, creating a gingado benchmark offers some advantages: it is easier to compare alternative models, and the model documentation is done more seamlessly.\nA custom benchmark model must implement the following steps:\n\nsub-class ggdBenchmark (or alternatively implement its methods)\ndefine an estimator that is compatible with scikit-learn’s API:\n\nat the very least, it has a fit method that returns self\n\n\nIf the user is relying on a custom estimator - like in this case, a random walk estimator to align with the literature - then this custom estimator also has some requirements:\n\nit should ideally subclass scikit-learn’s BaseEstimator (mostly for the get_params / set_params methods)\nthree methods are necessary:\n\nfit, which should at least create an attribute ending in an underline (“_“), so that gingado knows it is fitted\npredict\nscore\n\n\n\n\nCode\nimport numpy as np\nfrom gingado.benchmark import ggdBenchmark\nfrom sklearn.base import BaseEstimator\nfrom sklearn.ensemble import VotingRegressor\nfrom sklearn.model_selection import TimeSeriesSplit\n\n\n\n\nCode\nclass RandomWalkEstimator(BaseEstimator):\n    def __init__(self, scoring='neg_root_mean_squared_error'):\n        self.scoring = scoring\n    \n    def fit(self, X, y=None):\n        self.n_samples_ = X.shape[0]\n        return self\n\n    def predict(self, X):\n        return np.zeros(X.shape[0])\n\n    def score(self, X, y, sample_weight=None):\n        from sklearn.metrics import mean_squared_error\n        y_pred = self.predict(X)\n        return mean_squared_error(y, y_pred, sample_weight=sample_weight, squared=False)\n\n    def forecast(self, forecast_horizon=1):\n        self.forecast_horizon = forecast_horizon\n        return np.zeros(self.forecast_horizon)\n\nclass RandomWalkBenchmark(ggdBenchmark):\n    def __init__(\n        self, \n        estimator=RandomWalkEstimator(), \n        auto_document=ModelCard,\n        cv=TimeSeriesSplit(n_splits=10, test_size=60), \n        ensemble_method=VotingRegressor, \n        verbose_grid=None):\n        self.estimator=estimator\n        self.auto_document=auto_document\n        self.cv=cv\n        self.ensemble_method=ensemble_method\n        self.verbose_grid=verbose_grid\n\n    def fit(self, X, y=None):\n        self.benchmark=self.estimator\n        self.benchmark.fit(X, y)\n        return self\n\n\n\n\nTraining the candidate models\nNow that we have a benchmark, we can create candidate models that will try to beat it.\nIn this simplified example, we will choose only two: a random forest, an AdaBoost regressor and a Lasso model. Their hyperparameters are not particularly important for the example, but of course they could be fine-tuned as well.\nIn the language of Rossi (2013), the models below are one “single-equation, lagged fundamental model” for each currency.\n\n\nCode\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.linear_model import Lasso\n\n\n\n\nCode\nforest = RandomForestRegressor(n_estimators=250, max_features='log2').fit(X_train, y_train['USDBRL'])\nadaboost = AdaBoostRegressor(n_estimators=150).fit(X_train, y_train['USDBRL'])\nlasso = Lasso(alpha=0.1).fit(X_train, y_train['USDBRL'])\n\nrw = RandomWalkBenchmark().fit(X_train, y_train['USDBRL'])\n\n\nWe can now compare the model results, using the test dataset we held out previously.\nNote that we must pass the criterion against which we are comparing the forecasts.\n\n\nCode\nfrom sklearn.metrics import mean_squared_error\n\n\n\n\nCode\nresults = rw.compare_fitted_candidates(\n    X_test, y_test['USDBRL'],\n    candidates=[forest, adaboost, lasso],\n    scoring_func=mean_squared_error)\n\npd.Series(results)\n\n\nRandomWalkEstimator()                                           0.000127\nRandomForestRegressor(max_features='log2', n_estimators=250)    0.000129\nAdaBoostRegressor(n_estimators=150)                             0.000132\nLasso(alpha=0.1)                                                0.000126\ndtype: float64\n\n\nAs mentioned above, benchmarks can facilitate the model documentation. In addition to the broader documentation that is already ongoing, each benchmark object create their own where they store model information. We can use that for the broader documentation.\nIn our case, the only parameter we created above during fit is the number of samples: not a particularly informative variable but it was included just for illustration purposes. In any case, the parameter appears in the “model_details” section, item “info”, of the benchmark’s rw documentation. Similarly, the parameters of more fully-fledged estimators also appear in that section.\n\n\nCode\nrw.document()\n\nrw.model_documentation.show_json()['model_details']['info']\n\n\n{'n_samples_': 4394}\n\n\n\n\nCode\nmodel_doc.fill_info({\n    'model_details': {'info': rw.model_documentation.show_json()['model_details']['info']}\n})\n\n\n\n\nCode\nmodel_doc.show_json()\n\n\n{'model_details': {'developer': 'Person or organisation developing the model',\n  'datetime': '2022-09-24 00:51:37 ',\n  'version': 'Model version',\n  'type': 'Model type',\n  'info': {'n_samples_': 4394},\n  'paper': 'Paper or other resource for more information',\n  'citation': 'Citation details',\n  'license': 'License',\n  'contact': 'Where to send questions or comments about the model'},\n 'intended_use': {'primary_uses': 'These models are simplified toy models made to illustrate the use of gingado',\n  'primary_users': 'Primary intended users',\n  'out_of_scope': 'These models were not constructed for decision-making and as such their use as predictors in real life decisions is strongly discouraged and out of scope.'},\n 'factors': {'relevant': 'Relevant factors',\n  'evaluation': 'Evaluation factors'},\n 'metrics': {'performance_measures': 'Consistent with most papers reviewed by Rossi (2013), these models were evaluated by their root mean squared error.',\n  'thresholds': 'Decision thresholds',\n  'variation_approaches': 'Variation approaches'},\n 'evaluation_data': {'datasets': 'Datasets',\n  'motivation': 'Motivation',\n  'preprocessing': 'Preprocessing'},\n 'training_data': {'training_data': '\\n        The training data comprise time series obtained from official sources (BIS and ECB) on:\\n        * foreign exchange rates\\n        * central bank policy rates\\n        * an estimated indicator for systemic stress\\n        The training and evaluation datasets are the same time series, only different windows in time.'},\n 'quant_analyses': {'unitary': 'Unitary results',\n  'intersectional': 'Intersectional results'},\n 'ethical_considerations': {'sensitive_data': 'These models were not trained with sensitive data.',\n  'human_life': 'The models do not involve the collection or use of individual-level data, and have no foreseen impact on human life.',\n  'mitigations': 'What risk mitigation strategies were used during model development?',\n  'risks_and_harms': '\\n            What risks may be present in model usage? Try to identify the potential recipients, likelihood, and magnitude of harms. \\n            If these cannot be determined, note that they were consid- ered but remain unknown',\n  'use_cases': 'Are there any known model use cases that are especially fraught?',\n  'additional_information': '\\n            If possible, this section should also include any additional ethical considerations that went into model development, \\n            for example, review by an external board, or testing with a specific community.'},\n 'caveats_recommendations': {'caveats': 'For example, did the results suggest any further testing? Were there any relevant groups that were not represented in the evaluation dataset?',\n  'recommendations': 'Are there additional recommendations for model use? What are the ideal characteristics of an evaluation dataset for this model?'}}\n\n\nWe can save the documentation to disk in JSON format with model_doc.save_json(), or parse it to create other documents (eg, a PDF file) using third-party libraries."
  },
  {
    "objectID": "forecast.html#references",
    "href": "forecast.html#references",
    "title": "Using gingado to forecast financial series",
    "section": "References",
    "text": "References\n\n\nBank for International Settlements. 2017. “Recent Enhancements to the BIS Statistics.” BIS Quarterly Review. Vol. September. https://www.bis.org/publ/qtrpdf/r_qt1709c.htm.\n\n\nHollo, Daniel, Manfred Kremer, and Marco Lo Duca. 2012. “CISS-a Composite Indicator of Systemic Stress in the Financial System.”\n\n\nRossi, Barbara. 2013. “Exchange Rate Predictability.” Journal of Economic Literature 51 (4): 1063–1119."
  }
]