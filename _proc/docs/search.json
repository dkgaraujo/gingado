[
  {
    "objectID": "augmentation.html",
    "href": "augmentation.html",
    "title": "Data augmentation",
    "section": "",
    "text": "gingado provides data augmentation functionalities that can help users to augment their datasets with a time series dimension. This can be done both on a stand-alone basis as the user incorporates new data on top of the original dataset, or as part of a scikit-learn Pipeline that also includes other steps like data transformation and model estimation."
  },
  {
    "objectID": "augmentation.html#data-augmentation-with-sdmx",
    "href": "augmentation.html#data-augmentation-with-sdmx",
    "title": "Data augmentation",
    "section": "1 Data augmentation with SDMX",
    "text": "1 Data augmentation with SDMX\nThe Statistical Data and Metadata eXchange (SDMX) is an ISO standard comprising: * technical standards * statistical guidelines, including cross-domain concepts and codelists * an IT architecture and tools\nSDMX is sponsored by the Bank for International Settlements, European Central Bank, Eurostat, International Monetary Fund, Organisation for Economic Co-operation and Development, United Nations, and World Bank Group.\nMore information about the SDMX is available on its webpage.\n/Users/douglasaraujo/Coding/.venv_gingado/lib/python3.10/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Notes\n  else: warn(msg)\n\nsource\n\n1.1 AugmentSDMX\n\n AugmentSDMX (sources={'BIS': 'WS_CBPOL_D'}, variance_threshold=None,\n              propagate_last_known_value=True, fillna=0, verbose=True)\n\nBase class for all estimators in scikit-learn.\nAs mentioned above, gingado’s transformers are built to be compatible with scikit-learn. The code below demonstrates this compatibility.\nFirst, we create the example dataset. In this case, it comprises the daily foreign exchange rate of selected currencies to the Euro. The Brazilian Real (BRL) is chosen for this example as the dependent variable.\n\n\nCode\n#collapse_output\nfrom gingado.utils import load_SDMX_data, Lag\nfrom sklearn.model_selection import TimeSeriesSplit\n\nX = load_SDMX_data(\n    sources={'ECB': 'EXR'}, \n    keys={'FREQ': 'D', 'CURRENCY': ['EUR', 'AUD', 'BRL', 'CAD', 'CHF', 'GBP', 'JPY', 'SGD', 'USD']},\n    params={\"startPeriod\": 2003}\n    )\n# drop rows with empty values\nX.dropna(inplace=True)\n# adjust column names in this simple example for ease of understanding:\n# remove parts related to source and dataflow names\nX.columns = X.columns.str.replace(\"ECB__EXR_D__\", \"\").str.replace(\"__EUR__SP00__A\", \"\")\nX = Lag(lags=1, jump=0, keep_contemporaneous_X=True).fit_transform(X)\ny = X.pop('BRL')\n# retain only the lagged variables in the X variable\nX = X[X.columns[X.columns.str.contains('_lag_')]]\n\n\nQuerying data from ECB's dataflow 'EXR' - Exchange Rates...\n\n\n\n\nCode\nX_train, X_test = X.iloc[:-1], X.tail(1)\ny_train, y_test = y.iloc[:-1], y.tail(1)\n\nX_train.shape, y_train.shape, X_test.shape, y_test.shape\n\n\n((4970, 8), (4970,), (1, 8), (1,))\n\n\nNext, the data augmentation object provided by gingado adds more data. In this case, for brevity only one dataflow from one source is listed. If users want to add more SDMX sources, simply add more keys to the dictionary. And if users want data from all dataflows from a given source provided the keys and parameters such as frequency and dates match, the value should be set to 'all', as in {'ECB': ['CISS'], 'BIS': 'all'}.\n\n\nCode\n#collapse_output\ntest_src = {'ECB': ['CISS'], 'BIS': ['WS_CBPOL_D']}\n\nX_train__fit_transform = AugmentSDMX(sources=test_src).fit_transform(X=X_train)\nX_train__fit_then_transform = AugmentSDMX(sources=test_src).fit(X=X_train).transform(X=X_train, training=True)\n\nassert X_train__fit_transform.shape == X_train__fit_then_transform.shape\n\n\nQuerying data from ECB's dataflow 'CISS' - Composite Indicator of Systemic Stress...\n\n\n2022-06-01 01:11:00,886 pandasdmx.reader.sdmxml - INFO: Use supplied dsd=… argument for non–structure-specific message\n\n\nQuerying data from BIS's dataflow 'WS_CBPOL_D' - Policy rates daily...\n\n\n2022-06-01 01:12:24,776 pandasdmx.reader.sdmxml - INFO: Use supplied dsd=… argument for non–structure-specific message\n\n\nQuerying data from ECB's dataflow 'CISS' - Composite Indicator of Systemic Stress...\nQuerying data from BIS's dataflow 'WS_CBPOL_D' - Policy rates daily...\n\n\nAugmentSDMX can also be part of a Pipeline object, which minimises operational errors during modelling and avoids using testing data during training:\nThis is the dataset now after this particular augmentation:\n\n\nCode\n#collapse_output\nprint(f\"No of columns: {len(X_train__fit_transform.columns)} {X_train__fit_transform.columns}\")\nX_train__fit_transform\n\n\nNo of columns: 68 Index(['AUD_lag_1', 'BRL_lag_1', 'CAD_lag_1', 'CHF_lag_1', 'GBP_lag_1',\n       'JPY_lag_1', 'SGD_lag_1', 'USD_lag_1',\n       'ECB__CISS_D__AT__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__BE__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__CN__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__DE__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__ES__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__FI__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__FR__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__GB__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__IE__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__IT__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__NL__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__PT__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_BM__CON',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_CI__IDX',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_CO__CON',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_EM__CON',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_FI__CON',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_FX__CON',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_MM__CON',\n       'ECB__CISS_D__US__Z0Z__4F__EC__SS_CI__IDX',\n       'ECB__CISS_D__US__Z0Z__4F__EC__SS_CIN__IDX', 'BIS__WS_CBPOL_D_D__AR',\n       'BIS__WS_CBPOL_D_D__AU', 'BIS__WS_CBPOL_D_D__BR',\n       'BIS__WS_CBPOL_D_D__CA', 'BIS__WS_CBPOL_D_D__CH',\n       'BIS__WS_CBPOL_D_D__CL', 'BIS__WS_CBPOL_D_D__CN',\n       'BIS__WS_CBPOL_D_D__CO', 'BIS__WS_CBPOL_D_D__CZ',\n       'BIS__WS_CBPOL_D_D__DK', 'BIS__WS_CBPOL_D_D__GB',\n       'BIS__WS_CBPOL_D_D__HK', 'BIS__WS_CBPOL_D_D__HR',\n       'BIS__WS_CBPOL_D_D__HU', 'BIS__WS_CBPOL_D_D__ID',\n       'BIS__WS_CBPOL_D_D__IL', 'BIS__WS_CBPOL_D_D__IN',\n       'BIS__WS_CBPOL_D_D__IS', 'BIS__WS_CBPOL_D_D__JP',\n       'BIS__WS_CBPOL_D_D__KR', 'BIS__WS_CBPOL_D_D__MK',\n       'BIS__WS_CBPOL_D_D__MX', 'BIS__WS_CBPOL_D_D__MY',\n       'BIS__WS_CBPOL_D_D__NO', 'BIS__WS_CBPOL_D_D__NZ',\n       'BIS__WS_CBPOL_D_D__PE', 'BIS__WS_CBPOL_D_D__PH',\n       'BIS__WS_CBPOL_D_D__PL', 'BIS__WS_CBPOL_D_D__RO',\n       'BIS__WS_CBPOL_D_D__RS', 'BIS__WS_CBPOL_D_D__RU',\n       'BIS__WS_CBPOL_D_D__SA', 'BIS__WS_CBPOL_D_D__SE',\n       'BIS__WS_CBPOL_D_D__TH', 'BIS__WS_CBPOL_D_D__TR',\n       'BIS__WS_CBPOL_D_D__US', 'BIS__WS_CBPOL_D_D__XM',\n       'BIS__WS_CBPOL_D_D__ZA'],\n      dtype='object')\n\n\n\n\n\n\n  \n    \n      \n      AUD_lag_1\n      BRL_lag_1\n      CAD_lag_1\n      CHF_lag_1\n      GBP_lag_1\n      JPY_lag_1\n      SGD_lag_1\n      USD_lag_1\n      ECB__CISS_D__AT__Z0Z__4F__EC__SS_CIN__IDX\n      ECB__CISS_D__BE__Z0Z__4F__EC__SS_CIN__IDX\n      ...\n      BIS__WS_CBPOL_D_D__RO\n      BIS__WS_CBPOL_D_D__RS\n      BIS__WS_CBPOL_D_D__RU\n      BIS__WS_CBPOL_D_D__SA\n      BIS__WS_CBPOL_D_D__SE\n      BIS__WS_CBPOL_D_D__TH\n      BIS__WS_CBPOL_D_D__TR\n      BIS__WS_CBPOL_D_D__US\n      BIS__WS_CBPOL_D_D__XM\n      BIS__WS_CBPOL_D_D__ZA\n    \n    \n      TIME_PERIOD\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2003-01-03\n      1.8554\n      3.6770\n      1.6422\n      1.4528\n      0.65200\n      124.40\n      1.8188\n      1.0446\n      0.021899\n      0.043292\n      ...\n      NaN\n      9.5\n      NaN\n      NaN\n      3.75\n      1.75\n      44.0\n      1.250\n      2.75\n      13.50\n    \n    \n      2003-01-06\n      1.8440\n      3.6112\n      1.6264\n      1.4555\n      0.65000\n      124.56\n      1.8132\n      1.0392\n      0.020801\n      0.039924\n      ...\n      19.75\n      9.5\n      NaN\n      2.00\n      3.75\n      1.75\n      44.0\n      1.250\n      2.75\n      13.50\n    \n    \n      2003-01-07\n      1.8281\n      3.5145\n      1.6383\n      1.4563\n      0.64950\n      124.40\n      1.8210\n      1.0488\n      0.019738\n      0.038084\n      ...\n      19.75\n      9.5\n      NaN\n      2.00\n      3.75\n      1.75\n      44.0\n      1.250\n      2.75\n      13.50\n    \n    \n      2003-01-08\n      1.8160\n      3.5139\n      1.6257\n      1.4565\n      0.64960\n      124.82\n      1.8155\n      1.0425\n      0.019947\n      0.040338\n      ...\n      19.75\n      9.5\n      21.0\n      2.00\n      3.75\n      1.75\n      44.0\n      1.250\n      2.75\n      13.50\n    \n    \n      2003-01-09\n      1.8132\n      3.4405\n      1.6231\n      1.4586\n      0.64950\n      124.90\n      1.8102\n      1.0377\n      0.017026\n      0.040535\n      ...\n      19.75\n      9.5\n      21.0\n      2.00\n      3.75\n      1.75\n      44.0\n      1.250\n      2.75\n      13.50\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2022-05-24\n      1.4982\n      5.1623\n      1.3626\n      1.0310\n      0.84783\n      136.05\n      1.4639\n      1.0659\n      0.269626\n      0.200358\n      ...\n      3.75\n      2.0\n      14.0\n      1.75\n      0.25\n      0.50\n      14.0\n      0.875\n      0.00\n      4.75\n    \n    \n      2022-05-25\n      1.5152\n      5.1793\n      1.3714\n      1.0334\n      0.85750\n      136.49\n      1.4722\n      1.0720\n      0.264778\n      0.204644\n      ...\n      3.75\n      2.0\n      14.0\n      1.75\n      0.25\n      0.50\n      14.0\n      0.875\n      0.00\n      4.75\n    \n    \n      2022-05-26\n      1.5126\n      5.1736\n      1.3720\n      1.0269\n      0.85295\n      135.34\n      1.4676\n      1.0656\n      0.249738\n      0.198993\n      ...\n      3.75\n      2.0\n      14.0\n      1.75\n      0.25\n      0.50\n      14.0\n      0.875\n      0.00\n      4.75\n    \n    \n      2022-05-27\n      1.5110\n      5.1741\n      1.3715\n      1.0283\n      0.85073\n      135.95\n      1.4709\n      1.0697\n      0.237198\n      0.188693\n      ...\n      3.75\n      2.0\n      14.0\n      1.75\n      0.25\n      0.50\n      14.0\n      0.875\n      0.00\n      4.75\n    \n    \n      2022-05-30\n      1.4995\n      5.0959\n      1.3661\n      1.0258\n      0.84875\n      136.05\n      1.4679\n      1.0722\n      0.219695\n      0.184084\n      ...\n      3.75\n      2.0\n      14.0\n      1.75\n      0.25\n      0.50\n      14.0\n      0.875\n      0.00\n      4.75\n    \n  \n\n4970 rows × 68 columns\n\n\n\n\n\nCode\nfrom gingado.augmentation import AugmentSDMX\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\n\npipeline = Pipeline([\n    ('augmentation', AugmentSDMX(sources={'BIS': 'WS_CBPOL_D'})),\n    ('imp', IterativeImputer(max_iter=10)),\n    ('forest', RandomForestRegressor())\n], verbose=True)\n\n\n\n\n1.2 Tuning the data augmentation to enhance model performance\nAnd since AugmentSDMX can be included in a Pipeline, it can also be fine-tuned by parameter search techniques (such as grid search), further helping users make the best of available data to enhance performance of their models.\n\n\nCode\n#collapse_output\ngrid = GridSearchCV(\n    estimator=pipeline,\n    param_grid={'augmentation': ['passthrough', AugmentSDMX(sources={'ECB': 'CISS'})]},\n    verbose=3,\n    cv=TimeSeriesSplit()\n    )\n\ny_pred_grid = grid.fit(X_train, y_train).predict(X_test)\n\n\nFitting 5 folds for each of 2 candidates, totalling 10 fits\n[Pipeline] ...... (step 1 of 3) Processing augmentation, total=   0.0s\n[Pipeline] ............... (step 2 of 3) Processing imp, total=   0.0s\n[Pipeline] ............ (step 3 of 3) Processing forest, total=   0.3s\n[CV 1/5] END ..........augmentation=passthrough;, score=0.623 total time=   0.3s\n[Pipeline] ...... (step 1 of 3) Processing augmentation, total=   0.0s\n[Pipeline] ............... (step 2 of 3) Processing imp, total=   0.0s\n[Pipeline] ............ (step 3 of 3) Processing forest, total=   0.6s\n[CV 2/5] END ..........augmentation=passthrough;, score=0.423 total time=   0.6s\n[Pipeline] ...... (step 1 of 3) Processing augmentation, total=   0.0s\n[Pipeline] ............... (step 2 of 3) Processing imp, total=   0.0s\n[Pipeline] ............ (step 3 of 3) Processing forest, total=   0.9s\n[CV 3/5] END ..........augmentation=passthrough;, score=0.912 total time=   0.9s\n[Pipeline] ...... (step 1 of 3) Processing augmentation, total=   0.0s\n[Pipeline] ............... (step 2 of 3) Processing imp, total=   0.0s\n[Pipeline] ............ (step 3 of 3) Processing forest, total=   1.1s\n[CV 4/5] END ..........augmentation=passthrough;, score=0.950 total time=   1.2s\n[Pipeline] ...... (step 1 of 3) Processing augmentation, total=   0.0s\n[Pipeline] ............... (step 2 of 3) Processing imp, total=   0.0s\n[Pipeline] ............ (step 3 of 3) Processing forest, total=   1.5s\n[CV 5/5] END .........augmentation=passthrough;, score=-0.979 total time=   1.5s\n\n\n2022-06-01 01:31:08,006 pandasdmx.reader.sdmxml - INFO: Use supplied dsd=… argument for non–structure-specific message\n\n\nQuerying data from ECB's dataflow 'CISS' - Composite Indicator of Systemic Stress...\n[Pipeline] ...... (step 1 of 3) Processing augmentation, total=   3.4s\n[Pipeline] ............... (step 2 of 3) Processing imp, total=   0.1s\n[Pipeline] ............ (step 3 of 3) Processing forest, total=   0.7s\n\n\n2022-06-01 01:31:12,084 pandasdmx.reader.sdmxml - INFO: Use supplied dsd=… argument for non–structure-specific message\n\n\nQuerying data from ECB's dataflow 'CISS' - Composite Indicator of Systemic Stress...\n[CV 1/5] END augmentation=AugmentSDMX(sources={'ECB': 'CISS'});, score=0.453 total time=   7.8s\n\n\n2022-06-01 01:31:15,679 pandasdmx.reader.sdmxml - INFO: Use supplied dsd=… argument for non–structure-specific message\n\n\nQuerying data from ECB's dataflow 'CISS' - Composite Indicator of Systemic Stress...\n[Pipeline] ...... (step 1 of 3) Processing augmentation, total=  13.6s\n[Pipeline] ............... (step 2 of 3) Processing imp, total=   0.2s\n[Pipeline] ............ (step 3 of 3) Processing forest, total=   1.8s\n\n\n2022-06-01 01:31:31,223 pandasdmx.reader.sdmxml - INFO: Use supplied dsd=… argument for non–structure-specific message\n\n\nQuerying data from ECB's dataflow 'CISS' - Composite Indicator of Systemic Stress...\n[CV 2/5] END augmentation=AugmentSDMX(sources={'ECB': 'CISS'});, score=0.385 total time=  24.2s\n\n\n2022-06-01 01:31:39,924 pandasdmx.reader.sdmxml - INFO: Use supplied dsd=… argument for non–structure-specific message\n\n\nQuerying data from ECB's dataflow 'CISS' - Composite Indicator of Systemic Stress...\n[Pipeline] ...... (step 1 of 3) Processing augmentation, total=  13.4s\n[Pipeline] ............... (step 2 of 3) Processing imp, total=   0.2s\n[Pipeline] ............ (step 3 of 3) Processing forest, total=   2.8s\n\n\n2022-06-01 01:31:56,266 pandasdmx.reader.sdmxml - INFO: Use supplied dsd=… argument for non–structure-specific message\n\n\nQuerying data from ECB's dataflow 'CISS' - Composite Indicator of Systemic Stress...\n[CV 3/5] END augmentation=AugmentSDMX(sources={'ECB': 'CISS'});, score=0.917 total time=  28.6s\n\n\n2022-06-01 01:32:08,450 pandasdmx.reader.sdmxml - INFO: Use supplied dsd=… argument for non–structure-specific message\n\n\nQuerying data from ECB's dataflow 'CISS' - Composite Indicator of Systemic Stress...\n[Pipeline] ...... (step 1 of 3) Processing augmentation, total=  16.7s\n[Pipeline] ............... (step 2 of 3) Processing imp, total=   0.3s\n[Pipeline] ............ (step 3 of 3) Processing forest, total=   3.9s\n\n\n2022-06-01 01:32:29,346 pandasdmx.reader.sdmxml - INFO: Use supplied dsd=… argument for non–structure-specific message\n\n\nQuerying data from ECB's dataflow 'CISS' - Composite Indicator of Systemic Stress...\n[CV 4/5] END augmentation=AugmentSDMX(sources={'ECB': 'CISS'});, score=0.926 total time=  36.8s\nQuerying data from ECB's dataflow 'CISS' - Composite Indicator of Systemic Stress...\n\n\n2022-06-01 01:32:45,419 pandasdmx.reader.sdmxml - INFO: Use supplied dsd=… argument for non–structure-specific message\n\n\n[Pipeline] ...... (step 1 of 3) Processing augmentation, total=  21.5s\n[Pipeline] ............... (step 2 of 3) Processing imp, total=   0.3s\n[Pipeline] ............ (step 3 of 3) Processing forest, total=   4.8s\n\n\n2022-06-01 01:33:11,953 pandasdmx.reader.sdmxml - INFO: Use supplied dsd=… argument for non–structure-specific message\n\n\nQuerying data from ECB's dataflow 'CISS' - Composite Indicator of Systemic Stress...\n[CV 5/5] END augmentation=AugmentSDMX(sources={'ECB': 'CISS'});, score=-1.428 total time=  47.8s\n[Pipeline] ...... (step 1 of 3) Processing augmentation, total=   0.0s\n[Pipeline] ............... (step 2 of 3) Processing imp, total=   0.0s\n[Pipeline] ............ (step 3 of 3) Processing forest, total=   1.9s\n\n\n\n\nCode\ngrid.best_params_\n\n\n{'augmentation': 'passthrough'}\n\n\n\n\nCode\nprint(f\"The best model was achieved by {'not ' if grid.best_params_['augmentation'] == 'passthrough' else ''}using the data augmentation.\")\n\n\nThe best model was achieved by *not* using the data augmentation.\n\n\n\n\nCode\nprint(f\"The last value in the training dataset was {y_train.tail(1).to_numpy()}. The predicted value was {y_pred_grid}, and the actual value was {y_test.to_numpy()}.\")\n\n\nThe last value in the training dataset was [5.0629]. The predicted value was [5.102027], and the actual value was [5.0965].\n\n\n\n\n1.3 Sources of data\ngingado seeks to only lists realiable data sources by choice, with a focus on official sources. This is meant to provide users with the trust that their dataset will be complemented by reliable sources. Unfortunately, it is not possible at this stage to include all official sources given the substantial manual and maintenance work. gingado leverages the existence of the Statistical Data and Metadata eXchange (SDMX), an organisation of official data sources that establishes common data and metadata formats, to download data that is relevant (and hopefully also useful) to users.\nThe function below from the package simpledmx returns a list of codes corresponding to the data sources available to provide gingado users with data through SDMX.\n\n\nCode\n#collapse_output\nfrom gingado.utils import list_SDMX_sources\nlist_SDMX_sources()\n\n\n['ABS',\n 'ABS_XML',\n 'BBK',\n 'BIS',\n 'CD2030',\n 'ECB',\n 'ESTAT',\n 'ILO',\n 'IMF',\n 'INEGI',\n 'INSEE',\n 'ISTAT',\n 'LSD',\n 'NB',\n 'NBB',\n 'OECD',\n 'SGR',\n 'SPC',\n 'STAT_EE',\n 'UNICEF',\n 'UNSD',\n 'WB',\n 'WB_WDI']\n\n\nYou can also see what the available dataflows are. The code below returns a dictionary where each key is the code for an SDMX source, and the values associated with each key are the code and name for the respective dataflows.\n\n\nCode\n#collapse_output\nfrom gingado.utils import list_all_dataflows\n\ndflows = list_all_dataflows()\ndflows\n\n\n\n\n\n--- SS without DSD ---\n{1: False}\n\n--- <class 'pandasdmx.message.StructureMessage'> ---\n{2: <pandasdmx.StructureMessage>\n  <Header>\n    id: 'IDREF73'\n    prepared: '2022-09-18T20:34:29.897766+02:00'\n    receiver: <Agency Unknown>\n    sender: <Agency Unknown>\n    source: \n    test: False}\n\n--- <class 'pandasdmx.model.DataStructureDefinition'> ---\n{'CLD_TPOP_SEX_AGE_GEO_NB': <DataStructureDefinition ILO:CLD_TPOP_SEX_AGE_GEO_NB(1.0)>, 'CLD_TPOP_SEX_AGE_NB': <DataStructureDefinition ILO:CLD_TPOP_SEX_AGE_NB(1.0)>, 'CLD_TPOP_SEX_AGE_STU_NB': <DataStructureDefinition ILO:CLD_TPOP_SEX_AGE_STU_NB(1.0)>, 'CLD_XCHD_SEX_AGE_NB': <DataStructureDefinition ILO:CLD_XCHD_SEX_AGE_NB(1.0)>, 'CLD_XCHL_SEX_AGE_ECO_NB': <DataStructureDefinition ILO:CLD_XCHL_SEX_AGE_ECO_NB(1.0)>, 'CLD_XCHL_SEX_AGE_GEO_NB': <DataStructureDefinition ILO:CLD_XCHL_SEX_AGE_GEO_NB(1.0)>, 'CLD_XCHL_SEX_AGE_GEO_RT': <DataStructureDefinition ILO:CLD_XCHL_SEX_AGE_GEO_RT(1.0)>, 'CLD_XCHL_SEX_AGE_NB': <DataStructureDefinition ILO:CLD_XCHL_SEX_AGE_NB(1.0)>, 'CLD_XCHL_SEX_AGE_RT': <DataStructureDefinition ILO:CLD_XCHL_SEX_AGE_RT(1.0)>, 'CLD_XCHL_SEX_AGE_STE_NB': <DataStructureDefinition ILO:CLD_XCHL_SEX_AGE_STE_NB(1.0)>, 'CLD_XCHL_SEX_AGE_STU_NB': <DataStructureDefinition ILO:CLD_XCHL_SEX_AGE_STU_NB(1.0)>, 'CLD_XCHL_SEX_AGE_STU_RT': <DataStructureDefinition ILO:CLD_XCHL_SEX_AGE_STU_RT(1.0)>, 'CLD_XCHN_SEX_AGE_GEO_NB': <DataStructureDefinition ILO:CLD_XCHN_SEX_AGE_GEO_NB(1.0)>, 'CLD_XCHS_SEX_AGE_GEO_NB': <DataStructureDefinition ILO:CLD_XCHS_SEX_AGE_GEO_NB(1.0)>, 'CLD_XHAD_SEX_AGE_NB': <DataStructureDefinition ILO:CLD_XHAD_SEX_AGE_NB(1.0)>, 'CLD_XHAN_SEX_AGE_GEO_NB': <DataStructureDefinition ILO:CLD_XHAN_SEX_AGE_GEO_NB(1.0)>, 'CLD_XHAS_SEX_AGE_GEO_NB': <DataStructureDefinition ILO:CLD_XHAS_SEX_AGE_GEO_NB(1.0)>, 'CLD_XHAZ_SEX_AGE_ECO_NB': <DataStructureDefinition ILO:CLD_XHAZ_SEX_AGE_ECO_NB(1.0)>, 'CLD_XHAZ_SEX_AGE_GEO_NB': <DataStructureDefinition ILO:CLD_XHAZ_SEX_AGE_GEO_NB(1.0)>, 'CLD_XHAZ_SEX_AGE_GEO_RT': <DataStructureDefinition ILO:CLD_XHAZ_SEX_AGE_GEO_RT(1.0)>, 'CLD_XHAZ_SEX_AGE_NB': <DataStructureDefinition ILO:CLD_XHAZ_SEX_AGE_NB(1.0)>, 'CLD_XHAZ_SEX_AGE_RT': <DataStructureDefinition ILO:CLD_XHAZ_SEX_AGE_RT(1.0)>, 'CLD_XHAZ_SEX_AGE_STE_NB': <DataStructureDefinition ILO:CLD_XHAZ_SEX_AGE_STE_NB(1.0)>, 'CLD_XHAZ_SEX_AGE_STU_NB': <DataStructureDefinition ILO:CLD_XHAZ_SEX_AGE_STU_NB(1.0)>, 'CLD_XHAZ_SEX_AGE_STU_RT': <DataStructureDefinition ILO:CLD_XHAZ_SEX_AGE_STU_RT(1.0)>, 'CLD_XSNA_SEX_AGE_ECO_NB': <DataStructureDefinition ILO:CLD_XSNA_SEX_AGE_ECO_NB(1.0)>, 'CLD_XSNA_SEX_AGE_GEO_NB': <DataStructureDefinition ILO:CLD_XSNA_SEX_AGE_GEO_NB(1.0)>, 'CLD_XSNA_SEX_AGE_GEO_RT': <DataStructureDefinition ILO:CLD_XSNA_SEX_AGE_GEO_RT(1.0)>, 'CLD_XSNA_SEX_AGE_NB': <DataStructureDefinition ILO:CLD_XSNA_SEX_AGE_NB(1.0)>, 'CLD_XSNA_SEX_AGE_RT': <DataStructureDefinition ILO:CLD_XSNA_SEX_AGE_RT(1.0)>, 'CLD_XSNA_SEX_AGE_STE_NB': <DataStructureDefinition ILO:CLD_XSNA_SEX_AGE_STE_NB(1.0)>, 'CLD_XSNA_SEX_AGE_STU_NB': <DataStructureDefinition ILO:CLD_XSNA_SEX_AGE_STU_NB(1.0)>, 'CLD_XSNA_SEX_AGE_STU_RT': <DataStructureDefinition ILO:CLD_XSNA_SEX_AGE_STU_RT(1.0)>, 'CLD_XSND_SEX_AGE_NB': <DataStructureDefinition ILO:CLD_XSND_SEX_AGE_NB(1.0)>, 'CLD_XSNN_SEX_AGE_GEO_NB': <DataStructureDefinition ILO:CLD_XSNN_SEX_AGE_GEO_NB(1.0)>, 'CLD_XSNS_SEX_AGE_GEO_NB': <DataStructureDefinition ILO:CLD_XSNS_SEX_AGE_GEO_NB(1.0)>, 'CPI_ACPI_COI_RT': <DataStructureDefinition ILO:CPI_ACPI_COI_RT(1.0)>, 'CPI_MCPI_COI_RT': <DataStructureDefinition ILO:CPI_MCPI_COI_RT(1.0)>, 'CPI_NCPD_COI_RT': <DataStructureDefinition ILO:CPI_NCPD_COI_RT(1.0)>, 'CPI_NCYR_COI_RT': <DataStructureDefinition ILO:CPI_NCYR_COI_RT(1.0)>, 'CPI_NWGT_COI_NB': <DataStructureDefinition ILO:CPI_NWGT_COI_NB(1.0)>, 'EAP_2EAP_SEX_AGE_GEO_NB': <DataStructureDefinition ILO:EAP_2EAP_SEX_AGE_GEO_NB(1.0)>, 'EAP_2EAP_SEX_AGE_NB': <DataStructureDefinition ILO:EAP_2EAP_SEX_AGE_NB(1.0)>, 'EAP_2MDN_SEX_NB': <DataStructureDefinition ILO:EAP_2MDN_SEX_NB(1.0)>, 'EAP_2WAP_SEX_AGE_GEO_RT': <DataStructureDefinition ILO:EAP_2WAP_SEX_AGE_GEO_RT(1.0)>, 'EAP_2WAP_SEX_AGE_RT': <DataStructureDefinition ILO:EAP_2WAP_SEX_AGE_RT(1.0)>, 'EAP_3EAP_SEX_AGE_DSB_NB': <DataStructureDefinition ILO:EAP_3EAP_SEX_AGE_DSB_NB(1.0)>, 'EAP_3EAP_SEX_AGE_EDU_NB': <DataStructureDefinition ILO:EAP_3EAP_SEX_AGE_EDU_NB(1.0)>, 'EAP_3EAP_SEX_AGE_GEO_NB': <DataStructureDefinition ILO:EAP_3EAP_SEX_AGE_GEO_NB(1.0)>, 'EAP_3EAP_SEX_AGE_STU_NB': <DataStructureDefinition ILO:EAP_3EAP_SEX_AGE_STU_NB(1.0)>, 'EAP_3WAP_SEX_AGE_DSB_RT': <DataStructureDefinition ILO:EAP_3WAP_SEX_AGE_DSB_RT(1.0)>, 'EAP_3WAP_SEX_AGE_EDU_RT': <DataStructureDefinition ILO:EAP_3WAP_SEX_AGE_EDU_RT(1.0)>, 'EAP_3WAP_SEX_AGE_GEO_RT': <DataStructureDefinition ILO:EAP_3WAP_SEX_AGE_GEO_RT(1.0)>, 'EAP_3WAP_SEX_AGE_STU_RT': <DataStructureDefinition ILO:EAP_3WAP_SEX_AGE_STU_RT(1.0)>, 'EAP_5EAP_SEX_AGE_NB': <DataStructureDefinition ILO:EAP_5EAP_SEX_AGE_NB(1.0)>, 'EAP_5WAP_SEX_AGE_RT': <DataStructureDefinition ILO:EAP_5WAP_SEX_AGE_RT(1.0)>, 'EAP_DWA1_SEX_AGE_RT': <DataStructureDefinition ILO:EAP_DWA1_SEX_AGE_RT(1.0)>, 'EAP_DWAP_SEX_AGE_DSB_RT': <DataStructureDefinition ILO:EAP_DWAP_SEX_AGE_DSB_RT(1.0)>, 'EAP_DWAP_SEX_AGE_EDU_RT': <DataStructureDefinition ILO:EAP_DWAP_SEX_AGE_EDU_RT(1.0)>, 'EAP_DWAP_SEX_AGE_GEO_RT': <DataStructureDefinition ILO:EAP_DWAP_SEX_AGE_GEO_RT(1.0)>, 'EAP_DWAP_SEX_AGE_MTS_RT': <DataStructureDefinition ILO:EAP_DWAP_SEX_AGE_MTS_RT(1.0)>, 'EAP_DWAP_SEX_AGE_RT': <DataStructureDefinition ILO:EAP_DWAP_SEX_AGE_RT(1.0)>, 'EAP_DWAP_SEX_EDU_DSB_RT': <DataStructureDefinition ILO:EAP_DWAP_SEX_EDU_DSB_RT(1.0)>, 'EAP_DWAP_SEX_EDU_GEO_RT': <DataStructureDefinition ILO:EAP_DWAP_SEX_EDU_GEO_RT(1.0)>, 'EAP_DWAP_SEX_EDU_MTS_RT': <DataStructureDefinition ILO:EAP_DWAP_SEX_EDU_MTS_RT(1.0)>, 'EAP_DWAP_SEX_EDU_RT': <DataStructureDefinition ILO:EAP_DWAP_SEX_EDU_RT(1.0)>, 'EAP_DWAP_SEX_GEO_MTS_RT': <DataStructureDefinition ILO:EAP_DWAP_SEX_GEO_MTS_RT(1.0)>, 'EAP_DWAP_SEX_MTS_RT': <DataStructureDefinition ILO:EAP_DWAP_SEX_MTS_RT(1.0)>, 'EAP_TEA1_SEX_AGE_NB': <DataStructureDefinition ILO:EAP_TEA1_SEX_AGE_NB(1.0)>, 'EAP_TEAP_SEX_AGE_DSB_NB': <DataStructureDefinition ILO:EAP_TEAP_SEX_AGE_DSB_NB(1.0)>, 'EAP_TEAP_SEX_AGE_EDU_NB': <DataStructureDefinition ILO:EAP_TEAP_SEX_AGE_EDU_NB(1.0)>, 'EAP_TEAP_SEX_AGE_GEO_NB': <DataStructureDefinition ILO:EAP_TEAP_SEX_AGE_GEO_NB(1.0)>, 'EAP_TEAP_SEX_AGE_MTS_NB': <DataStructureDefinition ILO:EAP_TEAP_SEX_AGE_MTS_NB(1.0)>, 'EAP_TEAP_SEX_AGE_NB': <DataStructureDefinition ILO:EAP_TEAP_SEX_AGE_NB(1.0)>, 'EAP_TEAP_SEX_DSB_NB': <DataStructureDefinition ILO:EAP_TEAP_SEX_DSB_NB(1.0)>, 'EAP_TEAP_SEX_EDU_DSB_NB': <DataStructureDefinition ILO:EAP_TEAP_SEX_EDU_DSB_NB(1.0)>, 'EAP_TEAP_SEX_EDU_GEO_NB': <DataStructureDefinition ILO:EAP_TEAP_SEX_EDU_GEO_NB(1.0)>, 'EAP_TEAP_SEX_EDU_MTS_NB': <DataStructureDefinition ILO:EAP_TEAP_SEX_EDU_MTS_NB(1.0)>, 'EAP_TEAP_SEX_EDU_NB': <DataStructureDefinition ILO:EAP_TEAP_SEX_EDU_NB(1.0)>, 'EAP_TEAP_SEX_GEO_MTS_NB': <DataStructureDefinition ILO:EAP_TEAP_SEX_GEO_MTS_NB(1.0)>, 'EAP_TEAP_SEX_MTS_NB': <DataStructureDefinition ILO:EAP_TEAP_SEX_MTS_NB(1.0)>, 'EAR_4HRL_SEX_OCU_CUR_NB': <DataStructureDefinition ILO:EAR_4HRL_SEX_OCU_CUR_NB(1.0)>, 'EAR_4MMN_CUR_NB': <DataStructureDefinition ILO:EAR_4MMN_CUR_NB(1.0)>, 'EAR_4MTH_SEX_DSB_CUR_NB': <DataStructureDefinition ILO:EAR_4MTH_SEX_DSB_CUR_NB(1.0)>, 'EAR_4MTH_SEX_ECO_CUR_NB': <DataStructureDefinition ILO:EAR_4MTH_SEX_ECO_CUR_NB(1.0)>, 'EAR_4MTH_SEX_OCU_CUR_NB': <DataStructureDefinition ILO:EAR_4MTH_SEX_OCU_CUR_NB(1.0)>, 'EAR_GGAP_OCU_RT': <DataStructureDefinition ILO:EAR_GGAP_OCU_RT(1.0)>, 'EAR_XEES_SEX_ECO_NB': <DataStructureDefinition ILO:EAR_XEES_SEX_ECO_NB(1.0)>, 'EAR_XFLS_NOC_RT': <DataStructureDefinition ILO:EAR_XFLS_NOC_RT(1.0)>, 'EAR_XTLP_SEX_RT': <DataStructureDefinition ILO:EAR_XTLP_SEX_RT(1.0)>, 'EES_3EES_SEX_AGE_JOB_NB': <DataStructureDefinition ILO:EES_3EES_SEX_AGE_JOB_NB(1.0)>, 'EES_TEES_AGE_EC2_NB': <DataStructureDefinition ILO:EES_TEES_AGE_EC2_NB(1.0)>, 'EES_TEES_AGE_OC2_NB': <DataStructureDefinition ILO:EES_TEES_AGE_OC2_NB(1.0)>, 'EES_TEES_ECO_OCU_NB': <DataStructureDefinition ILO:EES_TEES_ECO_OCU_NB(1.0)>, 'EES_TEES_SEX_AGE_JOB_NB': <DataStructureDefinition ILO:EES_TEES_SEX_AGE_JOB_NB(1.0)>, 'EES_TEES_SEX_EC2_NB': <DataStructureDefinition ILO:EES_TEES_SEX_EC2_NB(1.0)>, 'EES_TEES_SEX_ECO_NB': <DataStructureDefinition ILO:EES_TEES_SEX_ECO_NB(1.0)>, 'EES_TEES_SEX_HOW_NB': <DataStructureDefinition ILO:EES_TEES_SEX_HOW_NB(1.0)>, 'EES_TEES_SEX_INS_NB': <DataStructureDefinition ILO:EES_TEES_SEX_INS_NB(1.0)>, 'EES_TEES_SEX_OC2_NB': <DataStructureDefinition ILO:EES_TEES_SEX_OC2_NB(1.0)>, 'EES_TEES_SEX_OCU_NB': <DataStructureDefinition ILO:EES_TEES_SEX_OCU_NB(1.0)>, 'EES_XTMP_SEX_RT': <DataStructureDefinition ILO:EES_XTMP_SEX_RT(1.0)>, 'EIP_2EET_SEX_GEO_NB': <DataStructureDefinition ILO:EIP_2EET_SEX_GEO_NB(1.0)>, 'EIP_2EET_SEX_GEO_RT': <DataStructureDefinition ILO:EIP_2EET_SEX_GEO_RT(1.0)>, 'EIP_2EET_SEX_NB': <DataStructureDefinition ILO:EIP_2EET_SEX_NB(1.0)>, 'EIP_2EET_SEX_RT': <DataStructureDefinition ILO:EIP_2EET_SEX_RT(1.0)>, 'EIP_2EIP_SEX_AGE_NB': <DataStructureDefinition ILO:EIP_2EIP_SEX_AGE_NB(1.0)>, 'EIP_2PLF_SEX_AGE_GEO_NB': <DataStructureDefinition ILO:EIP_2PLF_SEX_AGE_GEO_NB(1.0)>, 'EIP_2PLF_SEX_AGE_NB': <DataStructureDefinition ILO:EIP_2PLF_SEX_AGE_NB(1.0)>, 'EIP_2PLF_SEX_AGE_RT': <DataStructureDefinition ILO:EIP_2PLF_SEX_AGE_RT(1.0)>, 'EIP_2WAP_SEX_AGE_GEO_RT': <DataStructureDefinition ILO:EIP_2WAP_SEX_AGE_GEO_RT(1.0)>, 'EIP_2WAP_SEX_AGE_RT': <DataStructureDefinition ILO:EIP_2WAP_SEX_AGE_RT(1.0)>, 'EIP_3DIS_SEX_AGE_GEO_NB': <DataStructureDefinition ILO:EIP_3DIS_SEX_AGE_GEO_NB(1.0)>, 'EIP_3EIP_SEX_AGE_DSB_NB': <DataStructureDefinition ILO:EIP_3EIP_SEX_AGE_DSB_NB(1.0)>, 'EIP_3EIP_SEX_AGE_EDU_NB': <DataStructureDefinition ILO:EIP_3EIP_SEX_AGE_EDU_NB(1.0)>, 'EIP_3EIP_SEX_AGE_GEO_NB': <DataStructureDefinition ILO:EIP_3EIP_SEX_AGE_GEO_NB(1.0)>, 'EIP_3EIP_SEX_AGE_STU_NB': <DataStructureDefinition ILO:EIP_3EIP_SEX_AGE_STU_NB(1.0)>, 'EIP_3WAP_SEX_AGE_DSB_RT': <DataStructureDefinition ILO:EIP_3WAP_SEX_AGE_DSB_RT(1.0)>, 'EIP_3WAP_SEX_AGE_EDU_RT': <DataStructureDefinition ILO:EIP_3WAP_SEX_AGE_EDU_RT(1.0)>, 'EIP_3WAP_SEX_AGE_GEO_RT': <DataStructureDefinition ILO:EIP_3WAP_SEX_AGE_GEO_RT(1.0)>, 'EIP_3WAP_SEX_AGE_STU_RT': <DataStructureDefinition ILO:EIP_3WAP_SEX_AGE_STU_RT(1.0)>, 'EIP_5EIP_SEX_AGE_NB': <DataStructureDefinition ILO:EIP_5EIP_SEX_AGE_NB(1.0)>, 'EIP_5PLF_SEX_AGE_NB': <DataStructureDefinition ILO:EIP_5PLF_SEX_AGE_NB(1.0)>, 'EIP_5WAP_SEX_AGE_RT': <DataStructureDefinition ILO:EIP_5WAP_SEX_AGE_RT(1.0)>, 'EIP_DWAP_SEX_AGE_DSB_RT': <DataStructureDefinition ILO:EIP_DWAP_SEX_AGE_DSB_RT(1.0)>, 'EIP_DWAP_SEX_AGE_EDU_RT': <DataStructureDefinition ILO:EIP_DWAP_SEX_AGE_EDU_RT(1.0)>, 'EIP_DWAP_SEX_AGE_GEO_RT': <DataStructureDefinition ILO:EIP_DWAP_SEX_AGE_GEO_RT(1.0)>, 'EIP_DWAP_SEX_AGE_MTS_RT': <DataStructureDefinition ILO:EIP_DWAP_SEX_AGE_MTS_RT(1.0)>, 'EIP_DWAP_SEX_AGE_RT': <DataStructureDefinition ILO:EIP_DWAP_SEX_AGE_RT(1.0)>, 'EIP_DWAP_SEX_EDU_DSB_RT': <DataStructureDefinition ILO:EIP_DWAP_SEX_EDU_DSB_RT(1.0)>, 'EIP_DWAP_SEX_EDU_GEO_RT': <DataStructureDefinition ILO:EIP_DWAP_SEX_EDU_GEO_RT(1.0)>, 'EIP_DWAP_SEX_EDU_MTS_RT': <DataStructureDefinition ILO:EIP_DWAP_SEX_EDU_MTS_RT(1.0)>, 'EIP_DWAP_SEX_EDU_RT': <DataStructureDefinition ILO:EIP_DWAP_SEX_EDU_RT(1.0)>, 'EIP_DWAP_SEX_GEO_MTS_RT': <DataStructureDefinition ILO:EIP_DWAP_SEX_GEO_MTS_RT(1.0)>, 'EIP_NEET_SEX_AGE_NB': <DataStructureDefinition ILO:EIP_NEET_SEX_AGE_NB(1.0)>, 'EIP_NEET_SEX_AGE_RT': <DataStructureDefinition ILO:EIP_NEET_SEX_AGE_RT(1.0)>, 'EIP_NEET_SEX_CBR_NB': <DataStructureDefinition ILO:EIP_NEET_SEX_CBR_NB(1.0)>, 'EIP_NEET_SEX_CBR_RT': <DataStructureDefinition ILO:EIP_NEET_SEX_CBR_RT(1.0)>, 'EIP_NEET_SEX_CCT_NB': <DataStructureDefinition ILO:EIP_NEET_SEX_CCT_NB(1.0)>, 'EIP_NEET_SEX_CCT_RT': <DataStructureDefinition ILO:EIP_NEET_SEX_CCT_RT(1.0)>, 'EIP_NEET_SEX_DSB_NB': <DataStructureDefinition ILO:EIP_NEET_SEX_DSB_NB(1.0)>, 'EIP_NEET_SEX_DSB_RT': <DataStructureDefinition ILO:EIP_NEET_SEX_DSB_RT(1.0)>, 'EIP_NEET_SEX_EDU_NB': <DataStructureDefinition ILO:EIP_NEET_SEX_EDU_NB(1.0)>, 'EIP_NEET_SEX_EDU_RT': <DataStructureDefinition ILO:EIP_NEET_SEX_EDU_RT(1.0)>, 'EIP_NEET_SEX_GEO_NB': <DataStructureDefinition ILO:EIP_NEET_SEX_GEO_NB(1.0)>, 'EIP_NEET_SEX_GEO_RT': <DataStructureDefinition ILO:EIP_NEET_SEX_GEO_RT(1.0)>, 'EIP_NEET_SEX_MTS_NB': <DataStructureDefinition ILO:EIP_NEET_SEX_MTS_NB(1.0)>, 'EIP_NEET_SEX_MTS_RT': <DataStructureDefinition ILO:EIP_NEET_SEX_MTS_RT(1.0)>, 'EIP_NEET_SEX_NB': <DataStructureDefinition ILO:EIP_NEET_SEX_NB(1.0)>, 'EIP_NEET_SEX_RT': <DataStructureDefinition ILO:EIP_NEET_SEX_RT(1.0)>, 'EIP_TEIP_SEX_AGE_DSB_NB': <DataStructureDefinition ILO:EIP_TEIP_SEX_AGE_DSB_NB(1.0)>, 'EIP_TEIP_SEX_AGE_EDU_NB': <DataStructureDefinition ILO:EIP_TEIP_SEX_AGE_EDU_NB(1.0)>, 'EIP_TEIP_SEX_AGE_GEO_NB': <DataStructureDefinition ILO:EIP_TEIP_SEX_AGE_GEO_NB(1.0)>, 'EIP_TEIP_SEX_AGE_MTS_NB': <DataStructureDefinition ILO:EIP_TEIP_SEX_AGE_MTS_NB(1.0)>, 'EIP_TEIP_SEX_AGE_NB': <DataStructureDefinition ILO:EIP_TEIP_SEX_AGE_NB(1.0)>, 'EIP_TEIP_SEX_EDU_DSB_NB': <DataStructureDefinition ILO:EIP_TEIP_SEX_EDU_DSB_NB(1.0)>, 'EIP_TEIP_SEX_EDU_GEO_NB': <DataStructureDefinition ILO:EIP_TEIP_SEX_EDU_GEO_NB(1.0)>, 'EIP_TEIP_SEX_EDU_MTS_NB': <DataStructureDefinition ILO:EIP_TEIP_SEX_EDU_MTS_NB(1.0)>, 'EIP_TEIP_SEX_EDU_NB': <DataStructureDefinition ILO:EIP_TEIP_SEX_EDU_NB(1.0)>, 'EIP_TEIP_SEX_GEO_MTS_NB': <DataStructureDefinition ILO:EIP_TEIP_SEX_GEO_MTS_NB(1.0)>, 'EIP_TEIP_SEX_MTS_NB': <DataStructureDefinition ILO:EIP_TEIP_SEX_MTS_NB(1.0)>, 'EIP_WDIS_SEX_AGE_GEO_NB': <DataStructureDefinition ILO:EIP_WDIS_SEX_AGE_GEO_NB(1.0)>, 'EIP_WDIS_SEX_AGE_MTS_NB': <DataStructureDefinition ILO:EIP_WDIS_SEX_AGE_MTS_NB(1.0)>, 'EIP_WDIS_SEX_AGE_NB': <DataStructureDefinition ILO:EIP_WDIS_SEX_AGE_NB(1.0)>, 'EIP_WDIS_SEX_EDU_GEO_NB': <DataStructureDefinition ILO:EIP_WDIS_SEX_EDU_GEO_NB(1.0)>, 'EIP_WDIS_SEX_EDU_MTS_NB': <DataStructureDefinition ILO:EIP_WDIS_SEX_EDU_MTS_NB(1.0)>, 'EIP_WDIS_SEX_EDU_NB': <DataStructureDefinition ILO:EIP_WDIS_SEX_EDU_NB(1.0)>, 'EIP_WDIS_SEX_GEO_MTS_NB': <DataStructureDefinition ILO:EIP_WDIS_SEX_GEO_MTS_NB(1.0)>, 'EIP_WPLF_SEX_AGE_GEO_NB': <DataStructureDefinition ILO:EIP_WPLF_SEX_AGE_GEO_NB(1.0)>, 'EIP_WPLF_SEX_AGE_MTS_NB': <DataStructureDefinition ILO:EIP_WPLF_SEX_AGE_MTS_NB(1.0)>, 'EIP_WPLF_SEX_AGE_NB': <DataStructureDefinition ILO:EIP_WPLF_SEX_AGE_NB(1.0)>, 'EIP_WPLF_SEX_EDU_GEO_NB': <DataStructureDefinition ILO:EIP_WPLF_SEX_EDU_GEO_NB(1.0)>, 'EIP_WPLF_SEX_EDU_MTS_NB': <DataStructureDefinition ILO:EIP_WPLF_SEX_EDU_MTS_NB(1.0)>, 'EIP_WPLF_SEX_EDU_NB': <DataStructureDefinition ILO:EIP_WPLF_SEX_EDU_NB(1.0)>, 'EIP_WPLF_SEX_GEO_MTS_NB': <DataStructureDefinition ILO:EIP_WPLF_SEX_GEO_MTS_NB(1.0)>, 'EMP_2EMP_AGE_STE_NB': <DataStructureDefinition ILO:EMP_2EMP_AGE_STE_NB(1.0)>, 'EMP_2EMP_SEX_AGE_CLA_NB': <DataStructureDefinition ILO:EMP_2EMP_SEX_AGE_CLA_NB(1.0)>, 'EMP_2EMP_SEX_AGE_GEO_NB': <DataStructureDefinition ILO:EMP_2EMP_SEX_AGE_GEO_NB(1.0)>, 'EMP_2EMP_SEX_AGE_NB': <DataStructureDefinition ILO:EMP_2EMP_SEX_AGE_NB(1.0)>, 'EMP_2EMP_SEX_ECO_NB': <DataStructureDefinition ILO:EMP_2EMP_SEX_ECO_NB(1.0)>, 'EMP_2EMP_SEX_GEO_ECO_NB': <DataStructureDefinition ILO:EMP_2EMP_SEX_GEO_ECO_NB(1.0)>, 'EMP_2EMP_SEX_GEO_OCU_NB': <DataStructureDefinition ILO:EMP_2EMP_SEX_GEO_OCU_NB(1.0)>, 'EMP_2EMP_SEX_GEO_STE_NB': <DataStructureDefinition ILO:EMP_2EMP_SEX_GEO_STE_NB(1.0)>, 'EMP_2EMP_SEX_OCU_NB': <DataStructureDefinition ILO:EMP_2EMP_SEX_OCU_NB(1.0)>, 'EMP_2EMP_SEX_STE_NB': <DataStructureDefinition ILO:EMP_2EMP_SEX_STE_NB(1.0)>, 'EMP_2TRU_SEX_AGE_GEO_NB': <DataStructureDefinition ILO:EMP_2TRU_SEX_AGE_GEO_NB(1.0)>, 'EMP_2TRU_SEX_AGE_NB': <DataStructureDefinition ILO:EMP_2TRU_SEX_AGE_NB(1.0)>, 'EMP_2TRU_SEX_AGE_RT': <DataStructureDefinition ILO:EMP_2TRU_SEX_AGE_RT(1.0)>, 'EMP_2WAP_SEX_AGE_GEO_RT': <DataStructureDefinition ILO:EMP_2WAP_SEX_AGE_GEO_RT(1.0)>, 'EMP_2WAP_SEX_AGE_RT': <DataStructureDefinition ILO:EMP_2WAP_SEX_AGE_RT(1.0)>, 'EMP_3EMP_SEX_AGE_DSB_NB': <DataStructureDefinition ILO:EMP_3EMP_SEX_AGE_DSB_NB(1.0)>, 'EMP_3EMP_SEX_AGE_ECO_NB': <DataStructureDefinition ILO:EMP_3EMP_SEX_AGE_ECO_NB(1.0)>, 'EMP_3EMP_SEX_AGE_EDU_NB': <DataStructureDefinition ILO:EMP_3EMP_SEX_AGE_EDU_NB(1.0)>, 'EMP_3EMP_SEX_AGE_GEO_NB': <DataStructureDefinition ILO:EMP_3EMP_SEX_AGE_GEO_NB(1.0)>, 'EMP_3EMP_SEX_AGE_HOW_NB': <DataStructureDefinition ILO:EMP_3EMP_SEX_AGE_HOW_NB(1.0)>, 'EMP_3EMP_SEX_AGE_JOB_NB': <DataStructureDefinition ILO:EMP_3EMP_SEX_AGE_JOB_NB(1.0)>, 'EMP_3EMP_SEX_AGE_OCU_NB': <DataStructureDefinition ILO:EMP_3EMP_SEX_AGE_OCU_NB(1.0)>, 'EMP_3EMP_SEX_AGE_STE_NB': <DataStructureDefinition ILO:EMP_3EMP_SEX_AGE_STE_NB(1.0)>, 'EMP_3EMP_SEX_AGE_STU_NB': <DataStructureDefinition ILO:EMP_3EMP_SEX_AGE_STU_NB(1.0)>, 'EMP_3WAP_SEX_AGE_DSB_RT': <DataStructureDefinition ILO:EMP_3WAP_SEX_AGE_DSB_RT(1.0)>, 'EMP_3WAP_SEX_AGE_EDU_RT': <DataStructureDefinition ILO:EMP_3WAP_SEX_AGE_EDU_RT(1.0)>, 'EMP_3WAP_SEX_AGE_GEO_RT': <DataStructureDefinition ILO:EMP_3WAP_SEX_AGE_GEO_RT(1.0)>, 'EMP_3WAP_SEX_AGE_STU_RT': <DataStructureDefinition ILO:EMP_3WAP_SEX_AGE_STU_RT(1.0)>, 'EMP_5EMP_SEX_AGE_NB': <DataStructureDefinition ILO:EMP_5EMP_SEX_AGE_NB(1.0)>, 'EMP_5NIF_SEX_AGE_NB': <DataStructureDefinition ILO:EMP_5NIF_SEX_AGE_NB(1.0)>, 'EMP_5NIF_SEX_AGE_RT': <DataStructureDefinition ILO:EMP_5NIF_SEX_AGE_RT(1.0)>, 'EMP_5PIF_SEX_AGE_NB': <DataStructureDefinition ILO:EMP_5PIF_SEX_AGE_NB(1.0)>, 'EMP_5PIF_SEX_AGE_RT': <DataStructureDefinition ILO:EMP_5PIF_SEX_AGE_RT(1.0)>, 'EMP_5WAP_SEX_AGE_RT': <DataStructureDefinition ILO:EMP_5WAP_SEX_AGE_RT(1.0)>, 'EMP_DWA1_SEX_AGE_RT': <DataStructureDefinition ILO:EMP_DWA1_SEX_AGE_RT(1.0)>, 'EMP_DWAP_SEX_AGE_DSB_RT': <DataStructureDefinition ILO:EMP_DWAP_SEX_AGE_DSB_RT(1.0)>, 'EMP_DWAP_SEX_AGE_EDU_RT': <DataStructureDefinition ILO:EMP_DWAP_SEX_AGE_EDU_RT(1.0)>, 'EMP_DWAP_SEX_AGE_GEO_RT': <DataStructureDefinition ILO:EMP_DWAP_SEX_AGE_GEO_RT(1.0)>, 'EMP_DWAP_SEX_AGE_MTS_RT': <DataStructureDefinition ILO:EMP_DWAP_SEX_AGE_MTS_RT(1.0)>, 'EMP_DWAP_SEX_AGE_RT': <DataStructureDefinition ILO:EMP_DWAP_SEX_AGE_RT(1.0)>, 'EMP_DWAP_SEX_EDU_DSB_RT': <DataStructureDefinition ILO:EMP_DWAP_SEX_EDU_DSB_RT(1.0)>, 'EMP_DWAP_SEX_EDU_GEO_RT': <DataStructureDefinition ILO:EMP_DWAP_SEX_EDU_GEO_RT(1.0)>, 'EMP_DWAP_SEX_EDU_MTS_RT': <DataStructureDefinition ILO:EMP_DWAP_SEX_EDU_MTS_RT(1.0)>, 'EMP_DWAP_SEX_EDU_RT': <DataStructureDefinition ILO:EMP_DWAP_SEX_EDU_RT(1.0)>, 'EMP_DWAP_SEX_GEO_MTS_RT': <DataStructureDefinition ILO:EMP_DWAP_SEX_GEO_MTS_RT(1.0)>, 'EMP_DWAP_SEX_MTS_RT': <DataStructureDefinition ILO:EMP_DWAP_SEX_MTS_RT(1.0)>, 'EMP_NIFL_SEX_AGE_NB': <DataStructureDefinition ILO:EMP_NIFL_SEX_AGE_NB(1.0)>, 'EMP_NIFL_SEX_AGE_RT': <DataStructureDefinition ILO:EMP_NIFL_SEX_AGE_RT(1.0)>, 'EMP_NIFL_SEX_DSB_NB': <DataStructureDefinition ILO:EMP_NIFL_SEX_DSB_NB(1.0)>, 'EMP_NIFL_SEX_DSB_RT': <DataStructureDefinition ILO:EMP_NIFL_SEX_DSB_RT(1.0)>, 'EMP_NIFL_SEX_ECO_NB': <DataStructureDefinition ILO:EMP_NIFL_SEX_ECO_NB(1.0)>, 'EMP_NIFL_SEX_ECO_RT': <DataStructureDefinition ILO:EMP_NIFL_SEX_ECO_RT(1.0)>}\n\n--- <class 'pandasdmx.model.Agency'> ---\n{'ILO': <Agency ILO>}\n\n--- <class 'pandasdmx.model.DataflowDefinition'> ---\n{'DF_CLD_TPOP_SEX_AGE_GEO_NB': <DataflowDefinition ILO:DF_CLD_TPOP_SEX_AGE_GEO_NB(1.0): Child population by sex, age and rural / urban areas>, 'DF_CLD_TPOP_SEX_AGE_NB': <DataflowDefinition ILO:DF_CLD_TPOP_SEX_AGE_NB(1.0): Child population by sex and age>, 'DF_CLD_TPOP_SEX_AGE_STU_NB': <DataflowDefinition ILO:DF_CLD_TPOP_SEX_AGE_STU_NB(1.0): Child population by sex, age and school attendance>, 'DF_CLD_XCHD_SEX_AGE_NB': <DataflowDefinition ILO:DF_CLD_XCHD_SEX_AGE_NB(1.0): Children in child labour in domestic work by sex and age>, 'DF_CLD_XCHL_SEX_AGE_ECO_NB': <DataflowDefinition ILO:DF_CLD_XCHL_SEX_AGE_ECO_NB(1.0): Children in child labour by sex, age and economic activity>, 'DF_CLD_XCHL_SEX_AGE_GEO_NB': <DataflowDefinition ILO:DF_CLD_XCHL_SEX_AGE_GEO_NB(1.0): Children in child labour by sex, age and rural / urban areas>, 'DF_CLD_XCHL_SEX_AGE_GEO_RT': <DataflowDefinition ILO:DF_CLD_XCHL_SEX_AGE_GEO_RT(1.0): Share of children in child labour by sex, age and and rural / urban areas>, 'DF_CLD_XCHL_SEX_AGE_NB': <DataflowDefinition ILO:DF_CLD_XCHL_SEX_AGE_NB(1.0): Children in child labour by sex and age>, 'DF_CLD_XCHL_SEX_AGE_RT': <DataflowDefinition ILO:DF_CLD_XCHL_SEX_AGE_RT(1.0): Share of children in child labour by sex and age>, 'DF_CLD_XCHL_SEX_AGE_STE_NB': <DataflowDefinition ILO:DF_CLD_XCHL_SEX_AGE_STE_NB(1.0): Children in child labour by sex, age and status in employment>, 'DF_CLD_XCHL_SEX_AGE_STU_NB': <DataflowDefinition ILO:DF_CLD_XCHL_SEX_AGE_STU_NB(1.0): Children in child labour by sex, age and school attendance status>, 'DF_CLD_XCHL_SEX_AGE_STU_RT': <DataflowDefinition ILO:DF_CLD_XCHL_SEX_AGE_STU_RT(1.0): Share of children in child labour by sex, age and school attendance>, 'DF_CLD_XCHN_SEX_AGE_GEO_NB': <DataflowDefinition ILO:DF_CLD_XCHN_SEX_AGE_GEO_NB(1.0): Children in child labour not attending school by sex, age and rural / urban areas>, 'DF_CLD_XCHS_SEX_AGE_GEO_NB': <DataflowDefinition ILO:DF_CLD_XCHS_SEX_AGE_GEO_NB(1.0): Children in child labour attending school by sex, age and rural / urban areas>, 'DF_CLD_XHAD_SEX_AGE_NB': <DataflowDefinition ILO:DF_CLD_XHAD_SEX_AGE_NB(1.0): Children in hazardous domestic work by sex and age>, 'DF_CLD_XHAN_SEX_AGE_GEO_NB': <DataflowDefinition ILO:DF_CLD_XHAN_SEX_AGE_GEO_NB(1.0): Children in hazardous work not attending school by sex, age and rural / urban areas>, 'DF_CLD_XHAS_SEX_AGE_GEO_NB': <DataflowDefinition ILO:DF_CLD_XHAS_SEX_AGE_GEO_NB(1.0): Children in hazardous work attending school by sex, age and rural / urban areas>, 'DF_CLD_XHAZ_SEX_AGE_ECO_NB': <DataflowDefinition ILO:DF_CLD_XHAZ_SEX_AGE_ECO_NB(1.0): Children in hazardous work by sex, age and economic activity>, 'DF_CLD_XHAZ_SEX_AGE_GEO_NB': <DataflowDefinition ILO:DF_CLD_XHAZ_SEX_AGE_GEO_NB(1.0): Children in hazardous work by sex, age and rural / urban areas>, 'DF_CLD_XHAZ_SEX_AGE_GEO_RT': <DataflowDefinition ILO:DF_CLD_XHAZ_SEX_AGE_GEO_RT(1.0): Share of children in hazardous work by sex, age and rural / urban areas>, 'DF_CLD_XHAZ_SEX_AGE_NB': <DataflowDefinition ILO:DF_CLD_XHAZ_SEX_AGE_NB(1.0): Children in hazardous work by sex and age>, 'DF_CLD_XHAZ_SEX_AGE_RT': <DataflowDefinition ILO:DF_CLD_XHAZ_SEX_AGE_RT(1.0): Share of children in hazardous work by sex and age>, 'DF_CLD_XHAZ_SEX_AGE_STE_NB': <DataflowDefinition ILO:DF_CLD_XHAZ_SEX_AGE_STE_NB(1.0): Children in hazardous work by sex, age and status in employment>, 'DF_CLD_XHAZ_SEX_AGE_STU_NB': <DataflowDefinition ILO:DF_CLD_XHAZ_SEX_AGE_STU_NB(1.0): Children in hazardous work by sex, age and school attendance status>, 'DF_CLD_XHAZ_SEX_AGE_STU_RT': <DataflowDefinition ILO:DF_CLD_XHAZ_SEX_AGE_STU_RT(1.0): Share of children in hazardous work by sex, age and school attendance>, 'DF_CLD_XSNA_SEX_AGE_ECO_NB': <DataflowDefinition ILO:DF_CLD_XSNA_SEX_AGE_ECO_NB(1.0): Children in employment by sex, age and economic activity>, 'DF_CLD_XSNA_SEX_AGE_GEO_NB': <DataflowDefinition ILO:DF_CLD_XSNA_SEX_AGE_GEO_NB(1.0): Children in employment by sex, age and rural / urban areas>, 'DF_CLD_XSNA_SEX_AGE_GEO_RT': <DataflowDefinition ILO:DF_CLD_XSNA_SEX_AGE_GEO_RT(1.0): Share of children engaged in economic activity by sex, age and rural / urban areas>, 'DF_CLD_XSNA_SEX_AGE_NB': <DataflowDefinition ILO:DF_CLD_XSNA_SEX_AGE_NB(1.0): Children in employment by sex and age>, 'DF_CLD_XSNA_SEX_AGE_RT': <DataflowDefinition ILO:DF_CLD_XSNA_SEX_AGE_RT(1.0): Share of children engaged in economic activity by sex and age>, 'DF_CLD_XSNA_SEX_AGE_STE_NB': <DataflowDefinition ILO:DF_CLD_XSNA_SEX_AGE_STE_NB(1.0): Children in employment by sex, age and status in employment>, 'DF_CLD_XSNA_SEX_AGE_STU_NB': <DataflowDefinition ILO:DF_CLD_XSNA_SEX_AGE_STU_NB(1.0): Children in employment by sex, age and school attendance status>, 'DF_CLD_XSNA_SEX_AGE_STU_RT': <DataflowDefinition ILO:DF_CLD_XSNA_SEX_AGE_STU_RT(1.0): Share of children engaged in economic activity by sex, age and school attendance>, 'DF_CLD_XSND_SEX_AGE_NB': <DataflowDefinition ILO:DF_CLD_XSND_SEX_AGE_NB(1.0): Children in domestic work by sex and age>, 'DF_CLD_XSNN_SEX_AGE_GEO_NB': <DataflowDefinition ILO:DF_CLD_XSNN_SEX_AGE_GEO_NB(1.0): Children in employment not attending school by sex, age and rural / urban areas>, 'DF_CLD_XSNS_SEX_AGE_GEO_NB': <DataflowDefinition ILO:DF_CLD_XSNS_SEX_AGE_GEO_NB(1.0): Children in employment attending school by sex, age and rural / urban areas>, 'DF_CPI_ACPI_COI_RT': <DataflowDefinition ILO:DF_CPI_ACPI_COI_RT(1.0): National consumer price index (CPI), annual rate of change - discontinued>, 'DF_CPI_MCPI_COI_RT': <DataflowDefinition ILO:DF_CPI_MCPI_COI_RT(1.0): National consumer price index (CPI), monthly rate of change - discontinued (Sub-annual)>, 'DF_CPI_NCPD_COI_RT': <DataflowDefinition ILO:DF_CPI_NCPD_COI_RT(1.0): National consumer price index (CPI) by COICOP, percentage change from previous period (Sub-annual)>, 'DF_CPI_NCYR_COI_RT': <DataflowDefinition ILO:DF_CPI_NCYR_COI_RT(1.0): National consumer price index (CPI) by COICOP, percentage change from previous year>, 'DF_CPI_NWGT_COI_NB': <DataflowDefinition ILO:DF_CPI_NWGT_COI_NB(1.0): National consumer price index (CPI) by COICOP, country weights>, 'DF_EAP_2EAP_SEX_AGE_GEO_NB': <DataflowDefinition ILO:DF_EAP_2EAP_SEX_AGE_GEO_NB(1.0): Labour force by sex, age and rural / urban areas -- ILO modelled estimates, Nov. 2021>, 'DF_EAP_2EAP_SEX_AGE_NB': <DataflowDefinition ILO:DF_EAP_2EAP_SEX_AGE_NB(1.0): Labour force by sex and age -- ILO modelled estimates, Nov. 2021>, 'DF_EAP_2MDN_SEX_NB': <DataflowDefinition ILO:DF_EAP_2MDN_SEX_NB(1.0): Median age of the labour force by sex -- ILO modelled estimates, Nov. 2020>, 'DF_EAP_2WAP_SEX_AGE_GEO_RT': <DataflowDefinition ILO:DF_EAP_2WAP_SEX_AGE_GEO_RT(1.0): Labour force participation rate by sex, age and rural / urban areas -- ILO modelled estimates, Nov. 2021>, 'DF_EAP_2WAP_SEX_AGE_RT': <DataflowDefinition ILO:DF_EAP_2WAP_SEX_AGE_RT(1.0): Labour force participation rate by sex and age -- ILO modelled estimates, Nov. 2021>, 'DF_EAP_3EAP_SEX_AGE_DSB_NB': <DataflowDefinition ILO:DF_EAP_3EAP_SEX_AGE_DSB_NB(1.0): Youth labour force by sex, age and disability status>, 'DF_EAP_3EAP_SEX_AGE_EDU_NB': <DataflowDefinition ILO:DF_EAP_3EAP_SEX_AGE_EDU_NB(1.0): Youth labour force by sex, age and education>, 'DF_EAP_3EAP_SEX_AGE_GEO_NB': <DataflowDefinition ILO:DF_EAP_3EAP_SEX_AGE_GEO_NB(1.0): Youth labour force by sex, age and rural / urban areas>, 'DF_EAP_3EAP_SEX_AGE_STU_NB': <DataflowDefinition ILO:DF_EAP_3EAP_SEX_AGE_STU_NB(1.0): Youth labour force by sex, age and school attendance status>, 'DF_EAP_3WAP_SEX_AGE_DSB_RT': <DataflowDefinition ILO:DF_EAP_3WAP_SEX_AGE_DSB_RT(1.0): Youth labour force participation rate by sex, age and disability status>, 'DF_EAP_3WAP_SEX_AGE_EDU_RT': <DataflowDefinition ILO:DF_EAP_3WAP_SEX_AGE_EDU_RT(1.0): Youth labour force participation rate by sex, age and education>, 'DF_EAP_3WAP_SEX_AGE_GEO_RT': <DataflowDefinition ILO:DF_EAP_3WAP_SEX_AGE_GEO_RT(1.0): Youth labour force participation rate by sex, age and rural / urban areas>, 'DF_EAP_3WAP_SEX_AGE_STU_RT': <DataflowDefinition ILO:DF_EAP_3WAP_SEX_AGE_STU_RT(1.0): Youth labour force participation rate by sex, age and school attendance status>, 'DF_EAP_5EAP_SEX_AGE_NB': <DataflowDefinition ILO:DF_EAP_5EAP_SEX_AGE_NB(1.0): Labour force by sex and age -- 19th ICLS>, 'DF_EAP_5WAP_SEX_AGE_RT': <DataflowDefinition ILO:DF_EAP_5WAP_SEX_AGE_RT(1.0): Labour force participation rate by sex and age -- 19th ICLS>, 'DF_EAP_DWA1_SEX_AGE_RT': <DataflowDefinition ILO:DF_EAP_DWA1_SEX_AGE_RT(1.0): Labour force participation rate by sex and age, seasonally adjusted series (Sub-annual)>, 'DF_EAP_DWAP_SEX_AGE_DSB_RT': <DataflowDefinition ILO:DF_EAP_DWAP_SEX_AGE_DSB_RT(1.0): Labour force participation rate by sex, age and disability status>, 'DF_EAP_DWAP_SEX_AGE_EDU_RT': <DataflowDefinition ILO:DF_EAP_DWAP_SEX_AGE_EDU_RT(1.0): Labour force participation rate by sex, age and education>, 'DF_EAP_DWAP_SEX_AGE_GEO_RT': <DataflowDefinition ILO:DF_EAP_DWAP_SEX_AGE_GEO_RT(1.0): Labour force participation rate by sex, age and rural / urban areas>, 'DF_EAP_DWAP_SEX_AGE_MTS_RT': <DataflowDefinition ILO:DF_EAP_DWAP_SEX_AGE_MTS_RT(1.0): Labour force participation rate by sex, age and marital status>, 'DF_EAP_DWAP_SEX_AGE_RT': <DataflowDefinition ILO:DF_EAP_DWAP_SEX_AGE_RT(1.0): Labour force participation rate by sex and age>, 'DF_EAP_DWAP_SEX_EDU_DSB_RT': <DataflowDefinition ILO:DF_EAP_DWAP_SEX_EDU_DSB_RT(1.0): Labour force participation rate by sex, education and disability status>, 'DF_EAP_DWAP_SEX_EDU_GEO_RT': <DataflowDefinition ILO:DF_EAP_DWAP_SEX_EDU_GEO_RT(1.0): Labour force participation rate by sex, education and rural / urban areas>, 'DF_EAP_DWAP_SEX_EDU_MTS_RT': <DataflowDefinition ILO:DF_EAP_DWAP_SEX_EDU_MTS_RT(1.0): Labour force participation rate by sex, education and marital status>, 'DF_EAP_DWAP_SEX_EDU_RT': <DataflowDefinition ILO:DF_EAP_DWAP_SEX_EDU_RT(1.0): Labour force participation rate by sex and education>, 'DF_EAP_DWAP_SEX_GEO_MTS_RT': <DataflowDefinition ILO:DF_EAP_DWAP_SEX_GEO_MTS_RT(1.0): Labour force participation rate by sex, rural / urban area and marital status>, 'DF_EAP_DWAP_SEX_MTS_RT': <DataflowDefinition ILO:DF_EAP_DWAP_SEX_MTS_RT(1.0): Labour force participation rate by sex and marital status>, 'DF_EAP_TEA1_SEX_AGE_NB': <DataflowDefinition ILO:DF_EAP_TEA1_SEX_AGE_NB(1.0): Labour force by sex and age, seasonally adjusted series (Sub-annual)>, 'DF_EAP_TEAP_SEX_AGE_DSB_NB': <DataflowDefinition ILO:DF_EAP_TEAP_SEX_AGE_DSB_NB(1.0): Labour force by sex, age and disability status>, 'DF_EAP_TEAP_SEX_AGE_EDU_NB': <DataflowDefinition ILO:DF_EAP_TEAP_SEX_AGE_EDU_NB(1.0): Labour force by sex, age and education>, 'DF_EAP_TEAP_SEX_AGE_GEO_NB': <DataflowDefinition ILO:DF_EAP_TEAP_SEX_AGE_GEO_NB(1.0): Labour force by sex, age and rural / urban areas>, 'DF_EAP_TEAP_SEX_AGE_MTS_NB': <DataflowDefinition ILO:DF_EAP_TEAP_SEX_AGE_MTS_NB(1.0): Labour force by sex, age and marital status>, 'DF_EAP_TEAP_SEX_AGE_NB': <DataflowDefinition ILO:DF_EAP_TEAP_SEX_AGE_NB(1.0): Labour force by sex and age>, 'DF_EAP_TEAP_SEX_DSB_NB': <DataflowDefinition ILO:DF_EAP_TEAP_SEX_DSB_NB(1.0): Labour force by sex and disability status>, 'DF_EAP_TEAP_SEX_EDU_DSB_NB': <DataflowDefinition ILO:DF_EAP_TEAP_SEX_EDU_DSB_NB(1.0): Labour force by sex, education and disability status>, 'DF_EAP_TEAP_SEX_EDU_GEO_NB': <DataflowDefinition ILO:DF_EAP_TEAP_SEX_EDU_GEO_NB(1.0): Labour force by sex, education and rural / urban areas>, 'DF_EAP_TEAP_SEX_EDU_MTS_NB': <DataflowDefinition ILO:DF_EAP_TEAP_SEX_EDU_MTS_NB(1.0): Labour force by sex, education and marital status>, 'DF_EAP_TEAP_SEX_EDU_NB': <DataflowDefinition ILO:DF_EAP_TEAP_SEX_EDU_NB(1.0): Labour force by sex and education>, 'DF_EAP_TEAP_SEX_GEO_MTS_NB': <DataflowDefinition ILO:DF_EAP_TEAP_SEX_GEO_MTS_NB(1.0): Labour force by sex, rural / urban area and marital status>, 'DF_EAP_TEAP_SEX_MTS_NB': <DataflowDefinition ILO:DF_EAP_TEAP_SEX_MTS_NB(1.0): Labour force by sex and marital status>, 'DF_EAR_4HRL_SEX_OCU_CUR_NB': <DataflowDefinition ILO:DF_EAR_4HRL_SEX_OCU_CUR_NB(1.0): Average hourly earnings of employees by sex and occupation>, 'DF_EAR_4MMN_CUR_NB': <DataflowDefinition ILO:DF_EAR_4MMN_CUR_NB(1.0): Statutory nominal gross monthly minimum wage>, 'DF_EAR_4MTH_SEX_DSB_CUR_NB': <DataflowDefinition ILO:DF_EAR_4MTH_SEX_DSB_CUR_NB(1.0): Average monthly earnings of employees by sex and disability status>, 'DF_EAR_4MTH_SEX_ECO_CUR_NB': <DataflowDefinition ILO:DF_EAR_4MTH_SEX_ECO_CUR_NB(1.0): Average monthly earnings of employees by sex and economic activity>, 'DF_EAR_4MTH_SEX_OCU_CUR_NB': <DataflowDefinition ILO:DF_EAR_4MTH_SEX_OCU_CUR_NB(1.0): Average monthly earnings of employees by sex and occupation>, 'DF_EAR_GGAP_OCU_RT': <DataflowDefinition ILO:DF_EAR_GGAP_OCU_RT(1.0): Gender wage gap by occupation>, 'DF_EAR_XEES_SEX_ECO_NB': <DataflowDefinition ILO:DF_EAR_XEES_SEX_ECO_NB(1.0): Mean nominal monthly earnings of employees by sex and economic activity (Sub-annual)>, 'DF_EAR_XFLS_NOC_RT': <DataflowDefinition ILO:DF_EAR_XFLS_NOC_RT(1.0): Female share of low pay earners>, 'DF_EAR_XTLP_SEX_RT': <DataflowDefinition ILO:DF_EAR_XTLP_SEX_RT(1.0): Low pay rate by sex>, 'DF_EES_3EES_SEX_AGE_JOB_NB': <DataflowDefinition ILO:DF_EES_3EES_SEX_AGE_JOB_NB(1.0): Youth employees by sex, age and type of job contract>, 'DF_EES_TEES_AGE_EC2_NB': <DataflowDefinition ILO:DF_EES_TEES_AGE_EC2_NB(1.0): Employees by age and economic activity - ISIC level 2>, 'DF_EES_TEES_AGE_OC2_NB': <DataflowDefinition ILO:DF_EES_TEES_AGE_OC2_NB(1.0): Employees by age and occupation - ISCO level 2>, 'DF_EES_TEES_ECO_OCU_NB': <DataflowDefinition ILO:DF_EES_TEES_ECO_OCU_NB(1.0): Employees by economic activity and occupation>, 'DF_EES_TEES_SEX_AGE_JOB_NB': <DataflowDefinition ILO:DF_EES_TEES_SEX_AGE_JOB_NB(1.0): Employees by sex, age and temporary or permanent contract>, 'DF_EES_TEES_SEX_EC2_NB': <DataflowDefinition ILO:DF_EES_TEES_SEX_EC2_NB(1.0): Employees by sex and economic activity - ISIC level 2>, 'DF_EES_TEES_SEX_ECO_NB': <DataflowDefinition ILO:DF_EES_TEES_SEX_ECO_NB(1.0): Employees by sex and economic activity>, 'DF_EES_TEES_SEX_HOW_NB': <DataflowDefinition ILO:DF_EES_TEES_SEX_HOW_NB(1.0): Employees by sex and weekly hours actually worked>, 'DF_EES_TEES_SEX_INS_NB': <DataflowDefinition ILO:DF_EES_TEES_SEX_INS_NB(1.0): Employees by sex and institutional sector>, 'DF_EES_TEES_SEX_OC2_NB': <DataflowDefinition ILO:DF_EES_TEES_SEX_OC2_NB(1.0): Employees by sex and occupation - ISCO level 2>, 'DF_EES_TEES_SEX_OCU_NB': <DataflowDefinition ILO:DF_EES_TEES_SEX_OCU_NB(1.0): Employees by sex and occupation>, 'DF_EES_XTMP_SEX_RT': <DataflowDefinition ILO:DF_EES_XTMP_SEX_RT(1.0): Share of temporary employees by sex>, 'DF_EIP_2EET_SEX_GEO_NB': <DataflowDefinition ILO:DF_EIP_2EET_SEX_GEO_NB(1.0): Youth not in employment, education or training (NEET) by sex and rural / urban areas -- ILO modelled estimates, Nov. 2020>, 'DF_EIP_2EET_SEX_GEO_RT': <DataflowDefinition ILO:DF_EIP_2EET_SEX_GEO_RT(1.0): Share of youth not in employment, education or training (NEET) by sex and rural / urban areas-- ILO modelled estimates, Nov. 2020>, 'DF_EIP_2EET_SEX_NB': <DataflowDefinition ILO:DF_EIP_2EET_SEX_NB(1.0): Youth not in employment, education or training (NEET) by sex -- ILO modelled estimates, Nov. 2021>, 'DF_EIP_2EET_SEX_RT': <DataflowDefinition ILO:DF_EIP_2EET_SEX_RT(1.0): Share of youth not in employment, education or training (NEET) by sex -- ILO modelled estimates, Nov. 2021>, 'DF_EIP_2EIP_SEX_AGE_NB': <DataflowDefinition ILO:DF_EIP_2EIP_SEX_AGE_NB(1.0): Persons outside the labour force by sex and age -- ILO modelled estimates, Nov. 2021>, 'DF_EIP_2PLF_SEX_AGE_GEO_NB': <DataflowDefinition ILO:DF_EIP_2PLF_SEX_AGE_GEO_NB(1.0): Potential labour force by sex, age and rural / urban areas -- ILO modelled estimates, Nov. 2020>, 'DF_EIP_2PLF_SEX_AGE_NB': <DataflowDefinition ILO:DF_EIP_2PLF_SEX_AGE_NB(1.0): Potential labour force by sex and age -- ILO modelled estimates, Nov. 2021>, 'DF_EIP_2PLF_SEX_AGE_RT': <DataflowDefinition ILO:DF_EIP_2PLF_SEX_AGE_RT(1.0): Potential labour force rate by sex and age -- ILO modelled estimates, Nov. 2021>, 'DF_EIP_2WAP_SEX_AGE_GEO_RT': <DataflowDefinition ILO:DF_EIP_2WAP_SEX_AGE_GEO_RT(1.0): Inactivity rate by sex, age and rural / urban areas -- ILO modelled estimates, Nov. 2021>, 'DF_EIP_2WAP_SEX_AGE_RT': <DataflowDefinition ILO:DF_EIP_2WAP_SEX_AGE_RT(1.0): Inactivity rate by sex and age -- ILO modelled estimates, Nov. 2021>, 'DF_EIP_3DIS_SEX_AGE_GEO_NB': <DataflowDefinition ILO:DF_EIP_3DIS_SEX_AGE_GEO_NB(1.0): Youth discouraged job-seekers by sex, age and rural / urban areas>, 'DF_EIP_3EIP_SEX_AGE_DSB_NB': <DataflowDefinition ILO:DF_EIP_3EIP_SEX_AGE_DSB_NB(1.0): Youth outside the labour force by sex, age and disability status>, 'DF_EIP_3EIP_SEX_AGE_EDU_NB': <DataflowDefinition ILO:DF_EIP_3EIP_SEX_AGE_EDU_NB(1.0): Youth outside the labour force by sex, age and education>, 'DF_EIP_3EIP_SEX_AGE_GEO_NB': <DataflowDefinition ILO:DF_EIP_3EIP_SEX_AGE_GEO_NB(1.0): Youth outside the labour force by sex, age and rural / urban areas>, 'DF_EIP_3EIP_SEX_AGE_STU_NB': <DataflowDefinition ILO:DF_EIP_3EIP_SEX_AGE_STU_NB(1.0): Youth outside the labour force by sex, age and school attendance status>, 'DF_EIP_3WAP_SEX_AGE_DSB_RT': <DataflowDefinition ILO:DF_EIP_3WAP_SEX_AGE_DSB_RT(1.0): Youth inactivity rate by sex, age and disability status>, 'DF_EIP_3WAP_SEX_AGE_EDU_RT': <DataflowDefinition ILO:DF_EIP_3WAP_SEX_AGE_EDU_RT(1.0): Youth inactivity rate by sex, age and education>, 'DF_EIP_3WAP_SEX_AGE_GEO_RT': <DataflowDefinition ILO:DF_EIP_3WAP_SEX_AGE_GEO_RT(1.0): Youth inactivity rate by sex, age and rural / urban areas>, 'DF_EIP_3WAP_SEX_AGE_STU_RT': <DataflowDefinition ILO:DF_EIP_3WAP_SEX_AGE_STU_RT(1.0): Youth inactivity rate by sex, age and school attendance status>, 'DF_EIP_5EIP_SEX_AGE_NB': <DataflowDefinition ILO:DF_EIP_5EIP_SEX_AGE_NB(1.0): Persons outside the labour force by sex and age -- 19th ICLS>, 'DF_EIP_5PLF_SEX_AGE_NB': <DataflowDefinition ILO:DF_EIP_5PLF_SEX_AGE_NB(1.0): Potential labour force by sex and age -- 19th ICLS>, 'DF_EIP_5WAP_SEX_AGE_RT': <DataflowDefinition ILO:DF_EIP_5WAP_SEX_AGE_RT(1.0): Inactivity rate by sex and age -- 19th ICLS>, 'DF_EIP_DWAP_SEX_AGE_DSB_RT': <DataflowDefinition ILO:DF_EIP_DWAP_SEX_AGE_DSB_RT(1.0): Inactivity rate by sex, age and disability status>, 'DF_EIP_DWAP_SEX_AGE_EDU_RT': <DataflowDefinition ILO:DF_EIP_DWAP_SEX_AGE_EDU_RT(1.0): Inactivity rate by sex, age and education>, 'DF_EIP_DWAP_SEX_AGE_GEO_RT': <DataflowDefinition ILO:DF_EIP_DWAP_SEX_AGE_GEO_RT(1.0): Inactivity rate by sex, age and rural / urban areas>, 'DF_EIP_DWAP_SEX_AGE_MTS_RT': <DataflowDefinition ILO:DF_EIP_DWAP_SEX_AGE_MTS_RT(1.0): Inactivity rate by sex, age and marital status>, 'DF_EIP_DWAP_SEX_AGE_RT': <DataflowDefinition ILO:DF_EIP_DWAP_SEX_AGE_RT(1.0): Inactivity rate by sex and age>, 'DF_EIP_DWAP_SEX_EDU_DSB_RT': <DataflowDefinition ILO:DF_EIP_DWAP_SEX_EDU_DSB_RT(1.0): Inactivity rate by sex, education and disability status>, 'DF_EIP_DWAP_SEX_EDU_GEO_RT': <DataflowDefinition ILO:DF_EIP_DWAP_SEX_EDU_GEO_RT(1.0): Inactivity rate by sex, education and rural / urban areas>, 'DF_EIP_DWAP_SEX_EDU_MTS_RT': <DataflowDefinition ILO:DF_EIP_DWAP_SEX_EDU_MTS_RT(1.0): Inactivity rate by sex, education and marital status>, 'DF_EIP_DWAP_SEX_EDU_RT': <DataflowDefinition ILO:DF_EIP_DWAP_SEX_EDU_RT(1.0): Inactivity rate by sex and education>, 'DF_EIP_DWAP_SEX_GEO_MTS_RT': <DataflowDefinition ILO:DF_EIP_DWAP_SEX_GEO_MTS_RT(1.0): Inactivity rate by sex, rural / urban area and marital status>, 'DF_EIP_NEET_SEX_AGE_NB': <DataflowDefinition ILO:DF_EIP_NEET_SEX_AGE_NB(1.0): Youth not in employment, education or training (NEET) by sex and age>, 'DF_EIP_NEET_SEX_AGE_RT': <DataflowDefinition ILO:DF_EIP_NEET_SEX_AGE_RT(1.0): Share of youth not in employment, education or training (NEET) by sex and age>, 'DF_EIP_NEET_SEX_CBR_NB': <DataflowDefinition ILO:DF_EIP_NEET_SEX_CBR_NB(1.0): Youth not in employment, education or training (NEET) by sex and place of birth>, 'DF_EIP_NEET_SEX_CBR_RT': <DataflowDefinition ILO:DF_EIP_NEET_SEX_CBR_RT(1.0): Share of youth not in employment, education or training (NEET) by sex and place of birth>, 'DF_EIP_NEET_SEX_CCT_NB': <DataflowDefinition ILO:DF_EIP_NEET_SEX_CCT_NB(1.0): Youth not in employment, education or training (NEET) by sex and citizenship>, 'DF_EIP_NEET_SEX_CCT_RT': <DataflowDefinition ILO:DF_EIP_NEET_SEX_CCT_RT(1.0): Share of youth not in employment, education or training (NEET) by sex and citizenship>, 'DF_EIP_NEET_SEX_DSB_NB': <DataflowDefinition ILO:DF_EIP_NEET_SEX_DSB_NB(1.0): Youth not in employment, education or training (NEET) by sex and disability status>, 'DF_EIP_NEET_SEX_DSB_RT': <DataflowDefinition ILO:DF_EIP_NEET_SEX_DSB_RT(1.0): Share of youth not in employment, education or training (NEET) by sex and disability status>, 'DF_EIP_NEET_SEX_EDU_NB': <DataflowDefinition ILO:DF_EIP_NEET_SEX_EDU_NB(1.0): Youth not in employment, education or training (NEET) by sex and education>, 'DF_EIP_NEET_SEX_EDU_RT': <DataflowDefinition ILO:DF_EIP_NEET_SEX_EDU_RT(1.0): Share of youth not in employment, education or training (NEET) by sex and education>, 'DF_EIP_NEET_SEX_GEO_NB': <DataflowDefinition ILO:DF_EIP_NEET_SEX_GEO_NB(1.0): Youth not in employment, education or training (NEET) by sex and rural / urban areas>, 'DF_EIP_NEET_SEX_GEO_RT': <DataflowDefinition ILO:DF_EIP_NEET_SEX_GEO_RT(1.0): Share of youth not in employment, education or training (NEET) by sex and rural / urban areas>, 'DF_EIP_NEET_SEX_MTS_NB': <DataflowDefinition ILO:DF_EIP_NEET_SEX_MTS_NB(1.0): Youth not in employment, education or training (NEET) by sex and marital status>, 'DF_EIP_NEET_SEX_MTS_RT': <DataflowDefinition ILO:DF_EIP_NEET_SEX_MTS_RT(1.0): Share of youth not in employment, education or training (NEET) by sex and marital status>, 'DF_EIP_NEET_SEX_NB': <DataflowDefinition ILO:DF_EIP_NEET_SEX_NB(1.0): Youth not in employment, education or training (NEET) by sex>, 'DF_EIP_NEET_SEX_RT': <DataflowDefinition ILO:DF_EIP_NEET_SEX_RT(1.0): Share of youth not in employment, education or training (NEET) by sex>, 'DF_EIP_TEIP_SEX_AGE_DSB_NB': <DataflowDefinition ILO:DF_EIP_TEIP_SEX_AGE_DSB_NB(1.0): Persons outside the labour force by sex, age and disability status>, 'DF_EIP_TEIP_SEX_AGE_EDU_NB': <DataflowDefinition ILO:DF_EIP_TEIP_SEX_AGE_EDU_NB(1.0): Persons outside the labour force by sex, age and education>, 'DF_EIP_TEIP_SEX_AGE_GEO_NB': <DataflowDefinition ILO:DF_EIP_TEIP_SEX_AGE_GEO_NB(1.0): Persons outside the labour force by sex, age and rural / urban areas>, 'DF_EIP_TEIP_SEX_AGE_MTS_NB': <DataflowDefinition ILO:DF_EIP_TEIP_SEX_AGE_MTS_NB(1.0): Persons outside the labour force by sex, age and marital status>, 'DF_EIP_TEIP_SEX_AGE_NB': <DataflowDefinition ILO:DF_EIP_TEIP_SEX_AGE_NB(1.0): Persons outside the labour force by sex and age>, 'DF_EIP_TEIP_SEX_EDU_DSB_NB': <DataflowDefinition ILO:DF_EIP_TEIP_SEX_EDU_DSB_NB(1.0): Persons outside the labour force by sex, education and disability status>, 'DF_EIP_TEIP_SEX_EDU_GEO_NB': <DataflowDefinition ILO:DF_EIP_TEIP_SEX_EDU_GEO_NB(1.0): Persons outside the labour force by sex, education and rural / urban areas>, 'DF_EIP_TEIP_SEX_EDU_MTS_NB': <DataflowDefinition ILO:DF_EIP_TEIP_SEX_EDU_MTS_NB(1.0): Persons outside the labour force by sex, education and marital status>, 'DF_EIP_TEIP_SEX_EDU_NB': <DataflowDefinition ILO:DF_EIP_TEIP_SEX_EDU_NB(1.0): Persons outside the labour force by sex and education>, 'DF_EIP_TEIP_SEX_GEO_MTS_NB': <DataflowDefinition ILO:DF_EIP_TEIP_SEX_GEO_MTS_NB(1.0): Persons outside the labour force by sex, rural / urban area and marital status>, 'DF_EIP_TEIP_SEX_MTS_NB': <DataflowDefinition ILO:DF_EIP_TEIP_SEX_MTS_NB(1.0): Persons outside the labour force by sex and marital status>, 'DF_EIP_WDIS_SEX_AGE_GEO_NB': <DataflowDefinition ILO:DF_EIP_WDIS_SEX_AGE_GEO_NB(1.0): Discouraged job-seekers by sex, age and rural / urban areas>, 'DF_EIP_WDIS_SEX_AGE_MTS_NB': <DataflowDefinition ILO:DF_EIP_WDIS_SEX_AGE_MTS_NB(1.0): Discouraged job-seekers by sex, age and marital status>, 'DF_EIP_WDIS_SEX_AGE_NB': <DataflowDefinition ILO:DF_EIP_WDIS_SEX_AGE_NB(1.0): Discouraged job-seekers by sex and age>, 'DF_EIP_WDIS_SEX_EDU_GEO_NB': <DataflowDefinition ILO:DF_EIP_WDIS_SEX_EDU_GEO_NB(1.0): Discouraged job-seekers by sex, education and rural / urban areas>, 'DF_EIP_WDIS_SEX_EDU_MTS_NB': <DataflowDefinition ILO:DF_EIP_WDIS_SEX_EDU_MTS_NB(1.0): Discouraged job-seekers by sex, education and marital status>, 'DF_EIP_WDIS_SEX_EDU_NB': <DataflowDefinition ILO:DF_EIP_WDIS_SEX_EDU_NB(1.0): Discouraged job-seekers by sex and education>, 'DF_EIP_WDIS_SEX_GEO_MTS_NB': <DataflowDefinition ILO:DF_EIP_WDIS_SEX_GEO_MTS_NB(1.0): Discouraged job-seekers by sex, rural / urban area and marital status>, 'DF_EIP_WPLF_SEX_AGE_GEO_NB': <DataflowDefinition ILO:DF_EIP_WPLF_SEX_AGE_GEO_NB(1.0): Potential labour force by sex, age and rural / urban areas>, 'DF_EIP_WPLF_SEX_AGE_MTS_NB': <DataflowDefinition ILO:DF_EIP_WPLF_SEX_AGE_MTS_NB(1.0): Potential labour force by sex, age and marital status>, 'DF_EIP_WPLF_SEX_AGE_NB': <DataflowDefinition ILO:DF_EIP_WPLF_SEX_AGE_NB(1.0): Potential labour force by sex and age>, 'DF_EIP_WPLF_SEX_EDU_GEO_NB': <DataflowDefinition ILO:DF_EIP_WPLF_SEX_EDU_GEO_NB(1.0): Potential labour force by sex, education and rural / urban areas>, 'DF_EIP_WPLF_SEX_EDU_MTS_NB': <DataflowDefinition ILO:DF_EIP_WPLF_SEX_EDU_MTS_NB(1.0): Potential labour force by sex, education and marital status>, 'DF_EIP_WPLF_SEX_EDU_NB': <DataflowDefinition ILO:DF_EIP_WPLF_SEX_EDU_NB(1.0): Potential labour force by sex and education>, 'DF_EIP_WPLF_SEX_GEO_MTS_NB': <DataflowDefinition ILO:DF_EIP_WPLF_SEX_GEO_MTS_NB(1.0): Potential labour force by sex, rural / urban area and marital status>, 'DF_EMP_2EMP_AGE_STE_NB': <DataflowDefinition ILO:DF_EMP_2EMP_AGE_STE_NB(1.0): Employment by age and status in employment -- ILO modelled estimates, Nov. 2020>, 'DF_EMP_2EMP_SEX_AGE_CLA_NB': <DataflowDefinition ILO:DF_EMP_2EMP_SEX_AGE_CLA_NB(1.0): Employment by sex, age and economic class -- ILO modelled estimates, Nov. 2021>, 'DF_EMP_2EMP_SEX_AGE_GEO_NB': <DataflowDefinition ILO:DF_EMP_2EMP_SEX_AGE_GEO_NB(1.0): Employment by sex, age and rural / urban areas -- ILO modelled estimates, Nov. 2021>, 'DF_EMP_2EMP_SEX_AGE_NB': <DataflowDefinition ILO:DF_EMP_2EMP_SEX_AGE_NB(1.0): Employment by sex and age -- ILO modelled estimates, Nov. 2021>, 'DF_EMP_2EMP_SEX_ECO_NB': <DataflowDefinition ILO:DF_EMP_2EMP_SEX_ECO_NB(1.0): Employment by sex and economic activity -- ILO modelled estimates, Nov. 2021>, 'DF_EMP_2EMP_SEX_GEO_ECO_NB': <DataflowDefinition ILO:DF_EMP_2EMP_SEX_GEO_ECO_NB(1.0): Employment by sex, rural / urban areas and economic activity -- ILO modelled estimates, Nov. 2020>, 'DF_EMP_2EMP_SEX_GEO_OCU_NB': <DataflowDefinition ILO:DF_EMP_2EMP_SEX_GEO_OCU_NB(1.0): Employment by sex, rural / urban areas and occupation -- ILO modelled estimates, Nov. 2020>, 'DF_EMP_2EMP_SEX_GEO_STE_NB': <DataflowDefinition ILO:DF_EMP_2EMP_SEX_GEO_STE_NB(1.0): Employment by sex, rural / urban areas and status in employment -- ILO modelled estimates, Nov. 2021>, 'DF_EMP_2EMP_SEX_OCU_NB': <DataflowDefinition ILO:DF_EMP_2EMP_SEX_OCU_NB(1.0): Employment by sex and occupation -- ILO modelled estimates, Nov. 2020>, 'DF_EMP_2EMP_SEX_STE_NB': <DataflowDefinition ILO:DF_EMP_2EMP_SEX_STE_NB(1.0): Employment by sex and status in employment -- ILO modelled estimates, Nov. 2021>, 'DF_EMP_2TRU_SEX_AGE_GEO_NB': <DataflowDefinition ILO:DF_EMP_2TRU_SEX_AGE_GEO_NB(1.0): Time-related underemployment by sex, age and rural / urban areas -- ILO modelled estimates, Nov. 2020>, 'DF_EMP_2TRU_SEX_AGE_NB': <DataflowDefinition ILO:DF_EMP_2TRU_SEX_AGE_NB(1.0): Time-related underemployment by sex and age -- ILO modelled estimates, Nov. 2020>, 'DF_EMP_2TRU_SEX_AGE_RT': <DataflowDefinition ILO:DF_EMP_2TRU_SEX_AGE_RT(1.0): Time-related underemployment rate by sex and age -- ILO modelled estimates, Nov. 2020>, 'DF_EMP_2WAP_SEX_AGE_GEO_RT': <DataflowDefinition ILO:DF_EMP_2WAP_SEX_AGE_GEO_RT(1.0): Employment-to-population ratio by sex, age and rural / urban areas -- ILO modelled estimates, Nov. 2021>, 'DF_EMP_2WAP_SEX_AGE_RT': <DataflowDefinition ILO:DF_EMP_2WAP_SEX_AGE_RT(1.0): Employment-to-population ratio by sex and age -- ILO modelled estimates, Nov. 2021>, 'DF_EMP_3EMP_SEX_AGE_DSB_NB': <DataflowDefinition ILO:DF_EMP_3EMP_SEX_AGE_DSB_NB(1.0): Youth employment by sex, age and disability status>, 'DF_EMP_3EMP_SEX_AGE_ECO_NB': <DataflowDefinition ILO:DF_EMP_3EMP_SEX_AGE_ECO_NB(1.0): Youth employment by sex, age and economic activity>, 'DF_EMP_3EMP_SEX_AGE_EDU_NB': <DataflowDefinition ILO:DF_EMP_3EMP_SEX_AGE_EDU_NB(1.0): Youth employment by sex, age and education>, 'DF_EMP_3EMP_SEX_AGE_GEO_NB': <DataflowDefinition ILO:DF_EMP_3EMP_SEX_AGE_GEO_NB(1.0): Youth employment by sex, age and rural / urban areas>, 'DF_EMP_3EMP_SEX_AGE_HOW_NB': <DataflowDefinition ILO:DF_EMP_3EMP_SEX_AGE_HOW_NB(1.0): Youth employment by sex, age and weekly hours actually worked>, 'DF_EMP_3EMP_SEX_AGE_JOB_NB': <DataflowDefinition ILO:DF_EMP_3EMP_SEX_AGE_JOB_NB(1.0): Youth employment by sex, age and working time arrangement>, 'DF_EMP_3EMP_SEX_AGE_OCU_NB': <DataflowDefinition ILO:DF_EMP_3EMP_SEX_AGE_OCU_NB(1.0): Youth employment by sex, age and occupation>, 'DF_EMP_3EMP_SEX_AGE_STE_NB': <DataflowDefinition ILO:DF_EMP_3EMP_SEX_AGE_STE_NB(1.0): Youth employment by sex, age and status in employment>, 'DF_EMP_3EMP_SEX_AGE_STU_NB': <DataflowDefinition ILO:DF_EMP_3EMP_SEX_AGE_STU_NB(1.0): Youth employment by sex, age and school attendance status>, 'DF_EMP_3WAP_SEX_AGE_DSB_RT': <DataflowDefinition ILO:DF_EMP_3WAP_SEX_AGE_DSB_RT(1.0): Youth employment-to-population ratio by sex, age and disability status>, 'DF_EMP_3WAP_SEX_AGE_EDU_RT': <DataflowDefinition ILO:DF_EMP_3WAP_SEX_AGE_EDU_RT(1.0): Youth employment-to-population ratio by sex, age and education>, 'DF_EMP_3WAP_SEX_AGE_GEO_RT': <DataflowDefinition ILO:DF_EMP_3WAP_SEX_AGE_GEO_RT(1.0): Youth employment-to-population ratio by sex, age and rural / urban areas>, 'DF_EMP_3WAP_SEX_AGE_STU_RT': <DataflowDefinition ILO:DF_EMP_3WAP_SEX_AGE_STU_RT(1.0): Youth employment-to-population ratio by sex, age and school attendance status>, 'DF_EMP_5EMP_SEX_AGE_NB': <DataflowDefinition ILO:DF_EMP_5EMP_SEX_AGE_NB(1.0): Employment by sex and age -- 19th ICLS>, 'DF_EMP_5NIF_SEX_AGE_NB': <DataflowDefinition ILO:DF_EMP_5NIF_SEX_AGE_NB(1.0): Informal employment by sex and age -- 19th ICLS>, 'DF_EMP_5NIF_SEX_AGE_RT': <DataflowDefinition ILO:DF_EMP_5NIF_SEX_AGE_RT(1.0): Employment outside the formal sector by sex and age -- 19th ICLS>, 'DF_EMP_5PIF_SEX_AGE_NB': <DataflowDefinition ILO:DF_EMP_5PIF_SEX_AGE_NB(1.0): Informal employment rate by sex and age -- 19th ICLS>, 'DF_EMP_5PIF_SEX_AGE_RT': <DataflowDefinition ILO:DF_EMP_5PIF_SEX_AGE_RT(1.0): Share of employment outside the formal sector by sex and age -- 19th ICLS>, 'DF_EMP_5WAP_SEX_AGE_RT': <DataflowDefinition ILO:DF_EMP_5WAP_SEX_AGE_RT(1.0): Employment-to-population ratio by sex and age -- 19th ICLS>, 'DF_EMP_DWA1_SEX_AGE_RT': <DataflowDefinition ILO:DF_EMP_DWA1_SEX_AGE_RT(1.0): Employment-to-population ratio by sex and age, seasonally adjusted series (Sub-annual)>, 'DF_EMP_DWAP_SEX_AGE_DSB_RT': <DataflowDefinition ILO:DF_EMP_DWAP_SEX_AGE_DSB_RT(1.0): Employment-to-population ratio by sex, age and disability status>, 'DF_EMP_DWAP_SEX_AGE_EDU_RT': <DataflowDefinition ILO:DF_EMP_DWAP_SEX_AGE_EDU_RT(1.0): Employment-to-population ratio by sex, age and education>, 'DF_EMP_DWAP_SEX_AGE_GEO_RT': <DataflowDefinition ILO:DF_EMP_DWAP_SEX_AGE_GEO_RT(1.0): Employment-to-population ratio by sex, age and rural / urban areas>, 'DF_EMP_DWAP_SEX_AGE_MTS_RT': <DataflowDefinition ILO:DF_EMP_DWAP_SEX_AGE_MTS_RT(1.0): Employment-to-population ratio by sex, age and marital status>, 'DF_EMP_DWAP_SEX_AGE_RT': <DataflowDefinition ILO:DF_EMP_DWAP_SEX_AGE_RT(1.0): Employment-to-population ratio by sex and age>, 'DF_EMP_DWAP_SEX_EDU_DSB_RT': <DataflowDefinition ILO:DF_EMP_DWAP_SEX_EDU_DSB_RT(1.0): Employment-to-population ratio by sex, education and disability status>, 'DF_EMP_DWAP_SEX_EDU_GEO_RT': <DataflowDefinition ILO:DF_EMP_DWAP_SEX_EDU_GEO_RT(1.0): Employment-to-population ratio by sex, education and rural / urban areas>, 'DF_EMP_DWAP_SEX_EDU_MTS_RT': <DataflowDefinition ILO:DF_EMP_DWAP_SEX_EDU_MTS_RT(1.0): Employment-to-population ratio by sex, education and marital status>, 'DF_EMP_DWAP_SEX_EDU_RT': <DataflowDefinition ILO:DF_EMP_DWAP_SEX_EDU_RT(1.0): Employment-to-population ratio by sex and education>, 'DF_EMP_DWAP_SEX_GEO_MTS_RT': <DataflowDefinition ILO:DF_EMP_DWAP_SEX_GEO_MTS_RT(1.0): Employment-to-population ratio by sex, rural / urban area and marital status>, 'DF_EMP_DWAP_SEX_MTS_RT': <DataflowDefinition ILO:DF_EMP_DWAP_SEX_MTS_RT(1.0): Employment-to-population ratio by sex and marital status>, 'DF_EMP_NIFL_SEX_AGE_NB': <DataflowDefinition ILO:DF_EMP_NIFL_SEX_AGE_NB(1.0): Informal employment by sex and age>, 'DF_EMP_NIFL_SEX_AGE_RT': <DataflowDefinition ILO:DF_EMP_NIFL_SEX_AGE_RT(1.0): Informal employment rate by sex and age>, 'DF_EMP_NIFL_SEX_DSB_NB': <DataflowDefinition ILO:DF_EMP_NIFL_SEX_DSB_NB(1.0): Informal employment by sex and disability status>, 'DF_EMP_NIFL_SEX_DSB_RT': <DataflowDefinition ILO:DF_EMP_NIFL_SEX_DSB_RT(1.0): Informal employment rate by sex and disability status>, 'DF_EMP_NIFL_SEX_ECO_NB': <DataflowDefinition ILO:DF_EMP_NIFL_SEX_ECO_NB(1.0): Informal employment by sex and economic activity>, 'DF_EMP_NIFL_SEX_ECO_RT': <DataflowDefinition ILO:DF_EMP_NIFL_SEX_ECO_RT(1.0): Informal employment rate by sex and economic activity>}\n\n--- <class 'pandasdmx.model.Annotation'> ---\n{8861: Annotation(id=None, title='9/17/2022 7:10:07 AM', type='LAST_UPDATE', url=None, text=), 8865: Annotation(id=None, title='EDU', type='LAYOUT_COLUMN', url=None, text=), 8869: Annotation(id=None, title='REF_AREA,SEX,TIME_PERIOD', type='LAYOUT_ROW', url=None, text=), 8873: Annotation(id=None, title='FREQ=A,endPeriod=2021-12-31,EDU=EDU_AGGREGATE_TOTAL+EDU_AGGREGATE_LTB+EDU_AGGREGATE_BAS+EDU_AGGREGATE_INT+EDU_AGGREGATE_ADV+EDU_AGGREGATE_X,SEX=SEX_T,LASTNOBSERVATIONS=1', type='DEFAULT', url=None, text=), 8879: Annotation(id=None, title=None, type='EXT_RESOURCE', url=None, text=en: Full CSV (ILO Bulk)|https://www.ilo.org/ilostat-files/WEB_bulk_download/indicator/EMP_NIFL_SEX_EDU_NB_A.csv.gz|https://www.ilo.org/ilostat-files/ISCO/images/file-archive-icon.svg\nes: Full CSV (ILO Bulk)|https://www.ilo.org/ilostat-files/WEB_bulk_download/indicator/EMP_NIFL_SEX_EDU_NB_A.csv.gz|https://www.ilo.org/ilostat-files/ISCO/images/file-archive-icon.svg\nfr: Full CSV (ILO Bulk)|https://www.ilo.org/ilostat-files/WEB_bulk_download/indicator/EMP_NIFL_SEX_EDU_NB_A.csv.gz|https://www.ilo.org/ilostat-files/ISCO/images/file-archive-icon.svg), 8883: Annotation(id=None, title='88888', type='ORDER', url=None, text=), 8887: Annotation(id=None, title='11112', type='SEARCH_WEIGHT', url=None, text=)}\n\n--- Name ---\n{8888: ('en', 'Informal employment by sex and education'), 8889: ('es', 'Ocupación informal según sexo y educación'), 8890: ('fr', 'Emploi informel par sexe et éducation')}\n\n--- Description ---\n{8891: ('en', '<strong>With the aim of promoting international comparability, statistics presented on ILOSTAT are based on standard international definitions wherever feasible and may differ from official national figures. </strong>This series is based on the 13th ICLS definitions and includes countries for which data are also available based on 19th ICLS definitions in the Work Statistics -- 19th ICLS (WORK) database. This harmonized series for informal employment is derived using the same set of criteria across countries to improve comparability. The criteria used are based on employment status, institutional sector, destination of production, bookkeeping, registration, social security contribution, places of work and size. Data disaggregated by level of education are provided on the highest level of education completed, classified according to the International Standard Classification of Education (ISCED). Data may have been regrouped from national classifications, which may not be strictly compatible with ISCED. For more information, refer to the <a href = \"https://ilostat.ilo.org/resources/concepts-and-definitions/description-education-and-mismatch-indicators/\">Education and Mismatch Indicators (EMI) database description</a>.')}\n\n<common:Description xmlns:common=\"http://www.sdmx.org/resources/sdmxml/schemas/v2_1/common\" xmlns:message=\"http://www.sdmx.org/resources/sdmxml/schemas/v2_1/message\" xmlns:structure=\"http://www.sdmx.org/resources/sdmxml/schemas/v2_1/structure\" xml:lang=\"es\"/>\n\n\n\nABS_XML  ABORIGINAL_POP_PROJ                 Projected population, Aboriginal and Torres St...\n         ABORIGINAL_POP_PROJ_REMOTE          Projected population, Aboriginal and Torres St...\n         ABS_ABORIGINAL_POPPROJ_INDREGION    Projected population, Aboriginal and Torres St...\n         ABS_ACLD_LFSTATUS                   Australian Census Longitudinal Dataset (ACLD):...\n         ABS_ACLD_TENURE                     Australian Census Longitudinal Dataset (ACLD):...\n                                                                   ...                        \nUNSD     DF_UNData_UNFCC                                                       SDMX_GHG_UNDATA\nWB       DF_WITS_Tariff_TRAINS                                WITS - UNCTAD TRAINS Tariff Data\n         DF_WITS_TradeStats_Development                             WITS TradeStats Devlopment\n         DF_WITS_TradeStats_Tariff                                      WITS TradeStats Tariff\n         DF_WITS_TradeStats_Trade                                        WITS TradeStats Trade\nName: dataflow, Length: 8742, dtype: object\n\n\nFor example, the dataflows from the World Bank are:\n\n\nCode\ndflows['WB']\n\n\nDF_WITS_Tariff_TRAINS             WITS - UNCTAD TRAINS Tariff Data\nDF_WITS_TradeStats_Development          WITS TradeStats Devlopment\nDF_WITS_TradeStats_Tariff                   WITS TradeStats Tariff\nDF_WITS_TradeStats_Trade                     WITS TradeStats Trade\ndtype: object"
  },
  {
    "objectID": "datasets.html",
    "href": "datasets.html",
    "title": "Datasets for economic research",
    "section": "",
    "text": "The functions that load datasets output the variables necessary to run and test economic models - machine learning or not."
  },
  {
    "objectID": "datasets.html#real-datasets",
    "href": "datasets.html#real-datasets",
    "title": "Datasets for economic research",
    "section": "Real datasets",
    "text": "Real datasets\n\nsource\n\nload_BarroLee_1994\n\n load_BarroLee_1994 ()\n\nDataset used in R Barro and J-W Lee’s Sources of Economic Growth (1994)\nRobert Barro and Jong-Wha Lee published in 1994 their cross-country study on sources of economic (GDP) growth. Their dataset has been used over time by other economists, such as by Domenico Giannone, Michele Lenza and Giorgio Primicieri (2021). This function uses the version available in their online annex. In that paper, this dataset corresponds to what the authors call “macro2”.\nThe original data, along with more information on the variables, can be found in this NBER website. A very helpful codebook is found in this repo."
  },
  {
    "objectID": "datasets.html#simulated-datasets",
    "href": "datasets.html#simulated-datasets",
    "title": "Datasets for economic research",
    "section": "Simulated datasets",
    "text": "Simulated datasets\n\n\n\n\n\n\nNote\n\n\n\nAll of the functions creating simulated datasets have a parameter random_state that allow for reproducible random numbers.\n\n\n\nsource\n\nmake_causal_effect\n\n make_causal_effect (n_samples:int=100, n_features:int=100,\n                     pretreatment_outcome=<function <lambda>>,\n                     treatment_propensity=<function <lambda>>,\n                     treatment_assignment=<function <lambda>>,\n                     treatment=<function <lambda>>,\n                     treatment_effect=<function <lambda>>, bias:float=0,\n                     noise=0, random_state=None, return_propensity=False,\n                     return_assignment=False,\n                     return_treatment_value=False,\n                     return_treatment_effect=True,\n                     return_pretreatment_y=False, return_as_dict=False)\n\nSimulated dataset with causal effects from treatment\nmake_causal_effect creates a dataset for when the question of interest is related to the causal effects of a treatment. For example, for a simulated dataset, we can check that \\(y\\) corresponds to the sum of the treatment effects plus the component that does not depend on the treatment:\n\ncausal_sim = make_causal_effect(\n    n_samples=2000,\n    n_features=100,\n    return_propensity=True,\n    return_treatment_effect=True, \n    return_pretreatment_y=True, \n    return_as_dict=True)\n\n assert not np.any(np.round(causal_sim['y'] - causal_sim['pretreatment_y'] - causal_sim['treatment_effect'], decimals=13))\n\n\nPre-treatment outcome\nThe pre-treatment outcome \\(Y_i|X_i\\) (the part of the outcome variable that is not dependent on the treatment) might be defined by the user. This corresponds to the value of the outcome for any untreated observations. The function should always take at least two arguments: X and bias, even if one of them is unused; bias is the constant. The argument is zero by default but can be set by the user to be another value.\n\ncausal_sim = make_causal_effect(\n    bias=0.123,\n    pretreatment_outcome=lambda X, bias: bias,\n    return_assignment=True,\n    return_as_dict=True\n)\n\nassert all(causal_sim['y'][causal_sim['treatment_assignment'] == 0] == 0.123)\n\nIf the outcome depends on specific columns of \\(X\\), this can be implemented as shown below.\n\ncausal_sim = make_causal_effect(\n    pretreatment_outcome=lambda X, bias: X[:, 1] + np.maximum(X[:,2], 0) + X[:,3] * X[:,4] + bias\n)\n\nAnd of course, the outcome might also have a random component.\nIn these cases (and in other parts of this function), when the user wants to use the same random number generator as the other parts of the function, the function must have an argment rng for the NumPy random number generator used in other parts of the function.\n\ncausal_sim_1 = make_causal_effect(\n    pretreatment_outcome=lambda X, bias, rng: X[:, 1] + np.maximum(X[:,2], 0) + X[:,3] * X[:,4] + bias + rng.standard_normal(size=X.shape[0]),\n    random_state=42,\n    return_pretreatment_y=True,\n    return_as_dict=True\n)\n\ncausal_sim_2 = make_causal_effect(\n    pretreatment_outcome=lambda X, bias, rng: X[:, 1] + np.maximum(X[:,2], 0) + X[:,3] * X[:,4] + bias + rng.standard_normal(size=X.shape[0]),\n    random_state=42,\n    return_pretreatment_y=True,\n    return_as_dict=True\n)\n\nassert all(causal_sim_1['X'].reshape(-1, 1) == causal_sim_2['X'].reshape(-1, 1))\nassert all(causal_sim_1['y'] == causal_sim_2['y'])\nassert all(causal_sim_1['pretreatment_y'] == causal_sim_2['pretreatment_y'])\n\n\n\nTreatment propensity\nThe treatment propensity of observations may all be the same, in which case treatment_propensity is a floating number between 0 and 1.\n\nsame_propensity_sim = make_causal_effect(\n    n_samples=485,\n    treatment_propensity=0.3,\n    return_propensity=True,\n    return_as_dict=True\n)\n\nassert np.unique(same_propensity_sim['propensity']) == 0.3\nassert len(same_propensity_sim['propensity']) == 485\n\nOr it might depend on the observation’s covariates, with the user passing a function with an argument ‘X’.\n\nheterogenous_propensities_sim = make_causal_effect(\n    n_samples=1000,\n    treatment_propensity=lambda X: 0.3 + (X[:, 0] > 0) * 0.2,\n    return_propensity=True,\n    return_as_dict=True\n)\n\nplt.title(\"Heterogenously distributed propensities\")\nplt.xlabel(\"Propensity\")\nplt.ylabel(\"No of observations\")\nplt.hist(heterogenous_propensities_sim['propensity'], bins=100)\nplt.show()\n\n\n\n\nThe propensity can also be randomly allocated, together with covariate dependence or not. Note that even if the propensity is completely random and does not depend on covariates, the function must still use the argument X to calculate a random vector with the appropriate size.\n\nrandom_propensities_sim = make_causal_effect(\n    n_samples=50000,\n    treatment_propensity=lambda X: np.random.uniform(size=X.shape[0]),\n    return_propensity=True,\n    return_as_dict=True\n)\n\nplt.title(\"Randomly distributed propensities\")\nplt.xlabel(\"Propensity\")\nplt.ylabel(\"No of observations\")\nplt.hist(random_propensities_sim['propensity'], bins=100)\nplt.show()\n\n\n\n\n\n\nTreatment assignment\nAs seen above, every observation has a given treatment propensity - the chance that they are treated. Users can define how this propensity translates into actual treatment with the argument treatment_assignment. This argument takes a function, which must have an argument called propensity.\nThe default value for this argument is a function returning 1s with probability propensity and 0s otherwise. Any other function should always return either 0s or 1s for the data simulator to work as expected.\n\ncausal_sim = make_causal_effect(\n    treatment_assignment=lambda propensity: np.random.binomial(1, propensity)\n)\n\nWhile the case above is likely to be the most useful in practice, this argument accepts more complex relationships between an observation’s propensity and the actual treatment assignment.\nFor example, if treatment is subject to rationing, then one could simulate data with 10 observations where only the samples with the highest (say, 3) propensity scores get treated, as below:\n\nrationed_treatment_sim = make_causal_effect(\n    n_samples=10,\n    treatment_propensity=lambda X: np.random.uniform(size=X.shape[0]),\n    treatment_assignment=lambda propensity: propensity >= propensity[np.argsort(propensity)][-3],\n    return_propensity=True,\n    return_assignment=True,\n    return_as_dict=True\n)\n\n\nrationed_treatment = pd.DataFrame(\n    np.column_stack((rationed_treatment_sim['propensity'], rationed_treatment_sim['treatment_assignment'])),\n    columns = ['propensity', 'assignment']\n    )\n\n\nrationed_treatment.sort_values('propensity')\n\n\n\n\n\n  \n    \n      \n      propensity\n      assignment\n    \n  \n  \n    \n      6\n      0.038075\n      0.0\n    \n    \n      7\n      0.106176\n      0.0\n    \n    \n      2\n      0.191808\n      0.0\n    \n    \n      3\n      0.197068\n      0.0\n    \n    \n      8\n      0.531224\n      0.0\n    \n    \n      9\n      0.619038\n      0.0\n    \n    \n      4\n      0.710921\n      0.0\n    \n    \n      1\n      0.788564\n      1.0\n    \n    \n      0\n      0.889001\n      1.0\n    \n    \n      5\n      0.890778\n      1.0\n    \n  \n\n\n\n\n\n\nTreatment value\nThe treatment argument indicates the magnitude of the treatment for each observation assigned for treatment. Its value is always a function that must have an argument called assignment, as in the first example below.\nIn the simplest case, the treatment is a binary variable indicating whether or not a variable was treated. In other words, the treatment is the same as the assignment, as in the default value.\nBut users can also simulate data with heterogenous treatment, conditional on assignment. This is done by including a pararemeter X in the function, as shown in the second example below.\n\nbinary_treatment_sim = make_causal_effect(\n    n_samples=15,\n    treatment=lambda assignment: assignment,\n    return_assignment=True,\n    return_treatment_value=True,\n    return_as_dict=True\n)\n\nassert sum(binary_treatment_sim['treatment_assignment'] - binary_treatment_sim['treatment_value'][0]) == 0\n\nHeterogenous treatments may occur in settings where treatment intensity, conditional on assignment, varies across observations. Please note the following: * the heterogenous treatment amount may or may not depend on covariates, but either way, if treatment values are heterogenous, then X needs to be an argument of the function passed to treatment, if nothing else to make sure the shapes match; and * if treatments are heterogenous, then it is important to multiply the treatment value with the assignment argument to ensure that observations that are not assigned to be treated are indeed not treated (the function will return an AssertionError otherwise).\n\nhetereogenous_treatment_sim = make_causal_effect(\n    n_samples=15,\n    treatment=lambda assignment, X: assignment * np.random.uniform(size=X.shape[0]),\n    return_assignment=True,\n    return_treatment_value=True,\n    return_as_dict=True\n)\n\nIn contrast to the function above, in the chunk below we function make_causal_effect fails because a treatment value is also assigned to observations that were not assigned for treatment.\n\ntest_fail(\n    make_causal_effect, \n    kwargs=dict(treatment=lambda assignment, X: assignment + np.random.uniform(size=X.shape[0]))\n)\n\n\n\nTreatment effect\nThe treatment effect can be homogenous, ie, is doesn’t depend on any other characteristic of the individual observations (in other words, does not depend on \\(X_i\\)), or heterogenous (where the treatment effect on \\(Y_i\\) does depend on each observation’s \\(X_i\\)). This can be done by specifying the causal relationship through a lambda function, as below:\n\nhomogenous_effects_sim = make_causal_effect(\n        treatment_effect=lambda treatment_value: treatment_value,\n        return_treatment_value=True,\n        return_as_dict=True\n)\n\nassert (homogenous_effects_sim['treatment_effect'] == homogenous_effects_sim['treatment_value']).all()\n\nheterogenous_effects_sim = make_causal_effect(\n        treatment_effect=lambda treatment_value, X: np.maximum(X[:, 1], 0) * treatment_value,\n        return_treatment_value=True,\n        return_as_dict=True\n)\n\nassert (heterogenous_effects_sim['treatment_effect'] != heterogenous_effects_sim['treatment_value']).any()"
  },
  {
    "objectID": "datasets.html#references",
    "href": "datasets.html#references",
    "title": "Datasets for economic research",
    "section": "References",
    "text": "References\nBarro, R and J-W Lee (1994): “Sources of economic growth”, Carnegie-Rochester Conference Series on Public Policy, 40, p. 1-46.\nGiannone, D, M Lenza, G Primiceri (2021): “Economic predictions with big data: the illusion of sparsity”, Econometrica, 89, 5, p. 2409-2437."
  },
  {
    "objectID": "barrolee1994.html",
    "href": "barrolee1994.html",
    "title": "Using gingado to understand economic growth",
    "section": "",
    "text": "This notebook showcases one possible use of gingado by estimating economic growth across countries, using the dataset studied by Barro and Lee (1994). You can run this notebook interactively, by clicking on the appropriate link above.\nThis dataset has been widely studied in economics. Belloni et al (2011) and Giannone et al (2021) are two studies of this dataset that are most related to machine learning.\nThis notebook will use gingado to compare quickly setup a well-performing machine learning model and use its results as evidence to support the conditional convergence hypothesis; compare different classes of models (and their combination in a single model), and use and document the best performing alternative.\nBecause the notebook is for pedagogical purposes only, please bear in mind some aspects of the machine learning workflow (such as carefully thinking about the cross-validation strategy) are glossed over in this notebook. Also, only the key academic references are cited; more references can be found in the papers mentioned in this example.\nFor a more thorough description of gingado, please refer to the package’s website and to the academic material about it."
  },
  {
    "objectID": "barrolee1994.html#setting-the-stage",
    "href": "barrolee1994.html#setting-the-stage",
    "title": "Using gingado to understand economic growth",
    "section": "Setting the stage",
    "text": "Setting the stage\nWe will import packages as the work progresses. This will help highlight the specific steps in the workflow that gingado can be helpful with.\n\nimport pandas as pd\n\nThe data is available in the online annex to Giannone et al (2021). In that paper, this dataset corresponds to what the authors call “macro2”. The original data, along with more information on the variables, can be found in this NBER website. A very helpful codebook is found in this repo.\n\nfrom gingado.datasets import load_BarroLee_1994\n\nX, y = load_BarroLee_1994()\n\nThe dataset contains explanatory variables representing per-capita growth between 1960 and 1985, for 90 countries.\n\nX.columns\n\nIndex(['gdpsh465', 'bmp1l', 'freeop', 'freetar', 'h65', 'hm65', 'hf65', 'p65',\n       'pm65', 'pf65', 's65', 'sm65', 'sf65', 'fert65', 'mort65', 'lifee065',\n       'gpop1', 'fert1', 'mort1', 'invsh41', 'geetot1', 'geerec1', 'gde1',\n       'govwb1', 'govsh41', 'gvxdxe41', 'high65', 'highm65', 'highf65',\n       'highc65', 'highcm65', 'highcf65', 'human65', 'humanm65', 'humanf65',\n       'hyr65', 'hyrm65', 'hyrf65', 'no65', 'nom65', 'nof65', 'pinstab1',\n       'pop65', 'worker65', 'pop1565', 'pop6565', 'sec65', 'secm65', 'secf65',\n       'secc65', 'seccm65', 'seccf65', 'syr65', 'syrm65', 'syrf65', 'teapri65',\n       'teasec65', 'ex1', 'im1', 'xr65', 'tot1'],\n      dtype='object')\n\n\n\nX.head().T\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n    \n  \n  \n    \n      gdpsh465\n      6.591674\n      6.829794\n      8.895082\n      7.565275\n      7.162397\n    \n    \n      bmp1l\n      0.283700\n      0.614100\n      0.000000\n      0.199700\n      0.174000\n    \n    \n      freeop\n      0.153491\n      0.313509\n      0.204244\n      0.248714\n      0.299252\n    \n    \n      freetar\n      0.043888\n      0.061827\n      0.009186\n      0.036270\n      0.037367\n    \n    \n      h65\n      0.007000\n      0.019000\n      0.260000\n      0.061000\n      0.017000\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      teasec65\n      17.300000\n      18.000000\n      20.700000\n      22.700000\n      17.600000\n    \n    \n      ex1\n      0.072900\n      0.094000\n      0.174100\n      0.126500\n      0.121100\n    \n    \n      im1\n      0.066700\n      0.143800\n      0.175000\n      0.149600\n      0.130800\n    \n    \n      xr65\n      0.348000\n      0.525000\n      1.082000\n      6.625000\n      2.500000\n    \n    \n      tot1\n      -0.014727\n      0.005750\n      -0.010040\n      -0.002195\n      0.003283\n    \n  \n\n61 rows × 5 columns\n\n\n\nThe outcome variable is represented here:\n\ny.plot.hist(bins=90, title='GDP growth')\n\n<AxesSubplot:title={'center':'GDP growth'}, ylabel='Frequency'>"
  },
  {
    "objectID": "barrolee1994.html#testing-the-conditional-converge-hypothesis",
    "href": "barrolee1994.html#testing-the-conditional-converge-hypothesis",
    "title": "Using gingado to understand economic growth",
    "section": "Testing the conditional converge hypothesis",
    "text": "Testing the conditional converge hypothesis\nNow we can leverage our automatic benchmark model to test the conditional converge hypothesis - ie, the preposition that countries with lower starting GDP tend to grow faster than other comparable countries. In other words, this hypothesis predicts that when GDP growth is regressed on the level of past GDP and on an adequate set of covariates \\(X\\), the coefficient on past GDP levels are negative.\nSince we have the results for the importance of each regressor in separating countries by their growth result, we can compare the estimated coefficient for GDP levels in regressions that include different regressors in the vector \\(X\\). To maintain this example a simple exercise, the following three models are estimated:\n\n\\(X\\) contains the five most important regressors, as estimated by the benchmark model (see the graph above)\n\\(X\\) contains the five least important regressors, from the same estimation as above\n\\(X\\) is the empty set - in other words, this is a simple equation on GDP growth on GDP levels\n\nA result that would be consistent with the conditionality of the conditional convergence hypothesis is the first equation resulting in a negative coefficient for starting GDP, while the following two equations may not necessarily be successful in identifying a negative coefficient. This is because the least important regressors are not likely to have sufficient predictive power to separate countries into comparable groups.\nThe five more and less important regressors are:\n\ntop_five = regressor_importance.sort_values(by=\"Importance\", ascending=False).head(5)\nbottom_five = regressor_importance.sort_values(by=\"Importance\", ascending=True).head(5)\n\ntop_five, bottom_five\n\n(          Importance\n bmp1l       0.176576\n pop6565     0.095107\n teasec65    0.054589\n xr65        0.041992\n teapri65    0.036713,\n          Importance\n no65       0.001799\n pm65       0.001983\n human65    0.002359\n hm65       0.002727\n p65        0.002760)\n\n\n\nimport statsmodels.api as sm\n\ngdp_level = 'gdpsh465'\n\n\n#collapse_output\nX_topfive = X[[gdp_level] + list(top_five.index)]\nX_topfive = sm.add_constant(X_topfive)\nX_topfive.head()\n\n\n\n\n\n  \n    \n      \n      const\n      gdpsh465\n      bmp1l\n      pop6565\n      teasec65\n      xr65\n      teapri65\n    \n  \n  \n    \n      0\n      1.0\n      6.591674\n      0.2837\n      0.027591\n      17.3\n      0.348\n      47.6\n    \n    \n      1\n      1.0\n      6.829794\n      0.6141\n      0.035637\n      18.0\n      0.525\n      57.1\n    \n    \n      2\n      1.0\n      8.895082\n      0.0000\n      0.076685\n      20.7\n      1.082\n      26.5\n    \n    \n      3\n      1.0\n      7.565275\n      0.1997\n      0.031039\n      22.7\n      6.625\n      27.8\n    \n    \n      4\n      1.0\n      7.162397\n      0.1740\n      0.026281\n      17.6\n      2.500\n      34.5\n    \n  \n\n\n\n\n\n#collapse_output\nX_bottomfive = X[[gdp_level] + list(bottom_five.index)]\nX_bottomfive = sm.add_constant(X_bottomfive)\nX_bottomfive.head()\n\n\n\n\n\n  \n    \n      \n      const\n      gdpsh465\n      no65\n      pm65\n      human65\n      hm65\n      p65\n    \n  \n  \n    \n      0\n      1.0\n      6.591674\n      89.46\n      0.37\n      0.301\n      0.013\n      0.29\n    \n    \n      1\n      1.0\n      6.829794\n      89.10\n      1.00\n      0.706\n      0.032\n      0.91\n    \n    \n      2\n      1.0\n      8.895082\n      1.40\n      1.00\n      8.317\n      0.325\n      1.00\n    \n    \n      3\n      1.0\n      7.565275\n      20.60\n      1.00\n      3.833\n      0.070\n      1.00\n    \n    \n      4\n      1.0\n      7.162397\n      58.73\n      0.85\n      1.900\n      0.027\n      0.82\n    \n  \n\n\n\n\n\n#collapse_output\nX_onlyGDPlevel = sm.add_constant(X[gdp_level])\nX_onlyGDPlevel.head()\n\n\n\n\n\n  \n    \n      \n      const\n      gdpsh465\n    \n  \n  \n    \n      0\n      1.0\n      6.591674\n    \n    \n      1\n      1.0\n      6.829794\n    \n    \n      2\n      1.0\n      8.895082\n    \n    \n      3\n      1.0\n      7.565275\n    \n    \n      4\n      1.0\n      7.162397\n    \n  \n\n\n\n\n\nmodels = dict(\n    topfive = sm.OLS(y, X_topfive).fit(),\n    bottomfive = sm.OLS(y, X_bottomfive).fit(),\n    onlyGDPlevel = sm.OLS(y, X_onlyGDPlevel).fit()\n)\n\n\ncoefs = pd.DataFrame({name: model.conf_int().loc[gdp_level] for name, model in models.items()})\ncoefs.loc[0.5] = [model.params[gdp_level] for _, model in models.items()]\ncoefs = coefs.sort_index().reset_index(drop=True)\ncoefs.index = ['[0.025', 'coef on GDP levels', '0.975]']\ncoefs\n\n\n\n\n\n  \n    \n      \n      topfive\n      bottomfive\n      onlyGDPlevel\n    \n  \n  \n    \n      [0.025\n      -0.031995\n      -0.036474\n      -0.010810\n    \n    \n      coef on GDP levels\n      -0.014089\n      -0.012508\n      0.001317\n    \n    \n      0.975]\n      0.003816\n      0.011458\n      0.013444\n    \n  \n\n\n\n\nThe equation using the top five regressors in explanatory power yielded a coefficient that is statistically speaking negative under the usual confidence interval levels. In contrast, the regression using the bottom five regressors failed to maintain that level of statistical significance (although the coefficient point estimate was still negative). And finally the regression on GDP level solely resulted, as in the past literature, on a point estimate that is also statistically not different than zero.\nThese results above offer a different way to add evidence to the conditional convergence hypothesis. In particular, with the help of gingado’s RegressionBenchmark model, it is possible to identify which covariates can meaningfully serve as covariates in a growth equation from those that cannot. This is important because if the covariate selection for some reason included only variables with little explanatory power instead of the most relevant ones, an economist might erroneously reach a different conclusion."
  },
  {
    "objectID": "barrolee1994.html#model-documentation",
    "href": "barrolee1994.html#model-documentation",
    "title": "Using gingado to understand economic growth",
    "section": "Model documentation",
    "text": "Model documentation\nImportantly for model documentation, the benchmark already has some baseline documentation set up. If the user wishes, they can use that as a basis to document their model. Note that the output is in a raw format that is suitable for machine reading and writing. Intermediary and advanced users may wish to use that format to construct personalised forms, documents, etc.\n\n#collapse_output\nbenchmark.model_documentation.show_json()\n\nSince there is some information in the model documentation that was automatically added, we might want to concentrate on the fields in the model card that are yet to be answered. Actually, this is the purpose of gingado’s automatic documentation: to afford users more time so they can invest, if they want, on model documentation.\n\n#collapse_output\nbenchmark.model_documentation.open_questions()\n\nLet’s fill some information:\n\nbenchmark.model_documentation.fill_info({\n    'intended_use': {\n        'primary_uses': 'This model is trained for pedagogical uses only.',\n        'primary_users': 'Everyone is welcome to follow the description showing the development of this benchmark.'\n    }\n})\n\nNote the format, based on a Python dictionary. In particular, the open_questions method results include keys divided by double underscores. As seen above, these should be interpreted as different levels of the documentation template, leading to a nested dictionary.\nNow when we confirm that the questions answered above are no longer “open questions”:\n\n#collapse_output\nbenchmark.model_documentation.open_questions()\n\nIf we want, at any time we can save the documentation to a local JSON file, as well as read another document."
  },
  {
    "objectID": "barrolee1994.html#trying-out-model-alternatives",
    "href": "barrolee1994.html#trying-out-model-alternatives",
    "title": "Using gingado to understand economic growth",
    "section": "Trying out model alternatives",
    "text": "Trying out model alternatives\nThe benchmark model may be enough for some analyses, or maybe the user is interested in using the benchmark to explore the data and have an understanding of the importance of each regressor, to concentrate their work on data that can be meaningful for their purposes. But oftentimes a user will want to seek a machine learning model that performs as well as possible.\nFor users that want to manually create other models, gingado allows the possibility of comparing them with the benchmark. If the user model is better, it becomes the new benchmark!\nFor the following analyses, we will use K-fold as cross-validation, with 5 splits of the sample.\n\nFirst candidate: a gradient boosting tree\n\n#collapse_output\nimport numpy as np\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nparam_grid = {\n    'learning_rate': [0.01, 0.1, 0.25],\n    'max_depth': [3, 6, 9]\n}\n\nreg_gradbooster = GradientBoostingRegressor()\n\ngradboosterg_grid = GridSearchCV(\n    reg_gradbooster,\n    param_grid,\n    n_jobs=-1,\n    verbose=3\n).fit(X, y)\n\n\ny_pred = gradboosterg_grid.predict(X)\npd.DataFrame({\n    'y': y,\n    'y_pred': y_pred\n    }).plot.scatter(x='y', y='y_pred', grid=True)\n\n\npd.DataFrame(y - y_pred).plot.hist(bins=30)\n\n\n\nSecond candidate: lasso\n\n#collapse_output\nfrom sklearn.linear_model import Lasso\n\nparam_grid = {\n    'alpha': [0.5, 1, 1.25],\n}\n\nreg_lasso = Lasso(fit_intercept=True)\n\nlasso_grid = GridSearchCV(\n    reg_lasso,\n    param_grid,\n    n_jobs=-1,\n    verbose=3\n).fit(X, y)\n\n\ny_pred = lasso_grid.predict(X)\npd.DataFrame({\n    'y': y,\n    'y_pred': y_pred\n    }).plot.scatter(x='y', y='y_pred', grid=True)\n\n\npd.DataFrame(y - y_pred).plot.hist(bins=30)"
  },
  {
    "objectID": "barrolee1994.html#comparing-the-models-with-the-benchmark",
    "href": "barrolee1994.html#comparing-the-models-with-the-benchmark",
    "title": "Using gingado to understand economic growth",
    "section": "Comparing the models with the benchmark",
    "text": "Comparing the models with the benchmark\ngingado allows users to compare different candidate models with the existing benchmark in a very simple way: using the compare method.\n\n#collapse_output\ncandidates = [gradboosterg_grid, lasso_grid]\nbenchmark.compare(X, y, candidates)\n\nThe output above clearly indicates that after evaluating the models - and their ensemble together with the existing benchmark - at least one of them was better than the current benchmark. Therefore, it will now be the new benchmark.\n\ny_pred = benchmark.predict(X)\npd.DataFrame({\n    'y': y,\n    'y_pred': y_pred\n    }).plot.scatter(x='y', y='y_pred', grid=True)\n\n\npd.DataFrame(y - y_pred).plot.hist(bins=30)"
  },
  {
    "objectID": "barrolee1994.html#model-documentation-1",
    "href": "barrolee1994.html#model-documentation-1",
    "title": "Using gingado to understand economic growth",
    "section": "Model documentation",
    "text": "Model documentation\nAfter this process, we can now see how the model documentation was updated automatically:\n\n#collapse_output\nbenchmark.model_documentation.show_json()\n\nAnd as before, any remaining open questions can be viewed and answered using the same methods as above."
  },
  {
    "objectID": "barrolee1994.html#references",
    "href": "barrolee1994.html#references",
    "title": "Using gingado to understand economic growth",
    "section": "References",
    "text": "References\nBarro, R and J-W Lee (1994): “Sources of economic growth”, Carnegie-Rochester Conference Series on Public Policy, 40, p. 1-46.\nBelloni, A, V Chernozhukov, C Hansen (2011): “Inference for high-dimensional sparse econometric models”, mimeo.\nGiannone, D, M Lenza, G Primiceri (2021): “Economic predictions with big data: the illusion of sparsity”, Econometrica, 89, 5, p. 2409-2437."
  },
  {
    "objectID": "benchmark.html",
    "href": "benchmark.html",
    "title": "Automatic benchmark model",
    "section": "",
    "text": "A gingado Benchmark object seeks to automatise a significant part of creating a benchmark model. Importantly, the Benchmark object also has a compare method that helps users evaluate if candidate models are better than the benchmark, and if one of them is, it becomes the new benchmark. This compare method takes as argument another fitted estimator (which could be itself a solo estimator or a whole pipeline) or a list of fitted estimators.\nBenchmarks start with default values that should perform reasonably well in most settings, but the user is also free to choose any of the benchmark’s components by passing as arguments the data split, pipeline, and/or a dictionary of parameters for the hyperparameter tuning.\n\n0.1 Base class\ngingado has a ggdBenchmark base class that contains the basic functionalities for Benchmark objects. It is not meant to be used by itself, but only as a hyperclass for Benchmark objects. gingado ships with two of these objects that subclass ggdBenchmark: ClassificationBenchmark and RegressionBenchmark. They are both described below in their respective sections.\nUsers are encouraged to submit a PR with their own benchmark models subclassing ggdBenchmark.\n\nsource\n\n\n0.2 ggdBenchmark\n\n ggdBenchmark ()\n\nThe base class for gingado’s Benchmark objects.\n\n\n0.3 Classification tasks\nThe default benchmark for classification tasks is a RandomForestClassifier object. Its parameters are fine-tuned in each case according to the user’s data.\n\nsource\n\n\n0.4 ClassificationBenchmark\n\n ClassificationBenchmark (cv=None,\n                          estimator=RandomForestClassifier(oob_score=True)\n                          , param_grid={'n_estimators': [100, 250],\n                          'max_features': ['sqrt', 'log2', None]},\n                          param_search=<class\n                          'sklearn.model_selection._search.GridSearchCV'>,\n                          scoring=None, auto_document=<class\n                          'gingado.model_documentation.ModelCard'>,\n                          random_state=None, verbose_grid=False,\n                          ensemble_method=<class\n                          'sklearn.ensemble._voting.VotingClassifier'>)\n\nThe base class for gingado’s Benchmark objects.\n\n\nCode\n#collapse_output\nfrom sklearn.datasets import make_classification\n\n# some mock up data\nX, y = make_classification()\n\n# the gingado benchmark\nbm = ClassificationBenchmark(verbose_grid=3).fit(X, y)\n\n# note that now the `bm` object can be used as an estimator\nassert bm.predict(X).shape == y.shape\n\n\nFitting 5 folds for each of 6 candidates, totalling 30 fits\n[CV 1/5] END max_features=sqrt, n_estimators=100;, score=1.000 total time=   0.1s\n[CV 2/5] END max_features=sqrt, n_estimators=100;, score=0.950 total time=   0.1s\n[CV 3/5] END max_features=sqrt, n_estimators=100;, score=0.850 total time=   0.1s\n[CV 4/5] END max_features=sqrt, n_estimators=100;, score=0.850 total time=   0.1s\n[CV 5/5] END max_features=sqrt, n_estimators=100;, score=1.000 total time=   0.1s\n[CV 1/5] END max_features=sqrt, n_estimators=250;, score=1.000 total time=   0.3s\n[CV 2/5] END max_features=sqrt, n_estimators=250;, score=1.000 total time=   0.3s\n[CV 3/5] END max_features=sqrt, n_estimators=250;, score=0.950 total time=   0.3s\n[CV 4/5] END max_features=sqrt, n_estimators=250;, score=0.850 total time=   0.3s\n[CV 5/5] END max_features=sqrt, n_estimators=250;, score=1.000 total time=   0.3s\n[CV 1/5] END max_features=log2, n_estimators=100;, score=1.000 total time=   0.1s\n[CV 2/5] END max_features=log2, n_estimators=100;, score=1.000 total time=   0.1s\n[CV 3/5] END max_features=log2, n_estimators=100;, score=0.950 total time=   0.1s\n[CV 4/5] END max_features=log2, n_estimators=100;, score=0.850 total time=   0.1s\n[CV 5/5] END max_features=log2, n_estimators=100;, score=1.000 total time=   0.1s\n[CV 1/5] END max_features=log2, n_estimators=250;, score=1.000 total time=   0.3s\n[CV 2/5] END max_features=log2, n_estimators=250;, score=1.000 total time=   0.3s\n[CV 3/5] END max_features=log2, n_estimators=250;, score=0.950 total time=   0.3s\n[CV 4/5] END max_features=log2, n_estimators=250;, score=0.850 total time=   0.3s\n[CV 5/5] END max_features=log2, n_estimators=250;, score=1.000 total time=   0.3s\n[CV 1/5] END max_features=None, n_estimators=100;, score=0.950 total time=   0.1s\n[CV 2/5] END max_features=None, n_estimators=100;, score=1.000 total time=   0.1s\n[CV 3/5] END max_features=None, n_estimators=100;, score=0.950 total time=   0.1s\n[CV 4/5] END max_features=None, n_estimators=100;, score=0.850 total time=   0.2s\n[CV 5/5] END max_features=None, n_estimators=100;, score=1.000 total time=   0.2s\n[CV 1/5] END max_features=None, n_estimators=250;, score=0.950 total time=   0.4s\n[CV 2/5] END max_features=None, n_estimators=250;, score=1.000 total time=   0.3s\n[CV 3/5] END max_features=None, n_estimators=250;, score=0.950 total time=   0.4s\n[CV 4/5] END max_features=None, n_estimators=250;, score=0.850 total time=   0.4s\n[CV 5/5] END max_features=None, n_estimators=250;, score=1.000 total time=   0.3s\n\n\nImportantly, gingado automatically provides some information to help the user documentat the benchmark model. More specifically, ggdBenchmark objects collect model information and pass it to a dictionary with key info in a field called model_details.\n\n\nCode\n#collapse_output\nbm.model_documentation.show_json()\n\n\n{'model_details': {'developer': 'Person or organisation developing the model',\n  'datetime': '2022-06-22 07:07:42 ',\n  'version': 'Model version',\n  'type': 'Model type',\n  'info': {'_estimator_type': 'classifier',\n   'best_estimator_': RandomForestClassifier(max_features='sqrt', oob_score=True),\n   'best_index_': 0,\n   'best_params_': {'max_features': 'sqrt', 'n_estimators': 100},\n   'best_score_': 0.8699999999999999,\n   'classes_': array([0, 1]),\n   'cv_results_': {'mean_fit_time': array([0.11491127, 0.26953239, 0.11343093, 0.26668282, 0.12870083,\n           0.29229512]),\n    'std_fit_time': array([0.00490172, 0.00231676, 0.00586447, 0.00314216, 0.01518557,\n           0.00245593]),\n    'mean_score_time': array([0.00815415, 0.01747737, 0.00765586, 0.01703396, 0.00855069,\n           0.01720548]),\n    'std_score_time': array([0.0010515 , 0.00053746, 0.00062577, 0.00023107, 0.00206887,\n           0.00027781]),\n    'param_max_features': masked_array(data=['sqrt', 'sqrt', 'log2', 'log2', None, None],\n                 mask=[False, False, False, False, False, False],\n           fill_value='?',\n                dtype=object),\n    'param_n_estimators': masked_array(data=[100, 250, 100, 250, 100, 250],\n                 mask=[False, False, False, False, False, False],\n           fill_value='?',\n                dtype=object),\n    'params': [{'max_features': 'sqrt', 'n_estimators': 100},\n     {'max_features': 'sqrt', 'n_estimators': 250},\n     {'max_features': 'log2', 'n_estimators': 100},\n     {'max_features': 'log2', 'n_estimators': 250},\n     {'max_features': None, 'n_estimators': 100},\n     {'max_features': None, 'n_estimators': 250}],\n    'split0_test_score': array([0.85, 0.85, 0.85, 0.85, 0.85, 0.85]),\n    'split1_test_score': array([0.95, 0.95, 0.95, 0.95, 0.8 , 0.9 ]),\n    'split2_test_score': array([0.9 , 0.9 , 0.95, 0.9 , 0.8 , 0.9 ]),\n    'split3_test_score': array([0.8, 0.8, 0.8, 0.8, 0.8, 0.8]),\n    'split4_test_score': array([0.85, 0.85, 0.8 , 0.8 , 0.8 , 0.75]),\n    'mean_test_score': array([0.87, 0.87, 0.87, 0.86, 0.81, 0.84]),\n    'std_test_score': array([0.0509902 , 0.0509902 , 0.0678233 , 0.05830952, 0.02      ,\n           0.05830952]),\n    'rank_test_score': array([1, 1, 1, 4, 6, 5], dtype=int32)},\n   'multimetric_': False,\n   'n_features_in_': 20,\n   'n_splits_': 5,\n   'refit_time_': 0.10747504234313965,\n   'scorer_': <function sklearn.metrics._scorer._passthrough_scorer(estimator, *args, **kwargs)>},\n  'paper': 'Paper or other resource for more information',\n  'citation': 'Citation details',\n  'license': 'License',\n  'contact': 'Where to send questions or comments about the model'},\n 'intended_use': {'primary_uses': 'Primary intended uses',\n  'primary_users': 'Primary intended users',\n  'out_of_scope': 'Out-of-scope use cases'},\n 'factors': {'relevant': 'Relevant factors',\n  'evaluation': 'Evaluation factors'},\n 'metrics': {'performance_measures': 'Model performance measures',\n  'thresholds': 'Decision thresholds',\n  'variation_approaches': 'Variation approaches'},\n 'evaluation_data': {'datasets': 'Datasets',\n  'motivation': 'Motivation',\n  'preprocessing': 'Preprocessing'},\n 'training_data': {'training_data': 'Information on training data'},\n 'quant_analyses': {'unitary': 'Unitary results',\n  'intersectional': 'Intersectional results'},\n 'ethical_considerations': {'sensitive_data': 'Does the model use any sensitive data (e.g., protected classes)?',\n  'human_life': 'Is the model intended to inform decisions about mat- ters central to human life or flourishing - e.g., health or safety? Or could it be used in such a way?',\n  'mitigations': 'What risk mitigation strategies were used during model development?',\n  'risks_and_harms': '\\n            What risks may be present in model usage? Try to identify the potential recipients, likelihood, and magnitude of harms.\\n            If these cannot be determined, note that they were consid- ered but remain unknown',\n  'use_cases': 'Are there any known model use cases that are especially fraught?',\n  'additional_information': '\\n            If possible, this section should also include any additional ethical considerations that went into model development,\\n            for example, review by an external board, or testing with a specific community.'},\n 'caveats_recommendations': {'caveats': 'For example, did the results suggest any further testing? Were there any relevant groups that were not represented in the evaluation dataset?',\n  'recommendations': 'Are there additional recommendations for model use? What are the ideal characteristics of an evaluation dataset for this model?'}}\n\n\nIt is also simple to define as benchmark a model that you already fitted and still benefit from the other functionalities provided by Benchmark class. This can also be done in case you are using a saved version of a fitted model (eg, the model you are using in production) and want to have that as the benchmark.\n\n\nCode\nfrom sklearn.ensemble import RandomForestClassifier\nforest = RandomForestClassifier().fit(X, y)\n\nbm.set_benchmark(estimator=forest)\n\nassert forest == bm.benchmark\nassert hasattr(bm.benchmark, \"predict\")\nassert bm.predict(X).shape == y.shape\n\n\n\n\n0.5 Regression tasks\nThe default benchmark for regression tasks is a RandomForestRegressor object. Its parameters are fine-tuned in each case according to the user’s data.\n\nsource\n\n\n0.6 RegressionBenchmark\n\n RegressionBenchmark (cv=None,\n                      estimator=RandomForestRegressor(oob_score=True),\n                      param_grid={'n_estimators': [100, 250],\n                      'max_features': ['sqrt', 'log2', None]},\n                      param_search=<class\n                      'sklearn.model_selection._search.GridSearchCV'>,\n                      scoring=None, auto_document=<class\n                      'gingado.model_documentation.ModelCard'>,\n                      random_state=None, verbose_grid=False,\n                      ensemble_method=<class\n                      'sklearn.ensemble._voting.VotingRegressor'>)\n\nThe base class for gingado’s Benchmark objects.\n\n\nCode\n#collapse_output\nfrom sklearn.datasets import make_regression\nfrom sklearn.ensemble import AdaBoostRegressor\n\n# some mock up data\nX, y = make_regression()\n\n# the gingado benchmark\nbm = RegressionBenchmark(verbose_grid=2).fit(X, y)\n\n# note that now the `bm` object can be used as an estimator\nassert bm.predict(X).shape == y.shape\n\n# the user might also like to set another model as the benchmark\nadaboost = AdaBoostRegressor().fit(X, y)\nbm.set_benchmark(estimator=adaboost)\n\nassert adaboost == bm.benchmark\nassert hasattr(bm.benchmark, \"predict\")\nassert bm.predict(X).shape == y.shape\n\n\nFitting 5 folds for each of 6 candidates, totalling 30 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.6s\n\n\nBelow we compare the benchmark (set above manually to be the adaboost algorithm) with two other candidate models: a Gaussian process and a linear Support Vector Machine (SVM).\n\n\nCode\n#collapse_output\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.svm import LinearSVR\n\ngauss_reg = GaussianProcessRegressor().fit(X, y)\nsvm_reg = LinearSVR().fit(X, y)\n\nbm.compare(X, y, candidates=[gauss_reg, svm_reg])\n\n\nFitting 5 folds for each of 4 candidates, totalling 20 fits\n[CV] END candidate_estimator=AdaBoostRegressor(), candidate_estimator__base_estimator=None, candidate_estimator__learning_rate=1.0, candidate_estimator__loss=linear, candidate_estimator__n_estimators=50, candidate_estimator__random_state=None; total time=   0.1s\n[CV] END candidate_estimator=AdaBoostRegressor(), candidate_estimator__base_estimator=None, candidate_estimator__learning_rate=1.0, candidate_estimator__loss=linear, candidate_estimator__n_estimators=50, candidate_estimator__random_state=None; total time=   0.1s\n[CV] END candidate_estimator=AdaBoostRegressor(), candidate_estimator__base_estimator=None, candidate_estimator__learning_rate=1.0, candidate_estimator__loss=linear, candidate_estimator__n_estimators=50, candidate_estimator__random_state=None; total time=   0.1s\n[CV] END candidate_estimator=AdaBoostRegressor(), candidate_estimator__base_estimator=None, candidate_estimator__learning_rate=1.0, candidate_estimator__loss=linear, candidate_estimator__n_estimators=50, candidate_estimator__random_state=None; total time=   0.1s\n[CV] END candidate_estimator=AdaBoostRegressor(), candidate_estimator__base_estimator=None, candidate_estimator__learning_rate=1.0, candidate_estimator__loss=linear, candidate_estimator__n_estimators=50, candidate_estimator__random_state=None; total time=   0.1s\n[CV] END candidate_estimator=GaussianProcessRegressor(), candidate_estimator__alpha=1e-10, candidate_estimator__copy_X_train=True, candidate_estimator__kernel=None, candidate_estimator__n_restarts_optimizer=0, candidate_estimator__normalize_y=False, candidate_estimator__optimizer=fmin_l_bfgs_b, candidate_estimator__random_state=None; total time=   0.0s\n[CV] END candidate_estimator=GaussianProcessRegressor(), candidate_estimator__alpha=1e-10, candidate_estimator__copy_X_train=True, candidate_estimator__kernel=None, candidate_estimator__n_restarts_optimizer=0, candidate_estimator__normalize_y=False, candidate_estimator__optimizer=fmin_l_bfgs_b, candidate_estimator__random_state=None; total time=   0.0s\n[CV] END candidate_estimator=GaussianProcessRegressor(), candidate_estimator__alpha=1e-10, candidate_estimator__copy_X_train=True, candidate_estimator__kernel=None, candidate_estimator__n_restarts_optimizer=0, candidate_estimator__normalize_y=False, candidate_estimator__optimizer=fmin_l_bfgs_b, candidate_estimator__random_state=None; total time=   0.0s\n[CV] END candidate_estimator=GaussianProcessRegressor(), candidate_estimator__alpha=1e-10, candidate_estimator__copy_X_train=True, candidate_estimator__kernel=None, candidate_estimator__n_restarts_optimizer=0, candidate_estimator__normalize_y=False, candidate_estimator__optimizer=fmin_l_bfgs_b, candidate_estimator__random_state=None; total time=   0.0s\n[CV] END candidate_estimator=GaussianProcessRegressor(), candidate_estimator__alpha=1e-10, candidate_estimator__copy_X_train=True, candidate_estimator__kernel=None, candidate_estimator__n_restarts_optimizer=0, candidate_estimator__normalize_y=False, candidate_estimator__optimizer=fmin_l_bfgs_b, candidate_estimator__random_state=None; total time=   0.0s\n[CV] END candidate_estimator=LinearSVR(), candidate_estimator__C=1.0, candidate_estimator__dual=True, candidate_estimator__epsilon=0.0, candidate_estimator__fit_intercept=True, candidate_estimator__intercept_scaling=1.0, candidate_estimator__loss=epsilon_insensitive, candidate_estimator__max_iter=1000, candidate_estimator__random_state=None, candidate_estimator__tol=0.0001, candidate_estimator__verbose=0; total time=   0.0s\n[CV] END candidate_estimator=LinearSVR(), candidate_estimator__C=1.0, candidate_estimator__dual=True, candidate_estimator__epsilon=0.0, candidate_estimator__fit_intercept=True, candidate_estimator__intercept_scaling=1.0, candidate_estimator__loss=epsilon_insensitive, candidate_estimator__max_iter=1000, candidate_estimator__random_state=None, candidate_estimator__tol=0.0001, candidate_estimator__verbose=0; total time=   0.0s\n[CV] END candidate_estimator=LinearSVR(), candidate_estimator__C=1.0, candidate_estimator__dual=True, candidate_estimator__epsilon=0.0, candidate_estimator__fit_intercept=True, candidate_estimator__intercept_scaling=1.0, candidate_estimator__loss=epsilon_insensitive, candidate_estimator__max_iter=1000, candidate_estimator__random_state=None, candidate_estimator__tol=0.0001, candidate_estimator__verbose=0; total time=   0.0s\n[CV] END candidate_estimator=LinearSVR(), candidate_estimator__C=1.0, candidate_estimator__dual=True, candidate_estimator__epsilon=0.0, candidate_estimator__fit_intercept=True, candidate_estimator__intercept_scaling=1.0, candidate_estimator__loss=epsilon_insensitive, candidate_estimator__max_iter=1000, candidate_estimator__random_state=None, candidate_estimator__tol=0.0001, candidate_estimator__verbose=0; total time=   0.0s\n[CV] END candidate_estimator=LinearSVR(), candidate_estimator__C=1.0, candidate_estimator__dual=True, candidate_estimator__epsilon=0.0, candidate_estimator__fit_intercept=True, candidate_estimator__intercept_scaling=1.0, candidate_estimator__loss=epsilon_insensitive, candidate_estimator__max_iter=1000, candidate_estimator__random_state=None, candidate_estimator__tol=0.0001, candidate_estimator__verbose=0; total time=   0.0s\n[CV] END candidate_estimator=VotingRegressor(estimators=[('candidate_1', AdaBoostRegressor()),\n                            ('candidate_2', GaussianProcessRegressor()),\n                            ('candidate_3', LinearSVR())]); total time=   0.1s\n[CV] END candidate_estimator=VotingRegressor(estimators=[('candidate_1', AdaBoostRegressor()),\n                            ('candidate_2', GaussianProcessRegressor()),\n                            ('candidate_3', LinearSVR())]); total time=   0.1s\n[CV] END candidate_estimator=VotingRegressor(estimators=[('candidate_1', AdaBoostRegressor()),\n                            ('candidate_2', GaussianProcessRegressor()),\n                            ('candidate_3', LinearSVR())]); total time=   0.1s\n[CV] END candidate_estimator=VotingRegressor(estimators=[('candidate_1', AdaBoostRegressor()),\n                            ('candidate_2', GaussianProcessRegressor()),\n                            ('candidate_3', LinearSVR())]); total time=   0.1s\n[CV] END candidate_estimator=VotingRegressor(estimators=[('candidate_1', AdaBoostRegressor()),\n                            ('candidate_2', GaussianProcessRegressor()),\n                            ('candidate_3', LinearSVR())]); total time=   0.1s\nBenchmark updated!\nNew benchmark:\nPipeline(steps=[('candidate_estimator', LinearSVR())])\n\n\nNote that when the benchmark object finds a model that performs better than it does, the user is informed that the benchmark is updated and the new benchmark model is shown. This only happens when the argument update_benchmark is set to True (as default).\nBelow we can see by how much it outperformed the other candidates, including the previous benchmark model and an ensemble of the previous benchmark and all the candidates. It is also a good opportunity to see how stable the performance of each model was, as judged by the standard deviation of the scores across the validation folds.\n\n\nCode\npd.DataFrame(bm.benchmark.cv_results_)[['params', 'mean_test_score', 'std_test_score', 'rank_test_score']]\n\n\n\n\n\n\n  \n    \n      \n      params\n      mean_test_score\n      std_test_score\n      rank_test_score\n    \n  \n  \n    \n      0\n      {'candidate_estimator': (DecisionTreeRegressor...\n      0.307168\n      0.088244\n      2\n    \n    \n      1\n      {'candidate_estimator': GaussianProcessRegress...\n      -0.047183\n      0.049953\n      4\n    \n    \n      2\n      {'candidate_estimator': LinearSVR(), 'candidat...\n      0.372070\n      0.118864\n      1\n    \n    \n      3\n      {'candidate_estimator': VotingRegressor(estima...\n      0.268379\n      0.101484\n      3\n    \n  \n\n\n\n\n\n\n0.7 General comments on benchmarks\n\n0.7.1 Scoring\nClassificationBenchmark and RegressionBenchmark both use the default scoring method for comparing model alternatives, both during estimation of the benchmark model and when comparing this benchmark with candidate models. Users are encouraged to consider if another scoring method is more suitable for their use case. More information on available scoring methods that are compatible with gingado Benchmark objects can be found here.\n\n\n\n0.8 Data split\ngingado benchmarks rely on hyperparameter tuning to discover the benchmark specification that is most likely to perform better with the user data. This tuning in turn depends on a data splitting strategy for the cross-validation. By default, gingado uses StratifiedShuffleSplit if the data is not time series and TimeSeriesSplit otherwise.\nPlease refer to this page for more information on the different Splitter classes available on scikit-learn, and this page for practical advice on how to choose a splitter for data that are not time series. Any one of these objects (or a custom splitter that is compatible with them) can be passed to a Benchmark object.\nThe API does not accept custom parameters for the splitters. Users that wish to use specific parameters should include the actual Splitter object as the parameter.\n\n\n0.9 Custom benchmarks\ngingado provides users with two Benchmark objects out of the box: ClassificationBenchmark and RegressionBenchmark, to be used depending on the task at hand. Both classes derive from a base class ggdBenchmark, which implements methods that facilitate model comparison. Users that want to create a customised benchmark model for themselves have two options:\n\nthe simpler possibility is to train the estimator as usual, and then assign the fitted estimator to a Benchmark object.\nif the user wants more control over the fitting process of estimating the benchmark, they can create a class that subclasses from ggdBenchmark and either implements custom fit, predict and score methods, or also subclasses from scikit-learn’s BaseEstimator.\n\nIn any case, if the user wants the benchmark to automatically detect if the data is a time series and also to document the model right after fitting, the fit method should call self._fit on the data. Otherwise, the user can simply implement any consistent logic in fit as the user sees fit (pun intended)."
  },
  {
    "objectID": "CONTRIBUTING.html",
    "href": "CONTRIBUTING.html",
    "title": "gingado",
    "section": "",
    "text": "Before anything else, please install the git hooks that run automatic scripts during each commit and merge to strip the notebooks of superfluous metadata (and avoid merge conflicts). After cloning the repository, run the following command inside it:\nnbdev_install_git_hooks\n\n\n\n\nEnsure the bug was not already reported by searching on GitHub under Issues.\nIf you’re unable to find an open issue addressing the problem, open a new one. Be sure to include a title and clear description, as much relevant information as possible, and a code sample or an executable test case demonstrating the expected behavior that is not occurring.\nBe sure to add the complete error messages.\n\n\n\n\nOpen a new GitHub pull request with the patch.\nEnsure that your PR includes a test that fails without your patch, and pass with it.\nEnsure the PR description clearly describes the problem and solution. Include the relevant issue number if applicable.\n\n\n\n\n\n\nKeep each PR focused. While it’s more convenient, do not combine several unrelated fixes together. Create as many branches as needing to keep each PR focused.\nDo not mix style changes/fixes with “functional” changes. It’s very difficult to review such PRs and it most likely get rejected.\nDo not add/remove vertical whitespace. Preserve the original style of the file you edit as much as you can.\nDo not turn an already submitted PR into your development playground. If after you submitted PR, you discovered that more work is needed - close the PR, do the required work and then submit a new PR. Otherwise each of your commits requires attention from maintainers of the project.\nIf, however, you submitted a PR and received a request for changes, you should proceed with commits inside that PR, so that the maintainer can see the incremental fixes and won’t need to review the whole PR again. In the exception case where you realize it’ll take many many commits to complete the requests, then it’s probably best to close the PR, do the work and then submit it again. Use common sense where you’d choose one way over another.\n\n\n\n\n\nDocs are automatically created from the notebooks in the nbs folder."
  },
  {
    "objectID": "dataset_transformation.html",
    "href": "dataset_transformation.html",
    "title": "Dataset transformation for uploading",
    "section": "",
    "text": "For more information on this data, consult the datasets page.\n\n\nCode\nimport pandas as pd\nfrom scipy import io\n\ngrowth_data = io.loadmat('data/GrowthData.mat')\ncolnames = [m[0].strip() for m in growth_data['Mnem'][0]]\ndf = pd.DataFrame(growth_data['data'], columns=colnames)\n\ndf.to_csv('gingado/dataset_BarroLee_1994.csv')\n\n\n\n\nCode\npd.set_option('display.max_rows', None)\ndf.describe().T\n\n\n\n\n\n\n  \n    \n      \n      count\n      mean\n      std\n      min\n      25%\n      50%\n      75%\n      max\n    \n  \n  \n    \n      gdpsh465\n      90.0\n      7.702907\n      0.896179\n      5.762051\n      7.131539\n      7.725700\n      8.441914\n      9.229849\n    \n    \n      bmp1l\n      90.0\n      0.168747\n      0.249116\n      0.000000\n      0.000000\n      0.063800\n      0.274550\n      1.637800\n    \n    \n      freeop\n      90.0\n      0.220102\n      0.074861\n      0.078488\n      0.166044\n      0.203972\n      0.286425\n      0.416234\n    \n    \n      freetar\n      90.0\n      0.028334\n      0.021855\n      0.000000\n      0.011589\n      0.025426\n      0.039745\n      0.109921\n    \n    \n      h65\n      90.0\n      0.111556\n      0.101361\n      0.002000\n      0.032250\n      0.089000\n      0.147500\n      0.573000\n    \n    \n      hm65\n      90.0\n      0.137156\n      0.116826\n      0.004000\n      0.042250\n      0.114500\n      0.181000\n      0.635000\n    \n    \n      hf65\n      90.0\n      0.082233\n      0.091549\n      0.000000\n      0.014000\n      0.055000\n      0.113000\n      0.527000\n    \n    \n      p65\n      90.0\n      0.893333\n      0.164938\n      0.290000\n      0.832500\n      0.985000\n      1.000000\n      1.000000\n    \n    \n      pm65\n      90.0\n      0.919556\n      0.134632\n      0.370000\n      0.882500\n      1.000000\n      1.000000\n      1.000000\n    \n    \n      pf65\n      90.0\n      0.849556\n      0.210899\n      0.210000\n      0.762500\n      0.970000\n      1.000000\n      1.000000\n    \n    \n      s65\n      90.0\n      0.406556\n      0.247219\n      0.020000\n      0.182500\n      0.395000\n      0.555000\n      0.910000\n    \n    \n      sm65\n      90.0\n      0.436222\n      0.246543\n      0.030000\n      0.220000\n      0.420000\n      0.607500\n      0.950000\n    \n    \n      sf65\n      90.0\n      0.379778\n      0.265800\n      0.010000\n      0.140000\n      0.345000\n      0.530000\n      0.920000\n    \n    \n      fert65\n      90.0\n      4.742000\n      1.974886\n      1.450000\n      2.845000\n      5.060000\n      6.560000\n      8.000000\n    \n    \n      mort65\n      90.0\n      0.070467\n      0.047166\n      0.009000\n      0.024000\n      0.064500\n      0.110750\n      0.183000\n    \n    \n      lifee065\n      90.0\n      4.102025\n      0.167895\n      3.693867\n      3.994061\n      4.110874\n      4.258443\n      4.317488\n    \n    \n      gpop1\n      90.0\n      0.021301\n      0.009851\n      0.002600\n      0.013150\n      0.022900\n      0.029900\n      0.039000\n    \n    \n      fert1\n      90.0\n      4.962444\n      1.936886\n      1.742000\n      2.906500\n      5.394000\n      6.680000\n      8.000000\n    \n    \n      mort1\n      90.0\n      0.087211\n      0.051937\n      0.015000\n      0.031500\n      0.085000\n      0.131000\n      0.204000\n    \n    \n      invsh41\n      90.0\n      0.196663\n      0.086981\n      0.027133\n      0.129175\n      0.191680\n      0.254760\n      0.475500\n    \n    \n      geetot1\n      90.0\n      0.035594\n      0.014543\n      0.011700\n      0.024325\n      0.033900\n      0.046400\n      0.077000\n    \n    \n      geerec1\n      90.0\n      0.029782\n      0.012673\n      0.004100\n      0.020925\n      0.028350\n      0.038250\n      0.068700\n    \n    \n      gde1\n      90.0\n      0.032133\n      0.033750\n      0.005000\n      0.015250\n      0.020500\n      0.039750\n      0.251000\n    \n    \n      govwb1\n      90.0\n      0.126661\n      0.045588\n      0.061500\n      0.096000\n      0.120950\n      0.145125\n      0.352300\n    \n    \n      govsh41\n      90.0\n      0.155161\n      0.061317\n      0.035500\n      0.118350\n      0.147250\n      0.183850\n      0.385900\n    \n    \n      gvxdxe41\n      90.0\n      0.094915\n      0.053846\n      0.010000\n      0.058965\n      0.086585\n      0.122365\n      0.310460\n    \n    \n      high65\n      90.0\n      4.203000\n      5.252551\n      0.120000\n      1.302500\n      2.735000\n      4.952500\n      30.900000\n    \n    \n      highm65\n      90.0\n      5.531556\n      5.908117\n      0.230000\n      1.890000\n      3.990000\n      6.525000\n      33.400000\n    \n    \n      highf65\n      90.0\n      2.946889\n      4.781800\n      0.010000\n      0.667500\n      1.320000\n      2.992500\n      28.500000\n    \n    \n      highc65\n      90.0\n      2.455556\n      2.487163\n      0.090000\n      1.007500\n      1.695000\n      3.135000\n      15.630000\n    \n    \n      highcm65\n      90.0\n      3.434222\n      3.111632\n      0.180000\n      1.347500\n      2.445000\n      4.517500\n      18.830000\n    \n    \n      highcf65\n      90.0\n      1.529667\n      2.066383\n      0.010000\n      0.432500\n      0.765000\n      1.925000\n      12.770000\n    \n    \n      human65\n      90.0\n      4.214878\n      2.530420\n      0.301000\n      2.212000\n      3.803500\n      5.674000\n      11.158000\n    \n    \n      humanm65\n      90.0\n      4.698267\n      2.423311\n      0.568000\n      2.888000\n      4.244500\n      6.030250\n      11.535000\n    \n    \n      humanf65\n      90.0\n      3.745967\n      2.692534\n      0.043000\n      1.605250\n      3.194000\n      5.178250\n      10.798000\n    \n    \n      hyr65\n      90.0\n      0.133178\n      0.151704\n      0.004000\n      0.046500\n      0.087000\n      0.161500\n      0.829000\n    \n    \n      hyrm65\n      90.0\n      0.179300\n      0.177153\n      0.008000\n      0.066250\n      0.125500\n      0.215250\n      0.960000\n    \n    \n      hyrf65\n      90.0\n      0.089533\n      0.133602\n      0.000000\n      0.021250\n      0.043500\n      0.095000\n      0.712000\n    \n    \n      no65\n      90.0\n      34.723111\n      29.101051\n      0.000000\n      4.527500\n      31.110000\n      59.075000\n      89.460000\n    \n    \n      nom65\n      90.0\n      28.999889\n      25.307305\n      0.000000\n      4.227500\n      25.180000\n      49.545000\n      82.350000\n    \n    \n      nof65\n      90.0\n      40.327111\n      33.446850\n      0.000000\n      4.275000\n      36.400000\n      69.550000\n      98.610000\n    \n    \n      pinstab1\n      90.0\n      0.114160\n      0.214415\n      0.000000\n      0.000000\n      0.000706\n      0.103919\n      1.068500\n    \n    \n      pop65\n      90.0\n      39825.300000\n      87794.312844\n      1482.000000\n      4961.000000\n      12275.000000\n      42263.500000\n      620701.000000\n    \n    \n      worker65\n      90.0\n      0.369268\n      0.069487\n      0.215600\n      0.311050\n      0.369400\n      0.410075\n      0.526000\n    \n    \n      pop1565\n      90.0\n      0.375158\n      0.093359\n      0.201800\n      0.280600\n      0.421100\n      0.456325\n      0.515800\n    \n    \n      pop6565\n      90.0\n      0.059175\n      0.037660\n      0.021548\n      0.031189\n      0.036967\n      0.086604\n      0.151105\n    \n    \n      sec65\n      90.0\n      15.318000\n      13.039270\n      0.450000\n      5.700000\n      11.200000\n      18.865000\n      57.100000\n    \n    \n      secm65\n      90.0\n      16.622111\n      12.421347\n      0.750000\n      7.730000\n      12.780000\n      22.307500\n      56.370000\n    \n    \n      secf65\n      90.0\n      14.024111\n      14.121763\n      0.170000\n      3.747500\n      8.865000\n      16.840000\n      57.800000\n    \n    \n      secc65\n      90.0\n      6.444111\n      6.351368\n      0.130000\n      2.352500\n      4.005000\n      7.870000\n      34.090000\n    \n    \n      seccm65\n      90.0\n      6.702889\n      6.182263\n      0.150000\n      2.635000\n      4.735000\n      8.910000\n      31.270000\n    \n    \n      seccf65\n      90.0\n      6.171778\n      6.857261\n      0.040000\n      1.500000\n      3.870000\n      8.042500\n      36.610000\n    \n    \n      syr65\n      90.0\n      0.912422\n      0.823222\n      0.033000\n      0.357750\n      0.678500\n      1.130750\n      4.211000\n    \n    \n      syrm65\n      90.0\n      1.045444\n      0.831756\n      0.057000\n      0.467500\n      0.770500\n      1.246750\n      4.227000\n    \n    \n      syrf65\n      90.0\n      0.783767\n      0.839486\n      0.010000\n      0.215250\n      0.512500\n      1.002000\n      4.198000\n    \n    \n      teapri65\n      90.0\n      33.203333\n      9.818516\n      18.200000\n      27.425000\n      32.200000\n      37.475000\n      62.400000\n    \n    \n      teasec65\n      90.0\n      19.412222\n      6.384194\n      7.200000\n      15.350000\n      18.400000\n      22.800000\n      37.100000\n    \n    \n      ex1\n      90.0\n      0.133981\n      0.118708\n      0.017800\n      0.063450\n      0.092300\n      0.169225\n      0.747000\n    \n    \n      im1\n      90.0\n      0.144356\n      0.120937\n      0.022200\n      0.070625\n      0.116300\n      0.181475\n      0.848900\n    \n    \n      xr65\n      90.0\n      42.663856\n      119.335089\n      0.003000\n      1.099000\n      4.762000\n      19.619000\n      652.850000\n    \n    \n      tot1\n      90.0\n      0.009236\n      0.059182\n      -0.156878\n      -0.016877\n      0.004890\n      0.018526\n      0.207492\n    \n    \n      Outcome\n      90.0\n      0.045349\n      0.051314\n      -0.100990\n      0.021045\n      0.046209\n      0.074029\n      0.185526"
  },
  {
    "objectID": "forecast.html",
    "href": "forecast.html",
    "title": "Using gingado to forecast financial series",
    "section": "",
    "text": "This notebook illustrates the use of gingado to build models for forecasting, using foreign exchange (FX) rate movements as an example. Please note that the results or the model should not be taken as investment advice.\nForecasting exchange rates is notoriously difficult (Rossi, 2013 and references therein).\nThis exercise will illustrate various functionalities provided by gingado:\nUnlike most scripts that concentrate the package imports at the beginning, this walkthrough will import as needed, to better highlight where each contribution of gingado is used in the workflow.\nFirst, we will use gingado to run a simple example with the following characteristics:"
  },
  {
    "objectID": "forecast.html#downloading-fx-rates",
    "href": "forecast.html#downloading-fx-rates",
    "title": "Using gingado to forecast financial series",
    "section": "1 Downloading FX rates",
    "text": "1 Downloading FX rates\nIn this exercise, we will concentrate on the bilateral FX rates between the 🇺🇸 US Dollar (USD) and the 🇧🇷 Brazilian Real (BRL), 🇨🇦 Canadian Dollar (CAD), 🇨🇭 Swiss Franc (CHF), 🇪🇺 Euro (EUR), 🇬🇧 British Pound (GBP), 🇯🇵 Japanese Yen (JPY) and 🇲🇽 Mexican Peso (MXN).\nThe rates are standardised to measure the units in foreign currency bought by one USD. Therefore, positive returns represent USD is more valued compared to the other currency, and vice-versa.\n\n\nCode\nfrom gingado.utils import load_SDMX_data\n\n\n\n\nCode\ndf = load_SDMX_data(\n    sources={'BIS': 'WS_XRU_D'},\n    keys={\n        'FREQ': 'D', \n        'CURRENCY': ['BRL', 'CAD', 'CHF', 'EUR', 'GBP', 'JPY', 'MXN'],\n        'REF_AREA': ['BR', 'CA', 'CH', 'XM', 'GB', 'JP', 'MX']\n        },\n    params={'startPeriod': 2003}\n)\n\n\nQuerying data from BIS's dataflow 'WS_XRU' - US dollar exchange rates, m,q,a...\nthis dataflow does not have data in the desired frequency and time period.\nQuerying data from BIS's dataflow 'WS_XRU_D' - US dollar exchange rates, daily...\n\n\nThe code below simplifies the column names by removing the identification of the SDMX sources, dataflows and keys and replacing it with the usual code for the bilateral exchange rates.\n\n\nCode\nprint(\"Original column names:\")\nprint(df.columns)\n\ndf.columns = ['USD' + col.split('_')[9] for col in df.columns]\n\nprint(\"New column names:\")\nprint(df.columns)\n\n\nOriginal column names:\nIndex(['BIS__WS_XRU_D_D__BR__BRL__A', 'BIS__WS_XRU_D_D__CA__CAD__A',\n       'BIS__WS_XRU_D_D__CH__CHF__A', 'BIS__WS_XRU_D_D__GB__GBP__A',\n       'BIS__WS_XRU_D_D__JP__JPY__A', 'BIS__WS_XRU_D_D__MX__MXN__A',\n       'BIS__WS_XRU_D_D__XM__EUR__A'],\n      dtype='object')\nNew column names:\nIndex(['USDBRL', 'USDCAD', 'USDCHF', 'USDGBP', 'USDJPY', 'USDMXN', 'USDEUR'], dtype='object')\n\n\nThe dataset looks like this so far (most recent 5 rows displayed only):\n\n\nCode\ndf.tail()\n\n\n\n\n\n\n  \n    \n      \n      USDBRL\n      USDCAD\n      USDCHF\n      USDGBP\n      USDJPY\n      USDMXN\n      USDEUR\n    \n    \n      TIME_PERIOD\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2022-06-14\n      5.102277\n      1.293724\n      0.994451\n      0.828339\n      134.538844\n      20.554152\n      0.956755\n    \n    \n      2022-06-15\n      5.096731\n      1.294027\n      1.000383\n      0.827610\n      134.685073\n      20.588918\n      0.958681\n    \n    \n      2022-06-16\n      5.053750\n      1.292885\n      0.975192\n      0.822596\n      132.923077\n      20.587981\n      0.961538\n    \n    \n      2022-06-17\n      5.132939\n      1.299924\n      0.963666\n      0.815373\n      134.665268\n      20.453366\n      0.953652\n    \n    \n      2022-06-20\n      5.145669\n      1.299040\n      0.966245\n      0.815328\n      134.962442\n      20.254445\n      0.950841\n    \n  \n\n\n\n\nWe are interested in the percentage change from the previous day.\n\n\nCode\nFX_rate_changes = df.pct_change()\nFX_rate_changes.dropna(inplace=True)\n\n\n\n\nCode\nFX_rate_changes.plot(subplots=True, layout=(4, 2), figsize=(15, 15), sharex=True)\n\n\narray([[<AxesSubplot:xlabel='TIME_PERIOD'>,\n        <AxesSubplot:xlabel='TIME_PERIOD'>],\n       [<AxesSubplot:xlabel='TIME_PERIOD'>,\n        <AxesSubplot:xlabel='TIME_PERIOD'>],\n       [<AxesSubplot:xlabel='TIME_PERIOD'>,\n        <AxesSubplot:xlabel='TIME_PERIOD'>],\n       [<AxesSubplot:xlabel='TIME_PERIOD'>,\n        <AxesSubplot:xlabel='TIME_PERIOD'>]], dtype=object)"
  },
  {
    "objectID": "forecast.html#augmenting-the-dataset",
    "href": "forecast.html#augmenting-the-dataset",
    "title": "Using gingado to forecast financial series",
    "section": "2 Augmenting the dataset",
    "text": "2 Augmenting the dataset\nWe will complement the FX rates data with two other datasets: * daily central bank policy rates from the Bank for International Settlements (BIS), and * the daily Composite Indicator of Systemic Stress (CISS), created by Holló, Kremer, and Lo Duca (2012) and updated by the European Central Bank (ECB).\n\n\nCode\nfrom gingado.augmentation import AugmentSDMX\n\n\n\n\nCode\nX = AugmentSDMX(sources={'BIS': 'WS_CBPOL_D', 'ECB': 'CISS'}).fit_transform(FX_rate_changes)\n\n\nQuerying data from BIS's dataflow 'WS_CBPOL_D' - Policy rates daily...\nQuerying data from ECB's dataflow 'CISS' - Composite Indicator of Systemic Stress...\n\n\n2022-06-28 08:34:46,396 pandasdmx.reader.sdmxml - INFO: Use supplied dsd=… argument for non–structure-specific message\n\n\n\n\n\n\n\n\nNote\n\n\n\nit is acceptable in gingado to pass the variable of interest (the “y”, or in this case, FX_rate_changes) as the X argument in fit_transform. This is because this series will also be merged with the additional, augmented data and subsequently lagged along with it.\n\n\nYou can see below that the column names for the newly added columns reflect the source (BIS or ECB), the dataflow (separated from the source by a double underline), and then the specific keys to the series, which are specific to each dataflow.\n\n\nCode\nX.columns\n\n\nIndex(['USDBRL', 'USDCAD', 'USDCHF', 'USDGBP', 'USDJPY', 'USDMXN', 'USDEUR',\n       'BIS__WS_CBPOL_D_D__AR', 'BIS__WS_CBPOL_D_D__AU',\n       'BIS__WS_CBPOL_D_D__BR', 'BIS__WS_CBPOL_D_D__CA',\n       'BIS__WS_CBPOL_D_D__CH', 'BIS__WS_CBPOL_D_D__CL',\n       'BIS__WS_CBPOL_D_D__CN', 'BIS__WS_CBPOL_D_D__CO',\n       'BIS__WS_CBPOL_D_D__CZ', 'BIS__WS_CBPOL_D_D__DK',\n       'BIS__WS_CBPOL_D_D__GB', 'BIS__WS_CBPOL_D_D__HK',\n       'BIS__WS_CBPOL_D_D__HR', 'BIS__WS_CBPOL_D_D__HU',\n       'BIS__WS_CBPOL_D_D__ID', 'BIS__WS_CBPOL_D_D__IL',\n       'BIS__WS_CBPOL_D_D__IN', 'BIS__WS_CBPOL_D_D__IS',\n       'BIS__WS_CBPOL_D_D__JP', 'BIS__WS_CBPOL_D_D__KR',\n       'BIS__WS_CBPOL_D_D__MK', 'BIS__WS_CBPOL_D_D__MX',\n       'BIS__WS_CBPOL_D_D__MY', 'BIS__WS_CBPOL_D_D__NO',\n       'BIS__WS_CBPOL_D_D__NZ', 'BIS__WS_CBPOL_D_D__PE',\n       'BIS__WS_CBPOL_D_D__PH', 'BIS__WS_CBPOL_D_D__PL',\n       'BIS__WS_CBPOL_D_D__RO', 'BIS__WS_CBPOL_D_D__RS',\n       'BIS__WS_CBPOL_D_D__RU', 'BIS__WS_CBPOL_D_D__SA',\n       'BIS__WS_CBPOL_D_D__SE', 'BIS__WS_CBPOL_D_D__TH',\n       'BIS__WS_CBPOL_D_D__TR', 'BIS__WS_CBPOL_D_D__US',\n       'BIS__WS_CBPOL_D_D__XM', 'BIS__WS_CBPOL_D_D__ZA',\n       'ECB__CISS_D__AT__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__BE__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__CN__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__DE__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__ES__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__FI__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__FR__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__GB__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__IE__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__IT__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__NL__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__PT__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_BM__CON',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_CI__IDX',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_CO__CON',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_EM__CON',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_FI__CON',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_FX__CON',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_MM__CON',\n       'ECB__CISS_D__US__Z0Z__4F__EC__SS_CI__IDX',\n       'ECB__CISS_D__US__Z0Z__4F__EC__SS_CIN__IDX'],\n      dtype='object')\n\n\nBefore proceeding, we also include a differentiated version of the central bank policy data. It will be sparse, since these changes occur infrequently for most central banks, but it can help the model uncover how FX rate changes respond to central bank policy changes.\n\n\nCode\nimport pandas as pd\n\n\n\n\nCode\nX_diff = X.loc[:, X.columns.str.contains(\"BIS__WS_CBPOL_D\", case=False)].diff()\nX_diff.columns = [col + \"_diff\" for col in X_diff.columns]\nX = pd.concat([X, X_diff], axis=1)\n\n\nThis is how the data looks like now. Note that the names of the added columns reflect the source, dataflow and keys, all separated by underlines (the source is separated from the dataflow by two underlines at all cases). For example, the last key is the jurisdiction of the central bank.\nWe will keep all the newly added variables - even those that are from countries not in the currency list. This is because the model may uncover any relationship of interest between central bank policies from other countries and each particular currency pair.\n\n\nCode\nX.describe().transpose()\n\n\n\n\n\n\n  \n    \n      \n      count\n      mean\n      std\n      min\n      25%\n      50%\n      75%\n      max\n    \n  \n  \n    \n      USDBRL\n      5033.0\n      0.000132\n      0.010611\n      -0.080226\n      -0.005765\n      -0.000017\n      0.005375\n      0.120503\n    \n    \n      USDCAD\n      5033.0\n      -0.000021\n      0.005852\n      -0.043367\n      -0.003223\n      -0.000133\n      0.003036\n      0.036864\n    \n    \n      USDCHF\n      5033.0\n      -0.000051\n      0.006444\n      -0.139149\n      -0.003229\n      0.000000\n      0.003257\n      0.085326\n    \n    \n      USDGBP\n      5033.0\n      0.000071\n      0.005986\n      -0.038140\n      -0.003299\n      -0.000016\n      0.003211\n      0.085019\n    \n    \n      USDJPY\n      5033.0\n      0.000043\n      0.005943\n      -0.041963\n      -0.003008\n      0.000115\n      0.003163\n      0.032901\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      BIS__WS_CBPOL_D_D__TH_diff\n      5032.0\n      -0.000248\n      0.033622\n      -1.000000\n      0.000000\n      0.000000\n      0.000000\n      0.500000\n    \n    \n      BIS__WS_CBPOL_D_D__TR_diff\n      5032.0\n      -0.005962\n      0.253605\n      -4.250000\n      0.000000\n      0.000000\n      0.000000\n      8.500000\n    \n    \n      BIS__WS_CBPOL_D_D__US_diff\n      5032.0\n      0.000075\n      0.037509\n      -1.000000\n      0.000000\n      0.000000\n      0.000000\n      0.750000\n    \n    \n      BIS__WS_CBPOL_D_D__XM_diff\n      5032.0\n      -0.000547\n      0.024505\n      -0.750000\n      0.000000\n      0.000000\n      0.000000\n      0.250000\n    \n    \n      BIS__WS_CBPOL_D_D__ZA_diff\n      5032.0\n      -0.001739\n      0.061530\n      -1.500000\n      0.000000\n      0.000000\n      0.000000\n      0.500000\n    \n  \n\n105 rows × 8 columns\n\n\n\nThe policy rates for some central banks have less observations than the others, as seen above.\nBecause some data are missing, we will impute data for the missing dates, by simply propagating the last valid observation, and when that is not possible, replacing the missing information with a “0”.\n\n\nCode\nX.fillna(method='pad', inplace=True)\nX.fillna(value=0, inplace=True)\n\n\nNow is a good time to start the model documentation. For this, we can use the standard model card that already comes with gingado.\nThe goal is to facilitate economists who want to make model documentation a part of their normal workflow.\n\n\nCode\nfrom gingado.model_documentation import ModelCard\n\n\n\n\nCode\nmodel_doc = ModelCard()\nmodel_doc.open_questions()\n\n\n['model_details__developer',\n 'model_details__version',\n 'model_details__type',\n 'model_details__info',\n 'model_details__paper',\n 'model_details__citation',\n 'model_details__license',\n 'model_details__contact',\n 'intended_use__primary_uses',\n 'intended_use__primary_users',\n 'intended_use__out_of_scope',\n 'factors__relevant',\n 'factors__evaluation',\n 'metrics__performance_measures',\n 'metrics__thresholds',\n 'metrics__variation_approaches',\n 'evaluation_data__datasets',\n 'evaluation_data__motivation',\n 'evaluation_data__preprocessing',\n 'training_data__training_data',\n 'quant_analyses__unitary',\n 'quant_analyses__intersectional',\n 'ethical_considerations__sensitive_data',\n 'ethical_considerations__human_life',\n 'ethical_considerations__mitigations',\n 'ethical_considerations__risks_and_harms',\n 'ethical_considerations__use_cases',\n 'ethical_considerations__additional_information',\n 'caveats_recommendations__caveats',\n 'caveats_recommendations__recommendations']\n\n\nAs an example, we can add the following information to the model:\n\n\nCode\nmodel_doc.fill_info({\n    'intended_use': {\n        'primary_uses': 'These models are simplified toy models made to illustrate the use of gingado',\n        'out_of_scope': 'These models were not constructed for decision-making and as such their use as predictors in real life decisions is strongly discouraged and out of scope.'\n    },\n    'metrics': {\n        'performance_measures': 'Consistent with most papers reviewed by Rossi (2013), these models were evaluated by their root mean squared error.'\n    },\n    'ethical_considerations': {\n        'sensitive_data': 'These models were not trained with sensitive data.',\n        'human_life': 'The models do not involve the collection or use of individual-level data, and have no foreseen impact on human life.'\n    },\n    \n})"
  },
  {
    "objectID": "forecast.html#lagging-the-regressors",
    "href": "forecast.html#lagging-the-regressors",
    "title": "Using gingado to forecast financial series",
    "section": "3 Lagging the regressors",
    "text": "3 Lagging the regressors\nThis model will not include any contemporaneous variable. Therefore, all regresors must be lagged.\nFor illustration purposes, we use 5 lags in this exercise.\n\n\nCode\nfrom gingado.utils import Lag\n\n\n\n\nCode\nn_lags = 5\n\nX_lagged = Lag(lags=n_lags).fit_transform(X)\nX_lagged\n\ny = FX_rate_changes[n_lags:]\n\n\nNow is a good opportunity to check by how much we have increased our regressor space:\n\n\nCode\npd.Series({\n    \"FX rates only\": y.shape[1],\n    \"... with augmentation_\": X.shape[1],\n    \"... lagged\": X_lagged.shape[1]\n})\n\n\nFX rates only               7\n... with augmentation_    105\n... lagged                525\ndtype: int64"
  },
  {
    "objectID": "forecast.html#training-the-models",
    "href": "forecast.html#training-the-models",
    "title": "Using gingado to forecast financial series",
    "section": "4 Training the models",
    "text": "4 Training the models\nOur dataset is now complete. Before using it to train the models, we hold out the most recent data to serve as our testing dataset, so we can compare our models with real out-of-sample information.\nWe can choose, say, 1st January 2022.\n\n\nCode\ncutoff = '2020-01-01'\n\nX_train, X_test = X_lagged[:cutoff], X_lagged[cutoff:]\ny_train, y_test = y[:cutoff], y[cutoff:]\n\n\n\n\nCode\nmodel_doc.fill_info({\n    'training_data': \n    {'training_data': \n        \"\"\"\n        The training data comprise time series obtained from official sources (BIS and ECB) on:\n        * foreign exchange rates\n        * central bank policy rates\n        * an estimated indicator for systemic stress\n        The training and evaluation datasets are the same time series, only different windows in time.\"\"\"\n    }\n})\n\n\nThe current status of the documentation is:\n\n\nCode\npd.Series(model_doc.show_json())\n\n\nmodel_details              {'developer': 'Person or organisation developi...\nintended_use               {'primary_uses': 'These models are simplified ...\nfactors                    {'relevant': 'Relevant factors', 'evaluation':...\nmetrics                    {'performance_measures': 'Consistent with most...\nevaluation_data            {'datasets': 'Datasets', 'motivation': 'Motiva...\ntraining_data              {'training_data': '\n        The training data ...\nquant_analyses             {'unitary': 'Unitary results', 'intersectional...\nethical_considerations     {'sensitive_data': 'These models were not trai...\ncaveats_recommendations    {'caveats': 'For example, did the results sugg...\ndtype: object\n\n\n\n4.1 Creating a random walk benchmark\nRossi (2013) highlights that few predictors beat the random walk without drift model. This is a good opportunity to showcase how we can use gingado’s in-built base class ggdBenchmark to build our customised benchmark model, in this case a random walk.\nThe calculation of the random walk benchmark is very simple. Still, creating a gingado benchmark offers some advantages: it is easier to compare alternative models, and the model documentation is done more seamlessly.\nA custom benchmark model must implement the following steps: * sub-class ggdBenchmark (or alternatively implement its methods) * define an estimator that is compatible with scikit-learn’s API: * at the very least, it has a fit method that returns self\nIf the user is relying on a custom estimator - like in this case, a random walk estimator to align with the literature - then this custom estimator also has some requirements: * it should ideally subclass scikit-learn’s BaseEstimator (mostly for the get_params / set_params methods) * three methods are necessary: * fit, which should at least create an attribute ending in an underline (“_“), so that gingado knows it is fitted * predict * score\n\n\nCode\nimport numpy as np\nfrom gingado.benchmark import ggdBenchmark\nfrom sklearn.base import BaseEstimator\nfrom sklearn.ensemble import VotingRegressor\nfrom sklearn.model_selection import TimeSeriesSplit\n\n\n\n\nCode\nclass RandomWalkEstimator(BaseEstimator):\n    def __init__(self, scoring='neg_root_mean_squared_error'):\n        self.scoring = scoring\n    \n    def fit(self, X, y=None):\n        self.n_samples_ = X.shape[0]\n        return self\n\n    def predict(self, X):\n        return np.zeros(X.shape[0])\n\n    def score(self, X, y, sample_weight=None):\n        from sklearn.metrics import mean_squared_error\n        y_pred = self.predict(X)\n        return mean_squared_error(y, y_pred, sample_weight=sample_weight, squared=False)\n\n    def forecast(self, forecast_horizon=1):\n        self.forecast_horizon = forecast_horizon\n        return np.zeros(self.forecast_horizon)\n\nclass RandomWalkBenchmark(ggdBenchmark):\n    def __init__(\n        self, \n        estimator=RandomWalkEstimator(), \n        auto_document=ModelCard,\n        cv=TimeSeriesSplit(n_splits=10, test_size=60), \n        ensemble_method=VotingRegressor, \n        verbose_grid=None):\n        self.estimator = estimator\n        self.auto_document = auto_document\n        self.cv = cv\n        self.ensemble_method = ensemble_method\n        self.verbose_grid = verbose_grid\n\n    def fit(self, X, y=None):\n        self.benchmark = self.estimator\n        self.benchmark.fit(X, y)\n        return self\n\n\n\n\n4.2 Training the candidate models\nNow that we have a benchmark, we can create candidate models that will try to beat it.\nIn this simplified example, we will choose only two: a random forest, an AdaBoost regressor and a Lasso model. Their hyperparameters are not particularly important for the example, but of course they could be fine-tuned as well.\nIn the language of Rossi (2013), the models below are one “single-equation, lagged fundamental model” for each currency.\n\n\nCode\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.linear_model import Lasso\n\n\n\n\nCode\nforest = RandomForestRegressor(n_estimators=250, max_features='log2').fit(X_train, y_train['USDBRL'])\nadaboost = AdaBoostRegressor(n_estimators=150).fit(X_train, y_train['USDBRL'])\nlasso = Lasso(alpha=0.1).fit(X_train, y_train['USDBRL'])\n\nrw = RandomWalkBenchmark().fit(X_train, y_train['USDBRL'])\n\n\nWe can now compare the model results, using the test dataset we held out previously.\nNote that we must pass the criterion against which we are comparing the forecasts.\n\n\nCode\nfrom sklearn.metrics import mean_squared_error\n\n\n\n\nCode\nresults = rw.compare_fitted_candidates(\n    X_test, y_test['USDBRL'],\n    candidates=[forest, adaboost, lasso],\n    scoring_func=mean_squared_error)\n\npd.Series(results)\n\n\nRandomWalkEstimator()                                           0.000130\nRandomForestRegressor(max_features='log2', n_estimators=250)    0.000131\nAdaBoostRegressor(n_estimators=150)                             0.000138\nLasso(alpha=0.1)                                                0.000130\ndtype: float64\n\n\nAs mentioned above, benchmarks can facilitate the model documentation. In addition to the broader documentation that is already ongoing, each benchmark object create their own where they store model information. We can use that for the broader documentation.\nIn our case, the only parameter we created above during fit is the number of samples: not a particularly informative variable but it was included just for illustration purposes. In any case, the parameter appears in the “model_details” section, item “info”, of the benchmark’s rw documentation. Similarly, the parameters of more fully-fledged estimators also appear in that section.\n\n\nCode\nrw.document()\n\nrw.model_documentation.show_json()['model_details']['info']\n\n\n{'n_samples_': 4394}\n\n\n\n\nCode\nmodel_doc.fill_info({\n    'model_details': {'info': rw.model_documentation.show_json()['model_details']['info']}\n})\n\n\n\n\nCode\nmodel_doc.show_json()\n\n\n{'model_details': {'developer': 'Person or organisation developing the model',\n  'datetime': '2022-06-28 08:35:12 ',\n  'version': 'Model version',\n  'type': 'Model type',\n  'info': {'n_samples_': 4394},\n  'paper': 'Paper or other resource for more information',\n  'citation': 'Citation details',\n  'license': 'License',\n  'contact': 'Where to send questions or comments about the model'},\n 'intended_use': {'primary_uses': 'These models are simplified toy models made to illustrate the use of gingado',\n  'primary_users': 'Primary intended users',\n  'out_of_scope': 'These models were not constructed for decision-making and as such their use as predictors in real life decisions is strongly discouraged and out of scope.'},\n 'factors': {'relevant': 'Relevant factors',\n  'evaluation': 'Evaluation factors'},\n 'metrics': {'performance_measures': 'Consistent with most papers reviewed by Rossi (2013), these models were evaluated by their root mean squared error.',\n  'thresholds': 'Decision thresholds',\n  'variation_approaches': 'Variation approaches'},\n 'evaluation_data': {'datasets': 'Datasets',\n  'motivation': 'Motivation',\n  'preprocessing': 'Preprocessing'},\n 'training_data': {'training_data': '\\n        The training data comprise time series obtained from official sources (BIS and ECB) on:\\n        * foreign exchange rates\\n        * central bank policy rates\\n        * an estimated indicator for systemic stress\\n        The training and evaluation datasets are the same time series, only different windows in time.'},\n 'quant_analyses': {'unitary': 'Unitary results',\n  'intersectional': 'Intersectional results'},\n 'ethical_considerations': {'sensitive_data': 'These models were not trained with sensitive data.',\n  'human_life': 'The models do not involve the collection or use of individual-level data, and have no foreseen impact on human life.',\n  'mitigations': 'What risk mitigation strategies were used during model development?',\n  'risks_and_harms': '\\n            What risks may be present in model usage? Try to identify the potential recipients, likelihood, and magnitude of harms.\\n            If these cannot be determined, note that they were consid- ered but remain unknown',\n  'use_cases': 'Are there any known model use cases that are especially fraught?',\n  'additional_information': '\\n            If possible, this section should also include any additional ethical considerations that went into model development,\\n            for example, review by an external board, or testing with a specific community.'},\n 'caveats_recommendations': {'caveats': 'For example, did the results suggest any further testing? Were there any relevant groups that were not represented in the evaluation dataset?',\n  'recommendations': 'Are there additional recommendations for model use? What are the ideal characteristics of an evaluation dataset for this model?'}}\n\n\nWe can save the documentation to disk in JSON format with model_doc.save_json(), or parse it to create other documents (eg, a PDF file) using third-party libraries."
  },
  {
    "objectID": "forecast.html#references",
    "href": "forecast.html#references",
    "title": "Using gingado to forecast financial series",
    "section": "5 References",
    "text": "5 References\nBank for International Settlements (2017): “Recent enhancements to the BIS statistics”, Quarterly Review, September. Available at: https://www.bis.org/publ/qtrpdf/r_qt1709c.htm\nHolló, D, M Kremer, M Lo Duca (2012): “CISS - A composite indicator of systemic stress in the financial system”, ECB Working Paper. Available at: https://www.ecb.europa.eu/pub/pdf/scpwps/ecbwp1426.pdf\nRossi, B (2013): “Exchange rate predictability”, Journal of Economic Literature, 51, 4, p.1063-1119."
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "Utils",
    "section": "",
    "text": "Important\n\n\n\nUp until v0.0.1-11, function get_username existed. However, it was removed from utils since it depended on pwd, which is only available to Unix-like systems. Therefore, this issue was preventing Windows users from importing gingado.utils. Since the function was not essential, it was removed until a suitable alternative that works in all major ystems can be found.\n\n\n\nsource\n\n\n\n get_datetime ()\n\nReturns the time now\n\n\nCode\nd = get_datetime()\nassert isinstance(d, str)\nassert len(d) > 0"
  },
  {
    "objectID": "utils.html#support-for-time-series",
    "href": "utils.html#support-for-time-series",
    "title": "Utils",
    "section": "2 Support for time series",
    "text": "2 Support for time series\nObjects of the class Lag are similar to scikit-learn’s transformers.\n/Users/douglasaraujo/Coding/.venv_gingado/lib/python3.10/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Notes\n  else: warn(msg)\n\nsource\n\n2.1 Lag\n\n Lag (lags=1, jump=0, keep_contemporaneous_X=False)\n\nBase class for all estimators in scikit-learn.\nThe code below demonstrates how Lag works in practice. Note in particular that, because Lag is a transformer, it can be used as part of a scikit-learn’s Pipeline.\n\n\nCode\nimport numpy as np\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nrandomX = np.random.rand(15, 2)\nrandomY = np.random.rand(15)\n\nlags = 3\njump = 2\n\npipe = Pipeline([\n    ('scaler', StandardScaler()),\n    ('lagger', Lag(lags=lags, jump=jump, keep_contemporaneous_X=False))\n]).fit_transform(randomX, randomY)\n\n\nBelow we confirm that the lagger removes the correct number of rows corresponding to the lagged observations:\n\n\nCode\nassert randomX.shape[0] - lags - jump == pipe.shape[0]\n\n\nAnd because Lag is a transformer, its parameters (lags and jump) can be calibrated using hyperparameter tuning to achieve the best performance for a model."
  },
  {
    "objectID": "utils.html#support-for-data-augmentation-with-sdmx",
    "href": "utils.html#support-for-data-augmentation-with-sdmx",
    "title": "Utils",
    "section": "3 Support for data augmentation with SDMX",
    "text": "3 Support for data augmentation with SDMX\n\n\n\n\n\n\nNote\n\n\n\nplease note that working with SDMX may take some minutes depending on the amount of information you are downloading.\n\n\n\nsource\n\n3.1 list_SDMX_sources\n\n list_SDMX_sources ()\n\nReturns the list of codes representing the SDMX sources available for data download\n\n\nCode\nsources = list_SDMX_sources()\nprint(sources)\n\nassert len(sources) > 0\n# all elements are of type 'str'\nassert sum([isinstance(src, str) for src in sources]) == len(sources)\n\n\n['ABS', 'ABS_XML', 'BBK', 'BIS', 'CD2030', 'ECB', 'ESTAT', 'ILO', 'IMF', 'INEGI', 'INSEE', 'ISTAT', 'LSD', 'NB', 'NBB', 'OECD', 'SGR', 'SPC', 'STAT_EE', 'UNICEF', 'UNSD', 'WB', 'WB_WDI']\n\n\n\nsource\n\n\n3.2 list_all_dataflows\n\n list_all_dataflows (codes_only=False, return_pandas=True)\n\nReturns a dictionary listing all available dataflows for all sources. When using as a parameter to an AugmentSDMX object or to the load_SDMX_data function, set codes_only=True\n\n\nCode\ndflows = list_all_dataflows(return_pandas=False)\n\nassert isinstance(dflows, dict)\nall_sources = list_SDMX_sources()\nassert len([s for s in dflows.keys() if s in all_sources]) == len(dflows.keys())\n\n\n2022-06-23 03:11:44,954 pandasdmx.reader.sdmxml - DEBUG: Truncate sub-microsecond time in <Prepared>\n2022-06-23 03:11:56,586 pandasdmx.reader.sdmxml - DEBUG: Truncate sub-microsecond time in <Prepared>\n2022-06-23 03:12:00,619 pandasdmx.reader.sdmxml - DEBUG: Truncate sub-microsecond time in <Prepared>\n2022-06-23 03:12:01,299 pandasdmx.reader.sdmxml - DEBUG: Truncate sub-microsecond time in <Prepared>\n2022-06-23 03:12:04,337 pandasdmx.reader.sdmxml - DEBUG: Truncate sub-microsecond time in <Prepared>\n\n\nlist_all_dataflows returns by default a pandas Series, facilitating data discovery by users like so:\n\n\nCode\ndflows = list_all_dataflows(return_pandas=True)\nassert type(dflows) == pd.core.series.Series\n\ndflows\n\n\n2022-06-23 03:15:55,933 pandasdmx.reader.sdmxml - DEBUG: Truncate sub-microsecond time in <Prepared>\n2022-06-23 03:16:09,138 pandasdmx.reader.sdmxml - DEBUG: Truncate sub-microsecond time in <Prepared>\n2022-06-23 03:16:13,734 pandasdmx.reader.sdmxml - DEBUG: Truncate sub-microsecond time in <Prepared>\n2022-06-23 03:16:14,414 pandasdmx.reader.sdmxml - DEBUG: Truncate sub-microsecond time in <Prepared>\n2022-06-23 03:16:17,489 pandasdmx.reader.sdmxml - DEBUG: Truncate sub-microsecond time in <Prepared>\n2022-06-23 03:16:18,114 pandasdmx.reader.sdmxml - DEBUG: Truncate sub-microsecond time in <Prepared>\n\n\nThis format allows for more easily searching dflows by source:\n\n\nCode\nlist_all_dataflows(codes_only=True, return_pandas=True)\n\n\n2022-06-23 03:29:33,379 pandasdmx.reader.sdmxml - DEBUG: Truncate sub-microsecond time in <Prepared>\n2022-06-23 03:29:43,859 pandasdmx.reader.sdmxml - DEBUG: Truncate sub-microsecond time in <Prepared>\n2022-06-23 03:29:48,210 pandasdmx.reader.sdmxml - DEBUG: Truncate sub-microsecond time in <Prepared>\n2022-06-23 03:29:48,889 pandasdmx.reader.sdmxml - DEBUG: Truncate sub-microsecond time in <Prepared>\n2022-06-23 03:29:52,014 pandasdmx.reader.sdmxml - DEBUG: Truncate sub-microsecond time in <Prepared>\n2022-06-23 03:29:52,646 pandasdmx.reader.sdmxml - DEBUG: Truncate sub-microsecond time in <Prepared>\n\n\nABS_XML  0                 ABORIGINAL_POP_PROJ\n         1          ABORIGINAL_POP_PROJ_REMOTE\n         2    ABS_ABORIGINAL_POPPROJ_INDREGION\n         3                   ABS_ACLD_LFSTATUS\n         4                     ABS_ACLD_TENURE\n                            ...               \nUNSD     5                     DF_UNData_UNFCC\nWB       0               DF_WITS_Tariff_TRAINS\n         1      DF_WITS_TradeStats_Development\n         2           DF_WITS_TradeStats_Tariff\n         3            DF_WITS_TradeStats_Trade\nName: dataflow, Length: 9114, dtype: object\n\n\n\n\nCode\ndflows['BIS']\n\n\nWS_CBPOL_D                            Policy rates daily\nWS_CBPOL_M                          Policy rates monthly\nWS_CBS_PUB                      BIS consolidated banking\nWS_CREDIT_GAP                     BIS credit-to-GDP gaps\nWS_DEBT_SEC2_PUB                     BIS debt securities\nWS_DER_OTC_TOV                  OTC derivatives turnover\nWS_DSR                            BIS debt service ratio\nWS_EER_D              BIS effective exchange rates daily\nWS_EER_M            BIS effective exchange rates monthly\nWS_GLI                       Global liquidity indicators\nWS_LBS_D_PUB                      BIS locational banking\nWS_LONG_CPI                     BIS long consumer prices\nWS_OTC_DERIV2                OTC derivatives outstanding\nWS_SPP              BIS property prices: selected series\nWS_TC                    BIS long series on total credit\nWS_XRU                   US dollar exchange rates, m,q,a\nWS_XRU_D                 US dollar exchange rates, daily\nWS_XTD_DERIV                 Exchange traded derivatives\nName: dataflow, dtype: object\n\n\nOr the user can search dataflows by their human-readable name instead of their code. For example, this is one way to see if any dataflow has information on interest rates:\n\n\nCode\ndflows[dflows.str.contains('Interest rates', case=False)]\n\n\nECB    RIR                                         Retail Interest Rates\nESTAT  cpc_ecexint     Candidate countries and potential candidates: ...\n       ei_mfir_m                           Interest rates - monthly data\n       enpe_irt_st                           Money market interest rates\n       enpr_ecexint     ENP countries: exchange rates and interest rates\n       irt_st_a                Money market interest rates - annual data\n       irt_st_m               Money market interest rates - monthly data\n       irt_st_q             Money market interest rates - quarterly data\n       tec00034        Short-term interest rates: Day-to-day money rates\n       tec00035        Short-term interest rates: three-month interba...\n       teimf100                   Day-to-day money market interest rates\nIMF    6SR             M&B: Interest Rates and Share Prices (6SR) for...\n       INR                                                Interest rates\n       INR_NSTD                              Interest rates_Non-Standard\nNB     IR                                                 Interest rates\nName: dataflow, dtype: object\n\n\nThe function load_SDMX_data is a convenience function that downloads data from SDMX sources (and any specific dataflows passed as arguments) if they match the key and parameters set by the user.\n\nsource\n\n\n3.3 load_SDMX_data\n\n load_SDMX_data (sources, keys, params, verbose=True)\n\nLoads datasets from SDMX.\n\n\nCode\ndf = load_SDMX_data(sources={'ECB': 'CISS', 'BIS': 'WS_CBPOL_D'}, keys={'FREQ': 'D'}, params={'startPeriod': 2003})\n\nassert type(df) == pd.DataFrame\nassert df.shape[0] > 0\nassert df.shape[1] > 0\n\n\nQuerying data from ECB's dataflow 'CISS' - Composite Indicator of Systemic Stress...\n\n\n2022-06-01 01:43:59,553 pandasdmx.reader.sdmxml - INFO: Use supplied dsd=… argument for non–structure-specific message\n\n\nQuerying data from BIS's dataflow 'WS_CBPOL_D' - Policy rates daily...\n\n\n\n\n3.4 To be deprecated\nThe function load_EURFX_data is a helper function to download a test dataset containing real life data. This dataset was chosen due to the assumption that most users have at least an intuitive understanding of what a foreign exchange is: the price of changing one currency for the other. This example dataset does not imply this data is more or less relevant than others; it is used only for pedagogical purposes. :::{.callout-note}\nThis function will be deprecated in gingado version 0.0.2.\n:::\n\nsource\n\n\n3.5 load_EURFX_data\n\n load_EURFX_data (startYear=2003, lags=1, jump=0,\n                  keep_contemporaneous_X=True)\n\nLoads a real-life dataset for testing use cases.\n\n\nCode\nEUR_FX = load_EURFX_data()\n\nassert type(EUR_FX) == pd.DataFrame\nassert EUR_FX.shape[0] > 0\nassert EUR_FX.shape[1] > 0\n\nEUR_FX\n\n\n/var/folders/b9/p8z57lqd55xfk68xz34dg0s40000gn/T/ipykernel_32613/429544813.py:1: DeprecationWarning: Function 'load_EURFX_data' will no longer be present after gingado v0.0.2. Use 'load_SDMX_data(source={'ECB': 'EXR'}) instead.\n  EUR_FX = load_EURFX_data()\n\n\n\n\n\n\n  \n    \n      \n      AUD\n      BRL\n      CAD\n      CHF\n      GBP\n      JPY\n      SGD\n      USD\n      AUD_lag_1\n      BRL_lag_1\n      CAD_lag_1\n      CHF_lag_1\n      GBP_lag_1\n      JPY_lag_1\n      SGD_lag_1\n      USD_lag_1\n    \n    \n      TIME_PERIOD\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2003-01-03\n      1.8440\n      3.6112\n      1.6264\n      1.4555\n      0.65000\n      124.56\n      1.8132\n      1.0392\n      1.8554\n      3.6770\n      1.6422\n      1.4528\n      0.65200\n      124.40\n      1.8188\n      1.0446\n    \n    \n      2003-01-06\n      1.8281\n      3.5145\n      1.6383\n      1.4563\n      0.64950\n      124.40\n      1.8210\n      1.0488\n      1.8440\n      3.6112\n      1.6264\n      1.4555\n      0.65000\n      124.56\n      1.8132\n      1.0392\n    \n    \n      2003-01-07\n      1.8160\n      3.5139\n      1.6257\n      1.4565\n      0.64960\n      124.82\n      1.8155\n      1.0425\n      1.8281\n      3.5145\n      1.6383\n      1.4563\n      0.64950\n      124.40\n      1.8210\n      1.0488\n    \n    \n      2003-01-08\n      1.8132\n      3.4405\n      1.6231\n      1.4586\n      0.64950\n      124.90\n      1.8102\n      1.0377\n      1.8160\n      3.5139\n      1.6257\n      1.4565\n      0.64960\n      124.82\n      1.8155\n      1.0425\n    \n    \n      2003-01-09\n      1.8172\n      3.4915\n      1.6371\n      1.4597\n      0.65300\n      125.16\n      1.8244\n      1.0507\n      1.8132\n      3.4405\n      1.6231\n      1.4586\n      0.64950\n      124.90\n      1.8102\n      1.0377\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2022-05-25\n      1.5126\n      5.1736\n      1.3720\n      1.0269\n      0.85295\n      135.34\n      1.4676\n      1.0656\n      1.5152\n      5.1793\n      1.3714\n      1.0334\n      0.85750\n      136.49\n      1.4722\n      1.0720\n    \n    \n      2022-05-26\n      1.5110\n      5.1741\n      1.3715\n      1.0283\n      0.85073\n      135.95\n      1.4709\n      1.0697\n      1.5126\n      5.1736\n      1.3720\n      1.0269\n      0.85295\n      135.34\n      1.4676\n      1.0656\n    \n    \n      2022-05-27\n      1.4995\n      5.0959\n      1.3661\n      1.0258\n      0.84875\n      136.05\n      1.4679\n      1.0722\n      1.5110\n      5.1741\n      1.3715\n      1.0283\n      0.85073\n      135.95\n      1.4709\n      1.0697\n    \n    \n      2022-05-30\n      1.4982\n      5.0629\n      1.3647\n      1.0327\n      0.85150\n      137.25\n      1.4719\n      1.0764\n      1.4995\n      5.0959\n      1.3661\n      1.0258\n      0.84875\n      136.05\n      1.4679\n      1.0722\n    \n    \n      2022-05-31\n      1.4933\n      5.0965\n      1.3573\n      1.0281\n      0.85138\n      137.36\n      1.4687\n      1.0713\n      1.4982\n      5.0629\n      1.3647\n      1.0327\n      0.85150\n      137.25\n      1.4719\n      1.0764\n    \n  \n\n4971 rows × 16 columns"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to gingado!",
    "section": "",
    "text": "gingado seeks to facilitate the use of machine learning in economic and finance use cases, while promoting good practices. This package aims to be suitable for beginners and advanced users alike. Use cases may range from simple data retrievals to experimentation with machine learning algorithms to more complex model pipelines used in production."
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Welcome to gingado!",
    "section": "Overview",
    "text": "Overview\ngingado is a free, open source library built different functionalities:\n\ndata augmentation, to add more data from official sources, improving the machine models being trained by the user;\nautomatic benchmark model, to enable the user to assess their models against a reasonably well-performant model;\n(new!) relevant datasets, both real and simulamed, to allow for easier model development and comparison;\nsupport for model documentation, to embed documentation and ethical considerations in the model development phase; and\nutils, including tools to allow for lagging variables in a straightforward way.\n\nEach of these functionalities builds on top of the previous one. They can be used on a stand-alone basis, together, or even as part of a larger pipeline from data input to model training to documentation!\n\n\n\n\n\n\nTip\n\n\n\nNew functionalities are planned over time, so consider checking frequently on gingado for the latest toolsets."
  },
  {
    "objectID": "index.html#design-principles",
    "href": "index.html#design-principles",
    "title": "Welcome to gingado!",
    "section": "Design principles",
    "text": "Design principles\nThe choices made during development of gingado derive from the following principles, in no particular order:\n\nflexibility: users can use gingado out of the box or build custom processes on top of it;\ncompatibility: gingado works well with other widely used libraries in machine learning, such as scikit-learn and pandas; and\nresponsibility: gingado facilitates and promotes model documentation, including ethical considerations, as part of the machine learning development workflow."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Welcome to gingado!",
    "section": "Acknowledgements",
    "text": "Acknowledgements\ngingado’s API is inspired on the following libraries:\n\nscikit-learn (API description)\nkeras (website here and also, this essay)\nfastai (description here)\n\nIn addition, gingado is developed and maintained using nbdev."
  },
  {
    "objectID": "index.html#presentations-talks-papers",
    "href": "index.html#presentations-talks-papers",
    "title": "Welcome to gingado!",
    "section": "Presentations, talks, papers",
    "text": "Presentations, talks, papers\nThe most current version of the paper describing gingado is here. The paper and other material about gingado (ie, slide decks, papers) in this dedicated repository. Interested users are welcome to visit the repository and comment on the drafts or slide decks, preferably by opening an issue. I also store in this repository suggestions I receive as issues, so users can see what others commented (anonymously unless requested) and comment along as well!"
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "Welcome to gingado!",
    "section": "Install",
    "text": "Install\nTo install gingado, simply run the following code on the terminal:\n$ pip install gingado"
  },
  {
    "objectID": "documentation.html",
    "href": "documentation.html",
    "title": "Model documentation",
    "section": "",
    "text": "Each user has a specific documentation need, ranging from simply logging the model training to a more complex description of the model pipeline with a discusson of the model outcomes. gingado addresses this variety of needs by offering a class of objects, “Documenters”, that facilitate model documentation. A base class facilitates the creation of generic ways to document models, and gingado includes off-the-shelf one specific model documentation template as described below.\nThe model documentation is performed by Documenters, objects that subclass from the base class ggdModelDocumentation. This base class offers code that can be used by any Documenter to read the model in question, format the information according to a template and save the resulting documentation in a JSON format. Documenters save the underlying information using the JSON format. With the JSON documentation file at hand, the user can then use existing third-party libraries to transform the information stored in JSON into a variety of formats (eg, HTML, PDF) as needed.\nOne current area of development is the automatic filing of some fields related to the model. The objective is to automatise documentation of the information that can be fetched automatically from the model, leaving time for the analyst to concentrate on other tasks, such as considering the ethical implications of the machine learning model being trained.\nsource"
  },
  {
    "objectID": "documentation.html#modelcard",
    "href": "documentation.html#modelcard",
    "title": "Model documentation",
    "section": "1 ModelCard",
    "text": "1 ModelCard\nModelCard - the model documentation template inspired by the work of Mitchell et al, 2019 already comes with gingado. Its template can be used by users as is, or tweaked according to each need. The ModelCard template can also serve as inspiration for any custom documentation needs. Users with documentation needs beyond the out-of-the-box solutions provided by gingado can create their own class of Documenters (more information on that below), and compatibility with these custom documentation routines with the rest of the code is ensured. Users are encouraged to submit a pull request with their own documentation models subclassing ggdModelDocumentation if these custom templates can also benefit other users.\n\nsource\n\n1.1 ModelCard\n\n ModelCard (file_path='', autofill=True, indent_level=2)\n\nBase class for gingado Documenters"
  },
  {
    "objectID": "documentation.html#basic-functioning-of-model-documentation",
    "href": "documentation.html#basic-functioning-of-model-documentation",
    "title": "Model documentation",
    "section": "2 Basic functioning of model documentation",
    "text": "2 Basic functioning of model documentation\nAfter a Documenter object, such as ModelCard is instanciated, the user can see the underlying template with the module show_template, as below:\n\n\nCode\n#collapse_output\nmodel_doc = ModelCard(autofill=False)\nassert model_doc.show_template(indent=False) == ModelCard.template\n\nmodel_doc.show_template()\n\n\n{\n  \"model_details\": {\n    \"field_description\": \"Basic information about the model\",\n    \"developer\": \"Person or organisation developing the model\",\n    \"datetime\": \"Model date\",\n    \"version\": \"Model version\",\n    \"type\": \"Model type\",\n    \"info\": \"Information about training algorithms, parameters, fairness constraints or other applied approaches, and features\",\n    \"paper\": \"Paper or other resource for more information\",\n    \"citation\": \"Citation details\",\n    \"license\": \"License\",\n    \"contact\": \"Where to send questions or comments about the model\"\n  },\n  \"intended_use\": {\n    \"field_description\": \"Use cases that were envisioned during development\",\n    \"primary_uses\": \"Primary intended uses\",\n    \"primary_users\": \"Primary intended users\",\n    \"out_of_scope\": \"Out-of-scope use cases\"\n  },\n  \"factors\": {\n    \"field_description\": \"Factors could include demographic or phenotypic groups, environmental conditions, technical attributes, or others\",\n    \"relevant\": \"Relevant factors\",\n    \"evaluation\": \"Evaluation factors\"\n  },\n  \"metrics\": {\n    \"field_description\": \"Metrics should be chosen to reflect potential real world impacts of the model\",\n    \"performance_measures\": \"Model performance measures\",\n    \"thresholds\": \"Decision thresholds\",\n    \"variation_approaches\": \"Variation approaches\"\n  },\n  \"evaluation_data\": {\n    \"field_description\": \"Details on the dataset(s) used for the quantitative analyses in the documentation\",\n    \"datasets\": \"Datasets\",\n    \"motivation\": \"Motivation\",\n    \"preprocessing\": \"Preprocessing\"\n  },\n  \"training_data\": {\n    \"field_description\": \"\\n            May not be possible to provide in practice. When possible, this section should mirror 'Evaluation Data'. \\n            If such detail is not possible, minimal allowable information should be provided here, \\n            such as details of the distribution over various factors in the training datasets.\",\n    \"training_data\": \"Information on training data\"\n  },\n  \"quant_analyses\": {\n    \"field_description\": \"Quantitative Analyses\",\n    \"unitary\": \"Unitary results\",\n    \"intersectional\": \"Intersectional results\"\n  },\n  \"ethical_considerations\": {\n    \"field_description\": \"\\n            Ethical considerations that went into model development, surfacing ethical challenges and \\n            solutions to stakeholders. Ethical analysis does not always lead to precise solutions, but the process \\n            of ethical contemplation is worthwhile to inform on responsible practices and next steps in future work.\",\n    \"sensitive_data\": \"Does the model use any sensitive data (e.g., protected classes)?\",\n    \"human_life\": \"Is the model intended to inform decisions about mat- ters central to human life or flourishing - e.g., health or safety? Or could it be used in such a way?\",\n    \"mitigations\": \"What risk mitigation strategies were used during model development?\",\n    \"risks_and_harms\": \"\\n            What risks may be present in model usage? Try to identify the potential recipients, likelihood, and magnitude of harms. \\n            If these cannot be determined, note that they were consid- ered but remain unknown\",\n    \"use_cases\": \"Are there any known model use cases that are especially fraught?\",\n    \"additional_information\": \"\\n            If possible, this section should also include any additional ethical considerations that went into model development, \\n            for example, review by an external board, or testing with a specific community.\"\n  },\n  \"caveats_recommendations\": {\n    \"field_description\": \"Additional concerns that were not covered in the previous sections\",\n    \"caveats\": \"For example, did the results suggest any further testing? Were there any relevant groups that were not represented in the evaluation dataset?\",\n    \"recommendations\": \"Are there additional recommendations for model use? What are the ideal characteristics of an evaluation dataset for this model?\"\n  }\n}\n\n\nThe method show_json prints the Documenter’s documentation template, where the unfilled information retains the descriptions from the original template:\n\n\nCode\n#collapse_output\nmodel_doc = ModelCard(autofill=True)\nmodel_doc.show_json()\n\n\n{'model_details': {'developer': 'douglasaraujo',\n  'datetime': '2022-06-06 01:45:36 ',\n  'version': 'Model version',\n  'type': 'Model type',\n  'info': 'Information about training algorithms, parameters, fairness constraints or other applied approaches, and features',\n  'paper': 'Paper or other resource for more information',\n  'citation': 'Citation details',\n  'license': 'License',\n  'contact': 'Where to send questions or comments about the model'},\n 'intended_use': {'primary_uses': 'Primary intended uses',\n  'primary_users': 'Primary intended users',\n  'out_of_scope': 'Out-of-scope use cases'},\n 'factors': {'relevant': 'Relevant factors',\n  'evaluation': 'Evaluation factors'},\n 'metrics': {'performance_measures': 'Model performance measures',\n  'thresholds': 'Decision thresholds',\n  'variation_approaches': 'Variation approaches'},\n 'evaluation_data': {'datasets': 'Datasets',\n  'motivation': 'Motivation',\n  'preprocessing': 'Preprocessing'},\n 'training_data': {'training_data': 'Information on training data'},\n 'quant_analyses': {'unitary': 'Unitary results',\n  'intersectional': 'Intersectional results'},\n 'ethical_considerations': {'sensitive_data': 'Does the model use any sensitive data (e.g., protected classes)?',\n  'human_life': 'Is the model intended to inform decisions about mat- ters central to human life or flourishing - e.g., health or safety? Or could it be used in such a way?',\n  'mitigations': 'What risk mitigation strategies were used during model development?',\n  'risks_and_harms': '\\n            What risks may be present in model usage? Try to identify the potential recipients, likelihood, and magnitude of harms. \\n            If these cannot be determined, note that they were consid- ered but remain unknown',\n  'use_cases': 'Are there any known model use cases that are especially fraught?',\n  'additional_information': '\\n            If possible, this section should also include any additional ethical considerations that went into model development, \\n            for example, review by an external board, or testing with a specific community.'},\n 'caveats_recommendations': {'caveats': 'For example, did the results suggest any further testing? Were there any relevant groups that were not represented in the evaluation dataset?',\n  'recommendations': 'Are there additional recommendations for model use? What are the ideal characteristics of an evaluation dataset for this model?'}}\n\n\nThe template is protected from editing once a Documenter has been created. This way, even if a user unwarrantedly changes the template, this does not interfere with the Documenter functionality.\n\n\nCode\n#collapse_output\nmodel_doc.template = None\nmodel_doc.show_template()\n\nassert model_doc.show_template(indent=False) == ModelCard.template\n\n\n{\n  \"model_details\": {\n    \"field_description\": \"Basic information about the model\",\n    \"developer\": \"Person or organisation developing the model\",\n    \"datetime\": \"Model date\",\n    \"version\": \"Model version\",\n    \"type\": \"Model type\",\n    \"info\": \"Information about training algorithms, parameters, fairness constraints or other applied approaches, and features\",\n    \"paper\": \"Paper or other resource for more information\",\n    \"citation\": \"Citation details\",\n    \"license\": \"License\",\n    \"contact\": \"Where to send questions or comments about the model\"\n  },\n  \"intended_use\": {\n    \"field_description\": \"Use cases that were envisioned during development\",\n    \"primary_uses\": \"Primary intended uses\",\n    \"primary_users\": \"Primary intended users\",\n    \"out_of_scope\": \"Out-of-scope use cases\"\n  },\n  \"factors\": {\n    \"field_description\": \"Factors could include demographic or phenotypic groups, environmental conditions, technical attributes, or others\",\n    \"relevant\": \"Relevant factors\",\n    \"evaluation\": \"Evaluation factors\"\n  },\n  \"metrics\": {\n    \"field_description\": \"Metrics should be chosen to reflect potential real world impacts of the model\",\n    \"performance_measures\": \"Model performance measures\",\n    \"thresholds\": \"Decision thresholds\",\n    \"variation_approaches\": \"Variation approaches\"\n  },\n  \"evaluation_data\": {\n    \"field_description\": \"Details on the dataset(s) used for the quantitative analyses in the documentation\",\n    \"datasets\": \"Datasets\",\n    \"motivation\": \"Motivation\",\n    \"preprocessing\": \"Preprocessing\"\n  },\n  \"training_data\": {\n    \"field_description\": \"\\n            May not be possible to provide in practice. When possible, this section should mirror 'Evaluation Data'. \\n            If such detail is not possible, minimal allowable information should be provided here, \\n            such as details of the distribution over various factors in the training datasets.\",\n    \"training_data\": \"Information on training data\"\n  },\n  \"quant_analyses\": {\n    \"field_description\": \"Quantitative Analyses\",\n    \"unitary\": \"Unitary results\",\n    \"intersectional\": \"Intersectional results\"\n  },\n  \"ethical_considerations\": {\n    \"field_description\": \"\\n            Ethical considerations that went into model development, surfacing ethical challenges and \\n            solutions to stakeholders. Ethical analysis does not always lead to precise solutions, but the process \\n            of ethical contemplation is worthwhile to inform on responsible practices and next steps in future work.\",\n    \"sensitive_data\": \"Does the model use any sensitive data (e.g., protected classes)?\",\n    \"human_life\": \"Is the model intended to inform decisions about mat- ters central to human life or flourishing - e.g., health or safety? Or could it be used in such a way?\",\n    \"mitigations\": \"What risk mitigation strategies were used during model development?\",\n    \"risks_and_harms\": \"\\n            What risks may be present in model usage? Try to identify the potential recipients, likelihood, and magnitude of harms. \\n            If these cannot be determined, note that they were consid- ered but remain unknown\",\n    \"use_cases\": \"Are there any known model use cases that are especially fraught?\",\n    \"additional_information\": \"\\n            If possible, this section should also include any additional ethical considerations that went into model development, \\n            for example, review by an external board, or testing with a specific community.\"\n  },\n  \"caveats_recommendations\": {\n    \"field_description\": \"Additional concerns that were not covered in the previous sections\",\n    \"caveats\": \"For example, did the results suggest any further testing? Were there any relevant groups that were not represented in the evaluation dataset?\",\n    \"recommendations\": \"Are there additional recommendations for model use? What are the ideal characteristics of an evaluation dataset for this model?\"\n  }\n}\n\n\nUsers can find which fields in their templates are still without response by using the module open_questions. The levels of the template are reflected in the resulting dictionary, with double underscores separating the different dictionary levels in the underlying template.\nBelow we see that after inputting information for the item caveats in the section caveats_recommendations, this item does not appear in the results of the open_questions method.\n\n\nCode\nmodel_doc.fill_info({'caveats_recommendations': {'caveats': 'This is another test'}})\nassert model_doc.json_doc['caveats_recommendations']['caveats'] == \"This is another test\"\n\n# note that caveats_recommendations__caveats is no longer considered an open question\n# after being filled in through `fill_info`.\nprint([oq for oq in model_doc.open_questions() if oq.startswith('caveats')])\n\n\n['caveats_recommendations__recommendations']\n\n\nAnd now the complete result of the open_questions method:\n\n\nCode\n#collapse_output\nmodel_doc.open_questions()\n\n\n['model_details__version',\n 'model_details__type',\n 'model_details__info',\n 'model_details__paper',\n 'model_details__citation',\n 'model_details__license',\n 'model_details__contact',\n 'intended_use__primary_uses',\n 'intended_use__primary_users',\n 'intended_use__out_of_scope',\n 'factors__relevant',\n 'factors__evaluation',\n 'metrics__performance_measures',\n 'metrics__thresholds',\n 'metrics__variation_approaches',\n 'evaluation_data__datasets',\n 'evaluation_data__motivation',\n 'evaluation_data__preprocessing',\n 'training_data__training_data',\n 'quant_analyses__unitary',\n 'quant_analyses__intersectional',\n 'ethical_considerations__sensitive_data',\n 'ethical_considerations__human_life',\n 'ethical_considerations__mitigations',\n 'ethical_considerations__risks_and_harms',\n 'ethical_considerations__use_cases',\n 'ethical_considerations__additional_information',\n 'caveats_recommendations__recommendations']\n\n\nIf the user wants to fill in an empty field such as the ones identified above by the method open_questions, the user simply needs to pass to the module fill_info a dictionary with the corresponding information. Depending on the template, the dictionary may be nested.\n\n\n\n\n\n\nNote\n\n\n\nit is technically possible to attribute the element directly to the attribute json_doc, but this should be avoided in favour of using the method fill_info. The latter tests whether the new information is valid according to the documentation template and also enables the filling of more than one question at the same time. In addition, attributing information directly to json_doc is not logged, and may unwarrantedly create new entries that are not part of the template (eg, if a new dictionary key is created due to typos).\n\n\nThe template serves to provide specific instances of the Documenter object with a form-like structure, indicating which fields are open and thus require some answers or information. Consequently, the template does not change when the actual document object changes after information is added by fill_info.\n\n\nCode\nnew_info = {\n    'metrics': {'performance_measures': \"This is a test\"},\n    'caveats_recommendations': {'caveats': \"This is another test\"}\n    }\n\nmodel_doc.fill_info(new_info)\nprint([model_doc.json_doc['metrics'], ModelCard.template['metrics']])\n\nassert model_doc.show_template(indent=False) == ModelCard.template\n\n\n[{'performance_measures': 'This is a test', 'thresholds': 'Decision thresholds', 'variation_approaches': 'Variation approaches'}, {'field_description': 'Metrics should be chosen to reflect potential real world impacts of the model', 'performance_measures': 'Model performance measures', 'thresholds': 'Decision thresholds', 'variation_approaches': 'Variation approaches'}]"
  },
  {
    "objectID": "documentation.html#reading-information-from-models",
    "href": "documentation.html#reading-information-from-models",
    "title": "Model documentation",
    "section": "3 Reading information from models",
    "text": "3 Reading information from models\ngingado’s ggdModelDocumentation base class is able to extract information from machine learning models from a number of widely used libraries and make it available to the Documenter objects. This is done through the method read_model, which recognises whether the model is a gingado object or any of scikit-learn, keras, or fastai models and read the model characteristics appropriately. For filing out information from other models (eg, pytorch or even models coded from scratch, machine learning or not), the user can benefit from the module fill_model_info that every Documenter should have, as demonstrated below.\nIn the case of ModelCard, these informations are included under model_details, item info. But the model information could be saved in another area of a custom Documenter.\n\n\n\n\n\n\nNote\n\n\n\nthe model-specific information saved is different depending on the model’s original library.\n\n\n\n3.1 Preliminaries\nThe mock dataset below is used to construct models using different libraries, to demonstrate how they are read by Documenters.\n\n\nCode\nfrom sklearn.datasets import make_classification\n\n# some mock up data\nX, y = make_classification()\n\nX.shape, y.shape\n\n\n((100, 20), (100,))\n\n\n\n\n3.2 gingado Benchmark\n\n\nCode\nfrom gingado.benchmark import ClassificationBenchmark\n\n# the gingado benchmark\ngingado_clf = ClassificationBenchmark(verbose_grid=3).fit(X, y)\n\n\nFitting 5 folds for each of 6 candidates, totalling 30 fits\n[CV 1/5] END max_features=sqrt, n_estimators=100;, score=0.900 total time=   0.2s\n[CV 2/5] END max_features=sqrt, n_estimators=100;, score=0.850 total time=   0.1s\n[CV 3/5] END max_features=sqrt, n_estimators=100;, score=0.900 total time=   0.1s\n[CV 4/5] END max_features=sqrt, n_estimators=100;, score=1.000 total time=   0.1s\n[CV 5/5] END max_features=sqrt, n_estimators=100;, score=0.900 total time=   0.1s\n[CV 1/5] END max_features=sqrt, n_estimators=250;, score=0.900 total time=   0.3s\n[CV 2/5] END max_features=sqrt, n_estimators=250;, score=0.850 total time=   0.3s\n[CV 3/5] END max_features=sqrt, n_estimators=250;, score=0.950 total time=   0.3s\n[CV 4/5] END max_features=sqrt, n_estimators=250;, score=1.000 total time=   0.3s\n[CV 5/5] END max_features=sqrt, n_estimators=250;, score=0.900 total time=   0.3s\n[CV 1/5] END max_features=log2, n_estimators=100;, score=0.900 total time=   0.1s\n[CV 2/5] END max_features=log2, n_estimators=100;, score=0.850 total time=   0.1s\n[CV 3/5] END max_features=log2, n_estimators=100;, score=0.950 total time=   0.1s\n[CV 4/5] END max_features=log2, n_estimators=100;, score=1.000 total time=   0.1s\n[CV 5/5] END max_features=log2, n_estimators=100;, score=0.900 total time=   0.1s\n[CV 1/5] END max_features=log2, n_estimators=250;, score=0.900 total time=   0.3s\n[CV 2/5] END max_features=log2, n_estimators=250;, score=0.850 total time=   0.3s\n[CV 3/5] END max_features=log2, n_estimators=250;, score=0.950 total time=   0.3s\n[CV 4/5] END max_features=log2, n_estimators=250;, score=1.000 total time=   0.3s\n[CV 5/5] END max_features=log2, n_estimators=250;, score=0.900 total time=   0.3s\n[CV 1/5] END max_features=None, n_estimators=100;, score=0.900 total time=   0.1s\n[CV 2/5] END max_features=None, n_estimators=100;, score=0.850 total time=   0.1s\n[CV 3/5] END max_features=None, n_estimators=100;, score=0.950 total time=   0.1s\n[CV 4/5] END max_features=None, n_estimators=100;, score=0.950 total time=   0.2s\n[CV 5/5] END max_features=None, n_estimators=100;, score=0.850 total time=   0.2s\n[CV 1/5] END max_features=None, n_estimators=250;, score=0.900 total time=   0.4s\n[CV 2/5] END max_features=None, n_estimators=250;, score=0.850 total time=   0.3s\n[CV 3/5] END max_features=None, n_estimators=250;, score=0.950 total time=   0.4s\n[CV 4/5] END max_features=None, n_estimators=250;, score=0.950 total time=   0.4s\n[CV 5/5] END max_features=None, n_estimators=250;, score=0.850 total time=   0.3s\n\n\n\n\nCode\n#collapse_output\n# a new instance of ModelCard is created and used to document the model\nmodel_doc_gingado = ModelCard()\nmodel_doc_gingado.read_model(gingado_clf.benchmark)\nprint(model_doc_gingado.show_json()['model_details']['info'])\n\n# but given that gingado Benchmark objects already document the best model at every fit, we can check that they are equal:\nassert model_doc_gingado.show_json()['model_details']['info'] == gingado_clf.model_documentation.show_json()['model_details']['info']\n\n\n{'_estimator_type': 'classifier', 'best_estimator_': RandomForestClassifier(n_estimators=250), 'best_index_': 2, 'best_params_': {'n_estimators': 250}, 'best_score_': 0.9399999999999998, 'classes_': array([0, 1]), 'cv_results_': {'mean_fit_time': array([0.0422112 , 0.08873978, 0.21713166]), 'std_fit_time': array([0.00135132, 0.00518193, 0.0078622 ]), 'mean_score_time': array([0.00399241, 0.00711241, 0.02049341]), 'std_score_time': array([0.00042215, 0.00014769, 0.00255255]), 'param_n_estimators': masked_array(data=[50, 100, 250],\n             mask=[False, False, False],\n       fill_value='?',\n            dtype=object), 'params': [{'n_estimators': 50}, {'n_estimators': 100}, {'n_estimators': 250}], 'split0_test_score': array([0.95, 0.95, 0.95]), 'split1_test_score': array([0.95, 0.95, 0.95]), 'split2_test_score': array([0.95, 0.9 , 0.95]), 'split3_test_score': array([0.9, 0.9, 0.9]), 'split4_test_score': array([0.9 , 0.95, 0.95]), 'mean_test_score': array([0.93, 0.93, 0.94]), 'std_test_score': array([0.0244949, 0.0244949, 0.02     ]), 'rank_test_score': array([2, 2, 1], dtype=int32)}, 'multimetric_': False, 'n_features_in_': 20, 'n_splits_': 5, 'refit_time_': 0.21876978874206543, 'scorer_': <function _passthrough_scorer>}\n\n\n\n\n3.3 scikit-learn\n\n\nCode\nfrom sklearn.ensemble import RandomForestClassifier\n\nsklearn_clf = RandomForestClassifier().fit(X, y)\n\n\n\n\nCode\n#collapse_output\nmodel_doc_sklearn = ModelCard()\nmodel_doc_sklearn.read_model(sklearn_clf)\nprint(model_doc_sklearn.show_json()['model_details']['info'])\n\n\n{'_estimator_type': 'classifier', 'base_estimator_': DecisionTreeClassifier(), 'classes_': array([0, 1]), 'estimators_': [DecisionTreeClassifier(max_features='auto', random_state=41133039), DecisionTreeClassifier(max_features='auto', random_state=1202631833), DecisionTreeClassifier(max_features='auto', random_state=851097655), DecisionTreeClassifier(max_features='auto', random_state=1214981591), DecisionTreeClassifier(max_features='auto', random_state=432508233), DecisionTreeClassifier(max_features='auto', random_state=276374143), DecisionTreeClassifier(max_features='auto', random_state=2089672251), DecisionTreeClassifier(max_features='auto', random_state=1443147939), DecisionTreeClassifier(max_features='auto', random_state=1113101672), DecisionTreeClassifier(max_features='auto', random_state=1660438956), DecisionTreeClassifier(max_features='auto', random_state=1737532035), DecisionTreeClassifier(max_features='auto', random_state=1991274353), DecisionTreeClassifier(max_features='auto', random_state=37854801), DecisionTreeClassifier(max_features='auto', random_state=1319347421), DecisionTreeClassifier(max_features='auto', random_state=1485018085), DecisionTreeClassifier(max_features='auto', random_state=1489861295), DecisionTreeClassifier(max_features='auto', random_state=813618998), DecisionTreeClassifier(max_features='auto', random_state=52812599), DecisionTreeClassifier(max_features='auto', random_state=489375747), DecisionTreeClassifier(max_features='auto', random_state=1359076922), DecisionTreeClassifier(max_features='auto', random_state=751837518), DecisionTreeClassifier(max_features='auto', random_state=382698717), DecisionTreeClassifier(max_features='auto', random_state=1988331547), DecisionTreeClassifier(max_features='auto', random_state=1619665204), DecisionTreeClassifier(max_features='auto', random_state=924701217), DecisionTreeClassifier(max_features='auto', random_state=1463298929), DecisionTreeClassifier(max_features='auto', random_state=785452322), DecisionTreeClassifier(max_features='auto', random_state=1829322095), DecisionTreeClassifier(max_features='auto', random_state=1104922124), DecisionTreeClassifier(max_features='auto', random_state=1557612301), DecisionTreeClassifier(max_features='auto', random_state=1477307212), DecisionTreeClassifier(max_features='auto', random_state=1984706514), DecisionTreeClassifier(max_features='auto', random_state=1815249762), DecisionTreeClassifier(max_features='auto', random_state=246763536), DecisionTreeClassifier(max_features='auto', random_state=2095118571), DecisionTreeClassifier(max_features='auto', random_state=756888051), DecisionTreeClassifier(max_features='auto', random_state=286318778), DecisionTreeClassifier(max_features='auto', random_state=937335164), DecisionTreeClassifier(max_features='auto', random_state=1225113564), DecisionTreeClassifier(max_features='auto', random_state=770538409), DecisionTreeClassifier(max_features='auto', random_state=722741699), DecisionTreeClassifier(max_features='auto', random_state=1787329849), DecisionTreeClassifier(max_features='auto', random_state=445458406), DecisionTreeClassifier(max_features='auto', random_state=1699414818), DecisionTreeClassifier(max_features='auto', random_state=776165389), DecisionTreeClassifier(max_features='auto', random_state=1373554751), DecisionTreeClassifier(max_features='auto', random_state=1319056311), DecisionTreeClassifier(max_features='auto', random_state=665309444), DecisionTreeClassifier(max_features='auto', random_state=583367337), DecisionTreeClassifier(max_features='auto', random_state=1895650927), DecisionTreeClassifier(max_features='auto', random_state=61949215), DecisionTreeClassifier(max_features='auto', random_state=3262118), DecisionTreeClassifier(max_features='auto', random_state=76441463), DecisionTreeClassifier(max_features='auto', random_state=1778623344), DecisionTreeClassifier(max_features='auto', random_state=1311601758), DecisionTreeClassifier(max_features='auto', random_state=1635511540), DecisionTreeClassifier(max_features='auto', random_state=1109977305), DecisionTreeClassifier(max_features='auto', random_state=489739214), DecisionTreeClassifier(max_features='auto', random_state=2126238687), DecisionTreeClassifier(max_features='auto', random_state=960956342), DecisionTreeClassifier(max_features='auto', random_state=1057931643), DecisionTreeClassifier(max_features='auto', random_state=782018073), DecisionTreeClassifier(max_features='auto', random_state=76488698), DecisionTreeClassifier(max_features='auto', random_state=625931025), DecisionTreeClassifier(max_features='auto', random_state=1516536944), DecisionTreeClassifier(max_features='auto', random_state=2077421560), DecisionTreeClassifier(max_features='auto', random_state=1010306127), DecisionTreeClassifier(max_features='auto', random_state=1530989509), DecisionTreeClassifier(max_features='auto', random_state=1888049167), DecisionTreeClassifier(max_features='auto', random_state=252592508), DecisionTreeClassifier(max_features='auto', random_state=745791077), DecisionTreeClassifier(max_features='auto', random_state=2113279341), DecisionTreeClassifier(max_features='auto', random_state=767082320), DecisionTreeClassifier(max_features='auto', random_state=690717513), DecisionTreeClassifier(max_features='auto', random_state=850169541), DecisionTreeClassifier(max_features='auto', random_state=4583203), DecisionTreeClassifier(max_features='auto', random_state=1955343751), DecisionTreeClassifier(max_features='auto', random_state=439078627), DecisionTreeClassifier(max_features='auto', random_state=1141107859), DecisionTreeClassifier(max_features='auto', random_state=540920150), DecisionTreeClassifier(max_features='auto', random_state=165831915), DecisionTreeClassifier(max_features='auto', random_state=1949478337), DecisionTreeClassifier(max_features='auto', random_state=1645693753), DecisionTreeClassifier(max_features='auto', random_state=1700475198), DecisionTreeClassifier(max_features='auto', random_state=852844086), DecisionTreeClassifier(max_features='auto', random_state=922439881), DecisionTreeClassifier(max_features='auto', random_state=785562595), DecisionTreeClassifier(max_features='auto', random_state=1013049307), DecisionTreeClassifier(max_features='auto', random_state=1287719325), DecisionTreeClassifier(max_features='auto', random_state=1258286125), DecisionTreeClassifier(max_features='auto', random_state=1064837157), DecisionTreeClassifier(max_features='auto', random_state=1495246752), DecisionTreeClassifier(max_features='auto', random_state=1466862244), DecisionTreeClassifier(max_features='auto', random_state=2060094054), DecisionTreeClassifier(max_features='auto', random_state=456054286), DecisionTreeClassifier(max_features='auto', random_state=1560787013), DecisionTreeClassifier(max_features='auto', random_state=1512040060), DecisionTreeClassifier(max_features='auto', random_state=468369193), DecisionTreeClassifier(max_features='auto', random_state=1717764713), DecisionTreeClassifier(max_features='auto', random_state=1748026652)], 'feature_importances_': array([0.26329534, 0.01894208, 0.01027939, 0.01685944, 0.10884405,\n       0.02636191, 0.01986916, 0.01088893, 0.01228042, 0.32213409,\n       0.0127138 , 0.01552959, 0.02895565, 0.01458237, 0.02318153,\n       0.02219858, 0.01753817, 0.01232438, 0.01833626, 0.02488488]), 'n_classes_': 2, 'n_features_': 20, 'n_features_in_': 20, 'n_outputs_': 1}\n\n\n/Users/douglasaraujo/Coding/.venv_gingado/lib/python3.10/site-packages/sklearn/utils/deprecation.py:103: FutureWarning: Attribute `n_features_` was deprecated in version 1.0 and will be removed in 1.2. Use `n_features_in_` instead.\n  warnings.warn(msg, category=FutureWarning)\n\n\n\n\n3.4 Keras\n\n\nCode\nfrom tensorflow import keras\n\nkeras_clf = keras.Sequential()\nkeras_clf.add(keras.layers.Dense(16, activation='relu', input_shape=(20,)))\nkeras_clf.add(keras.layers.Dense(8, activation='relu'))\nkeras_clf.add(keras.layers.Dense(1, activation='sigmoid'))\nkeras_clf.compile(optimizer='sgd', loss='binary_crossentropy')\nkeras_clf.fit(X, y, batch_size=10, epochs=10)\n\n\nEpoch 1/10\n 1/10 [==>...........................] - ETA: 2s - loss: 0.6275\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 871us/step - loss: 0.7085\nEpoch 2/10\n 1/10 [==>...........................] - ETA: 0s - loss: 0.6972\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 916us/step - loss: 0.6942\nEpoch 3/10\n 1/10 [==>...........................] - ETA: 0s - loss: 0.6707\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 736us/step - loss: 0.6808\nEpoch 4/10\n 1/10 [==>...........................] - ETA: 0s - loss: 0.6759\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 638us/step - loss: 0.6685\nEpoch 5/10\n 1/10 [==>...........................] - ETA: 0s - loss: 0.6471\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 628us/step - loss: 0.6570\nEpoch 6/10\n 1/10 [==>...........................] - ETA: 0s - loss: 0.5799\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 612us/step - loss: 0.6454\nEpoch 7/10\n 1/10 [==>...........................] - ETA: 0s - loss: 0.6701\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 609us/step - loss: 0.6343\nEpoch 8/10\n 1/10 [==>...........................] - ETA: 0s - loss: 0.5413\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 582us/step - loss: 0.6237\nEpoch 9/10\n 1/10 [==>...........................] - ETA: 0s - loss: 0.5790\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 615us/step - loss: 0.6128\nEpoch 10/10\n 1/10 [==>...........................] - ETA: 0s - loss: 0.6145\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 604us/step - loss: 0.6021\n\n\n<keras.callbacks.History at 0x14ce11930>\n\n\n\n\nCode\nmodel_doc_keras = ModelCard()\nmodel_doc_keras.read_model(keras_clf)\nmodel_doc_keras.show_json()['model_details']['info']\n\n\n'{\"class_name\": \"Sequential\", \"config\": {\"name\": \"sequential\", \"layers\": [{\"class_name\": \"InputLayer\", \"config\": {\"batch_input_shape\": [null, 20], \"dtype\": \"float32\", \"sparse\": false, \"ragged\": false, \"name\": \"dense_input\"}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense\", \"trainable\": true, \"batch_input_shape\": [null, 20], \"dtype\": \"float32\", \"units\": 16, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_1\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 8, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_2\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 1, \"activation\": \"sigmoid\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}]}, \"keras_version\": \"2.8.0\", \"backend\": \"tensorflow\"}'\n\n\n\n\n3.5 Other models\nNative support for automatic documentation of other model types, such as from fastai, pytorch is expected to be available in future versions. Until then, these models,any models coded form scratch by the user as well as any other model can be documented by passing the information as an argument to the Documenter’s fill_model_info method. This can be done in any core Python format (a string, a list, a dictionary, etc). For example:\n\n\nCode\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\nclass MockDataset(torch.utils.data.Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X.astype(np.float32))\n        self.y = torch.from_numpy(y.astype(np.float32))\n        self.len = self.X.shape[0]\n\n    def __len__(self):\n        return self.len\n\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n\nclass PytorchNet(torch.nn.Module):\n    def __init__(self):\n        super(PytorchNet, self).__init__()\n        self.layer1 = torch.nn.Linear(20, 16)\n        self.layer2 = torch.nn.Linear(16, 8)\n        self.layer3 = torch.nn.Linear(8, 1)\n\n    def forward(self, x):\n        x = torch.relu(self.layer1(x))\n        x = torch.relu(self.layer2(x))\n        x = torch.sigmoid(self.layer3(x))\n        return x\n\npytorch_clf = PytorchNet()\n\ndataloader = MockDataset(X, y)\n\n\nloss_func = torch.nn.BCELoss()\noptimizer = torch.optim.SGD(pytorch_clf.parameters(), lr=0.001, momentum=0.9)\n\nfor epoch in range(10):\n    running_loss = 0.0\n    for i, data in enumerate(dataloader, 0):\n        _X, _y = data\n        optimizer.zero_grad()\n        y_pred_epoch = pytorch_clf(_X)\n        loss = loss_func(y_pred_epoch, _y.reshape(1))\n        loss.backward()\n        optimizer.step()\n\n\n\n\nCode\nmodel_doc_pytorch = ModelCard()\nmodel_doc_pytorch.fill_model_info(\"This model is a neural network consisting of two fully connected layers and ending in a linear layer with a sigmoid activation\")\nmodel_doc_pytorch.show_json()['model_details']['info']\n\n\n'This model is a neural network consisting of two fully connected layers and ending in a linear layer with a sigmoid activation'"
  },
  {
    "objectID": "documentation.html#creating-a-custom-documenter",
    "href": "documentation.html#creating-a-custom-documenter",
    "title": "Model documentation",
    "section": "4 Creating a custom Documenter",
    "text": "4 Creating a custom Documenter\ngingado users can easily transform their model documentation needs into a Documenter object. The main advantages of doing this are: * the documentation template becomes a “recyclable” object that can be saved, loaded, and used in other models or code routines; and * model documentation can be more closely aligned with model creation and training, thus decreasing the probability that the model and its documentation diverge during the process of model development.\nThe requirements for an object to be a gingado Documenter are: * it must subclass ggdModelDocumentation (or implement all its methods if the user does not want to keep a dependency to gingado), * include the actual template for the documentation as a dictionary (with at most two levels of keys) in a class attribute called template, * follow the scikit-learn convention of storing the __init__ parameters in attributes with the same name, * implement the autofill_template method using the fill_info method to set the automatically filled information fields, and * implement the fill_model_info method that assigns the information in model_info into the class custom template."
  },
  {
    "objectID": "model_documentation.html",
    "href": "model_documentation.html",
    "title": "Model documentation",
    "section": "",
    "text": "Each user has a specific documentation need, ranging from simply logging the model training to a more complex description of the model pipeline with a discusson of the model outcomes. gingado addresses this variety of needs by offering a class of objects, “Documenters”, that facilitate model documentation. A base class facilitates the creation of generic ways to document models, and gingado includes off-the-shelf one specific model documentation template as described below.\nThe model documentation is performed by Documenters, objects that subclass from the base class ggdModelDocumentation. This base class offers code that can be used by any Documenter to read the model in question, format the information according to a template and save the resulting documentation in a JSON format. Documenters save the underlying information using the JSON format. With the JSON documentation file at hand, the user can then use existing third-party libraries to transform the information stored in JSON into a variety of formats (eg, HTML, PDF) as needed.\nOne current area of development is the automatic filing of some fields related to the model. The objective is to automatise documentation of the information that can be fetched automatically from the model, leaving time for the analyst to concentrate on other tasks, such as considering the ethical implications of the machine learning model being trained."
  },
  {
    "objectID": "model_documentation.html#modelcard",
    "href": "model_documentation.html#modelcard",
    "title": "Model documentation",
    "section": "ModelCard",
    "text": "ModelCard\nModelCard - the model documentation template inspired by the work of Mitchell et al, 2019 already comes with gingado. Its template can be used by users as is, or tweaked according to each need. The ModelCard template can also serve as inspiration for any custom documentation needs. Users with documentation needs beyond the out-of-the-box solutions provided by gingado can create their own class of Documenters (more information on that below), and compatibility with these custom documentation routines with the rest of the code is ensured. Users are encouraged to submit a pull request with their own documentation models subclassing ggdModelDocumentation if these custom templates can also benefit other users.\n\n\nModelCard\n\n ModelCard (file_path='', autofill=True, indent_level=2)\n\nBase class for gingado Documenters"
  },
  {
    "objectID": "model_documentation.html#basic-functioning-of-model-documentation",
    "href": "model_documentation.html#basic-functioning-of-model-documentation",
    "title": "Model documentation",
    "section": "Basic functioning of model documentation",
    "text": "Basic functioning of model documentation\nAfter a Documenter object, such as ModelCard is instanciated, the user can see the underlying template with the module show_template, as below:\n::: {.cell 0=‘c’ 1=‘o’ 2=‘l’ 3=‘l’ 4=‘a’ 5=‘p’ 6=‘s’ 7=‘e’ 8=’_’ 9=‘o’ 10=‘u’ 11=‘t’ 12=‘p’ 13=‘u’ 14=‘t’}\nmodel_doc = ModelCard(autofill=False)\nassert model_doc.show_template(indent=False) == ModelCard.template\n\nmodel_doc.show_template()\n\n{\n  \"model_details\": {\n    \"field_description\": \"Basic information about the model\",\n    \"developer\": \"Person or organisation developing the model\",\n    \"datetime\": \"Model date\",\n    \"version\": \"Model version\",\n    \"type\": \"Model type\",\n    \"info\": \"Information about training algorithms, parameters, fairness constraints or other applied approaches, and features\",\n    \"paper\": \"Paper or other resource for more information\",\n    \"citation\": \"Citation details\",\n    \"license\": \"License\",\n    \"contact\": \"Where to send questions or comments about the model\"\n  },\n  \"intended_use\": {\n    \"field_description\": \"Use cases that were envisioned during development\",\n    \"primary_uses\": \"Primary intended uses\",\n    \"primary_users\": \"Primary intended users\",\n    \"out_of_scope\": \"Out-of-scope use cases\"\n  },\n  \"factors\": {\n    \"field_description\": \"Factors could include demographic or phenotypic groups, environmental conditions, technical attributes, or others\",\n    \"relevant\": \"Relevant factors\",\n    \"evaluation\": \"Evaluation factors\"\n  },\n  \"metrics\": {\n    \"field_description\": \"Metrics should be chosen to reflect potential real world impacts of the model\",\n    \"performance_measures\": \"Model performance measures\",\n    \"thresholds\": \"Decision thresholds\",\n    \"variation_approaches\": \"Variation approaches\"\n  },\n  \"evaluation_data\": {\n    \"field_description\": \"Details on the dataset(s) used for the quantitative analyses in the documentation\",\n    \"datasets\": \"Datasets\",\n    \"motivation\": \"Motivation\",\n    \"preprocessing\": \"Preprocessing\"\n  },\n  \"training_data\": {\n    \"field_description\": \"\\n            May not be possible to provide in practice. When possible, this section should mirror 'Evaluation Data'. \\n            If such detail is not possible, minimal allowable information should be provided here, \\n            such as details of the distribution over various factors in the training datasets.\",\n    \"training_data\": \"Information on training data\"\n  },\n  \"quant_analyses\": {\n    \"field_description\": \"Quantitative Analyses\",\n    \"unitary\": \"Unitary results\",\n    \"intersectional\": \"Intersectional results\"\n  },\n  \"ethical_considerations\": {\n    \"field_description\": \"\\n            Ethical considerations that went into model development, surfacing ethical challenges and \\n            solutions to stakeholders. Ethical analysis does not always lead to precise solutions, but the process \\n            of ethical contemplation is worthwhile to inform on responsible practices and next steps in future work.\",\n    \"sensitive_data\": \"Does the model use any sensitive data (e.g., protected classes)?\",\n    \"human_life\": \"Is the model intended to inform decisions about mat- ters central to human life or flourishing - e.g., health or safety? Or could it be used in such a way?\",\n    \"mitigations\": \"What risk mitigation strategies were used during model development?\",\n    \"risks_and_harms\": \"\\n            What risks may be present in model usage? Try to identify the potential recipients, likelihood, and magnitude of harms. \\n            If these cannot be determined, note that they were consid- ered but remain unknown\",\n    \"use_cases\": \"Are there any known model use cases that are especially fraught?\",\n    \"additional_information\": \"\\n            If possible, this section should also include any additional ethical considerations that went into model development, \\n            for example, review by an external board, or testing with a specific community.\"\n  },\n  \"caveats_recommendations\": {\n    \"field_description\": \"Additional concerns that were not covered in the previous sections\",\n    \"caveats\": \"For example, did the results suggest any further testing? Were there any relevant groups that were not represented in the evaluation dataset?\",\n    \"recommendations\": \"Are there additional recommendations for model use? What are the ideal characteristics of an evaluation dataset for this model?\"\n  }\n}\n\n:::\nThe method show_json prints the Documenter’s documentation template, where the unfilled information retains the descriptions from the original template:\n::: {.cell 0=‘c’ 1=‘o’ 2=‘l’ 3=‘l’ 4=‘a’ 5=‘p’ 6=‘s’ 7=‘e’ 8=’_’ 9=‘o’ 10=‘u’ 11=‘t’ 12=‘p’ 13=‘u’ 14=‘t’}\nmodel_doc = ModelCard(autofill=True)\nmodel_doc.show_json()\n\n{'model_details': {'developer': 'douglasaraujo',\n  'datetime': '2022-06-06 01:45:36 ',\n  'version': 'Model version',\n  'type': 'Model type',\n  'info': 'Information about training algorithms, parameters, fairness constraints or other applied approaches, and features',\n  'paper': 'Paper or other resource for more information',\n  'citation': 'Citation details',\n  'license': 'License',\n  'contact': 'Where to send questions or comments about the model'},\n 'intended_use': {'primary_uses': 'Primary intended uses',\n  'primary_users': 'Primary intended users',\n  'out_of_scope': 'Out-of-scope use cases'},\n 'factors': {'relevant': 'Relevant factors',\n  'evaluation': 'Evaluation factors'},\n 'metrics': {'performance_measures': 'Model performance measures',\n  'thresholds': 'Decision thresholds',\n  'variation_approaches': 'Variation approaches'},\n 'evaluation_data': {'datasets': 'Datasets',\n  'motivation': 'Motivation',\n  'preprocessing': 'Preprocessing'},\n 'training_data': {'training_data': 'Information on training data'},\n 'quant_analyses': {'unitary': 'Unitary results',\n  'intersectional': 'Intersectional results'},\n 'ethical_considerations': {'sensitive_data': 'Does the model use any sensitive data (e.g., protected classes)?',\n  'human_life': 'Is the model intended to inform decisions about mat- ters central to human life or flourishing - e.g., health or safety? Or could it be used in such a way?',\n  'mitigations': 'What risk mitigation strategies were used during model development?',\n  'risks_and_harms': '\\n            What risks may be present in model usage? Try to identify the potential recipients, likelihood, and magnitude of harms. \\n            If these cannot be determined, note that they were consid- ered but remain unknown',\n  'use_cases': 'Are there any known model use cases that are especially fraught?',\n  'additional_information': '\\n            If possible, this section should also include any additional ethical considerations that went into model development, \\n            for example, review by an external board, or testing with a specific community.'},\n 'caveats_recommendations': {'caveats': 'For example, did the results suggest any further testing? Were there any relevant groups that were not represented in the evaluation dataset?',\n  'recommendations': 'Are there additional recommendations for model use? What are the ideal characteristics of an evaluation dataset for this model?'}}\n\n:::\nThe template is protected from editing once a Documenter has been created. This way, even if a user unwarrantedly changes the template, this does not interfere with the Documenter functionality.\n::: {.cell 0=‘c’ 1=‘o’ 2=‘l’ 3=‘l’ 4=‘a’ 5=‘p’ 6=‘s’ 7=‘e’ 8=’_’ 9=‘o’ 10=‘u’ 11=‘t’ 12=‘p’ 13=‘u’ 14=‘t’}\nmodel_doc.template = None\nmodel_doc.show_template()\n\nassert model_doc.show_template(indent=False) == ModelCard.template\n\n{\n  \"model_details\": {\n    \"field_description\": \"Basic information about the model\",\n    \"developer\": \"Person or organisation developing the model\",\n    \"datetime\": \"Model date\",\n    \"version\": \"Model version\",\n    \"type\": \"Model type\",\n    \"info\": \"Information about training algorithms, parameters, fairness constraints or other applied approaches, and features\",\n    \"paper\": \"Paper or other resource for more information\",\n    \"citation\": \"Citation details\",\n    \"license\": \"License\",\n    \"contact\": \"Where to send questions or comments about the model\"\n  },\n  \"intended_use\": {\n    \"field_description\": \"Use cases that were envisioned during development\",\n    \"primary_uses\": \"Primary intended uses\",\n    \"primary_users\": \"Primary intended users\",\n    \"out_of_scope\": \"Out-of-scope use cases\"\n  },\n  \"factors\": {\n    \"field_description\": \"Factors could include demographic or phenotypic groups, environmental conditions, technical attributes, or others\",\n    \"relevant\": \"Relevant factors\",\n    \"evaluation\": \"Evaluation factors\"\n  },\n  \"metrics\": {\n    \"field_description\": \"Metrics should be chosen to reflect potential real world impacts of the model\",\n    \"performance_measures\": \"Model performance measures\",\n    \"thresholds\": \"Decision thresholds\",\n    \"variation_approaches\": \"Variation approaches\"\n  },\n  \"evaluation_data\": {\n    \"field_description\": \"Details on the dataset(s) used for the quantitative analyses in the documentation\",\n    \"datasets\": \"Datasets\",\n    \"motivation\": \"Motivation\",\n    \"preprocessing\": \"Preprocessing\"\n  },\n  \"training_data\": {\n    \"field_description\": \"\\n            May not be possible to provide in practice. When possible, this section should mirror 'Evaluation Data'. \\n            If such detail is not possible, minimal allowable information should be provided here, \\n            such as details of the distribution over various factors in the training datasets.\",\n    \"training_data\": \"Information on training data\"\n  },\n  \"quant_analyses\": {\n    \"field_description\": \"Quantitative Analyses\",\n    \"unitary\": \"Unitary results\",\n    \"intersectional\": \"Intersectional results\"\n  },\n  \"ethical_considerations\": {\n    \"field_description\": \"\\n            Ethical considerations that went into model development, surfacing ethical challenges and \\n            solutions to stakeholders. Ethical analysis does not always lead to precise solutions, but the process \\n            of ethical contemplation is worthwhile to inform on responsible practices and next steps in future work.\",\n    \"sensitive_data\": \"Does the model use any sensitive data (e.g., protected classes)?\",\n    \"human_life\": \"Is the model intended to inform decisions about mat- ters central to human life or flourishing - e.g., health or safety? Or could it be used in such a way?\",\n    \"mitigations\": \"What risk mitigation strategies were used during model development?\",\n    \"risks_and_harms\": \"\\n            What risks may be present in model usage? Try to identify the potential recipients, likelihood, and magnitude of harms. \\n            If these cannot be determined, note that they were consid- ered but remain unknown\",\n    \"use_cases\": \"Are there any known model use cases that are especially fraught?\",\n    \"additional_information\": \"\\n            If possible, this section should also include any additional ethical considerations that went into model development, \\n            for example, review by an external board, or testing with a specific community.\"\n  },\n  \"caveats_recommendations\": {\n    \"field_description\": \"Additional concerns that were not covered in the previous sections\",\n    \"caveats\": \"For example, did the results suggest any further testing? Were there any relevant groups that were not represented in the evaluation dataset?\",\n    \"recommendations\": \"Are there additional recommendations for model use? What are the ideal characteristics of an evaluation dataset for this model?\"\n  }\n}\n\n:::\nUsers can find which fields in their templates are still without response by using the module open_questions. The levels of the template are reflected in the resulting dictionary, with double underscores separating the different dictionary levels in the underlying template.\nBelow we see that after inputting information for the item caveats in the section caveats_recommendations, this item does not appear in the results of the open_questions method.\n\nmodel_doc.fill_info({'caveats_recommendations': {'caveats': 'This is another test'}})\nassert model_doc.json_doc['caveats_recommendations']['caveats'] == \"This is another test\"\n\n# note that caveats_recommendations__caveats is no longer considered an open question\n# after being filled in through `fill_info`.\nprint([oq for oq in model_doc.open_questions() if oq.startswith('caveats')])\n\n['caveats_recommendations__recommendations']\n\n\nAnd now the complete result of the open_questions method:\n::: {.cell 0=‘c’ 1=‘o’ 2=‘l’ 3=‘l’ 4=‘a’ 5=‘p’ 6=‘s’ 7=‘e’ 8=’_’ 9=‘o’ 10=‘u’ 11=‘t’ 12=‘p’ 13=‘u’ 14=‘t’}\nmodel_doc.open_questions()\n\n['model_details__version',\n 'model_details__type',\n 'model_details__info',\n 'model_details__paper',\n 'model_details__citation',\n 'model_details__license',\n 'model_details__contact',\n 'intended_use__primary_uses',\n 'intended_use__primary_users',\n 'intended_use__out_of_scope',\n 'factors__relevant',\n 'factors__evaluation',\n 'metrics__performance_measures',\n 'metrics__thresholds',\n 'metrics__variation_approaches',\n 'evaluation_data__datasets',\n 'evaluation_data__motivation',\n 'evaluation_data__preprocessing',\n 'training_data__training_data',\n 'quant_analyses__unitary',\n 'quant_analyses__intersectional',\n 'ethical_considerations__sensitive_data',\n 'ethical_considerations__human_life',\n 'ethical_considerations__mitigations',\n 'ethical_considerations__risks_and_harms',\n 'ethical_considerations__use_cases',\n 'ethical_considerations__additional_information',\n 'caveats_recommendations__recommendations']\n\n:::\nIf the user wants to fill in an empty field such as the ones identified above by the method open_questions, the user simply needs to pass to the module fill_info a dictionary with the corresponding information. Depending on the template, the dictionary may be nested.\n\n\n\n\n\n\nNote\n\n\n\nit is technically possible to attribute the element directly to the attribute json_doc, but this should be avoided in favour of using the method fill_info. The latter tests whether the new information is valid according to the documentation template and also enables the filling of more than one question at the same time. In addition, attributing information directly to json_doc is not logged, and may unwarrantedly create new entries that are not part of the template (eg, if a new dictionary key is created due to typos).\n\n\nThe template serves to provide specific instances of the Documenter object with a form-like structure, indicating which fields are open and thus require some answers or information. Consequently, the template does not change when the actual document object changes after information is added by fill_info.\n\nnew_info = {\n    'metrics': {'performance_measures': \"This is a test\"},\n    'caveats_recommendations': {'caveats': \"This is another test\"}\n    }\n\nmodel_doc.fill_info(new_info)\nprint([model_doc.json_doc['metrics'], ModelCard.template['metrics']])\n\nassert model_doc.show_template(indent=False) == ModelCard.template\n\n[{'performance_measures': 'This is a test', 'thresholds': 'Decision thresholds', 'variation_approaches': 'Variation approaches'}, {'field_description': 'Metrics should be chosen to reflect potential real world impacts of the model', 'performance_measures': 'Model performance measures', 'thresholds': 'Decision thresholds', 'variation_approaches': 'Variation approaches'}]"
  },
  {
    "objectID": "model_documentation.html#reading-information-from-models",
    "href": "model_documentation.html#reading-information-from-models",
    "title": "Model documentation",
    "section": "Reading information from models",
    "text": "Reading information from models\ngingado’s ggdModelDocumentation base class is able to extract information from machine learning models from a number of widely used libraries and make it available to the Documenter objects. This is done through the method read_model, which recognises whether the model is a gingado object or any of scikit-learn, keras, or fastai models and read the model characteristics appropriately. For filing out information from other models (eg, pytorch or even models coded from scratch, machine learning or not), the user can benefit from the module fill_model_info that every Documenter should have, as demonstrated below.\nIn the case of ModelCard, these informations are included under model_details, item info. But the model information could be saved in another area of a custom Documenter.\n\n\n\n\n\n\nNote\n\n\n\nthe model-specific information saved is different depending on the model’s original library.\n\n\n\nPreliminaries\nThe mock dataset below is used to construct models using different libraries, to demonstrate how they are read by Documenters.\n\nfrom sklearn.datasets import make_classification\n\n# some mock up data\nX, y = make_classification()\n\nX.shape, y.shape\n\n((100, 20), (100,))\n\n\n\n\ngingado Benchmark\n\nfrom gingado.benchmark import ClassificationBenchmark\n\n# the gingado benchmark\ngingado_clf = ClassificationBenchmark(verbose_grid=3).fit(X, y)\n\nFitting 5 folds for each of 6 candidates, totalling 30 fits\n[CV 1/5] END max_features=sqrt, n_estimators=100;, score=0.850 total time=   0.1s\n[CV 2/5] END max_features=sqrt, n_estimators=100;, score=0.850 total time=   0.2s\n[CV 3/5] END max_features=sqrt, n_estimators=100;, score=0.950 total time=   0.1s\n[CV 4/5] END max_features=sqrt, n_estimators=100;, score=0.950 total time=   0.1s\n[CV 5/5] END max_features=sqrt, n_estimators=100;, score=0.850 total time=   0.1s\n[CV 1/5] END max_features=sqrt, n_estimators=250;, score=0.900 total time=   0.3s\n[CV 2/5] END max_features=sqrt, n_estimators=250;, score=0.850 total time=   0.3s\n[CV 3/5] END max_features=sqrt, n_estimators=250;, score=0.950 total time=   0.3s\n[CV 4/5] END max_features=sqrt, n_estimators=250;, score=0.950 total time=   0.3s\n[CV 5/5] END max_features=sqrt, n_estimators=250;, score=0.850 total time=   0.3s\n[CV 1/5] END max_features=log2, n_estimators=100;, score=0.850 total time=   0.1s\n[CV 2/5] END max_features=log2, n_estimators=100;, score=0.850 total time=   0.1s\n[CV 3/5] END max_features=log2, n_estimators=100;, score=0.950 total time=   0.1s\n[CV 4/5] END max_features=log2, n_estimators=100;, score=0.950 total time=   0.1s\n[CV 5/5] END max_features=log2, n_estimators=100;, score=0.850 total time=   0.1s\n[CV 1/5] END max_features=log2, n_estimators=250;, score=0.850 total time=   0.3s\n[CV 2/5] END max_features=log2, n_estimators=250;, score=0.850 total time=   0.3s\n[CV 3/5] END max_features=log2, n_estimators=250;, score=0.950 total time=   0.3s\n[CV 4/5] END max_features=log2, n_estimators=250;, score=0.950 total time=   0.3s\n[CV 5/5] END max_features=log2, n_estimators=250;, score=0.850 total time=   0.3s\n[CV 1/5] END max_features=None, n_estimators=100;, score=0.850 total time=   0.1s\n[CV 2/5] END max_features=None, n_estimators=100;, score=0.850 total time=   0.1s\n[CV 3/5] END max_features=None, n_estimators=100;, score=0.900 total time=   0.1s\n[CV 4/5] END max_features=None, n_estimators=100;, score=0.900 total time=   0.1s\n[CV 5/5] END max_features=None, n_estimators=100;, score=0.750 total time=   0.1s\n[CV 1/5] END max_features=None, n_estimators=250;, score=0.850 total time=   0.3s\n[CV 2/5] END max_features=None, n_estimators=250;, score=0.850 total time=   0.3s\n[CV 3/5] END max_features=None, n_estimators=250;, score=0.900 total time=   0.3s\n[CV 4/5] END max_features=None, n_estimators=250;, score=0.950 total time=   0.4s\n[CV 5/5] END max_features=None, n_estimators=250;, score=0.750 total time=   0.3s\n\n\n\n#collapse_output\n# a new instance of ModelCard is created and used to document the model\nmodel_doc_gingado = ModelCard()\nmodel_doc_gingado.read_model(gingado_clf.benchmark)\nprint(model_doc_gingado.show_json()['model_details']['info'])\n\n# but given that gingado Benchmark objects already document the best model at every fit, we can check that they are equal:\nassert model_doc_gingado.show_json()['model_details']['info'] == gingado_clf.model_documentation.show_json()['model_details']['info']\n\n{'_estimator_type': 'classifier', 'best_estimator_': RandomForestClassifier(n_estimators=250), 'best_index_': 2, 'best_params_': {'n_estimators': 250}, 'best_score_': 0.9399999999999998, 'classes_': array([0, 1]), 'cv_results_': {'mean_fit_time': array([0.0422112 , 0.08873978, 0.21713166]), 'std_fit_time': array([0.00135132, 0.00518193, 0.0078622 ]), 'mean_score_time': array([0.00399241, 0.00711241, 0.02049341]), 'std_score_time': array([0.00042215, 0.00014769, 0.00255255]), 'param_n_estimators': masked_array(data=[50, 100, 250],\n             mask=[False, False, False],\n       fill_value='?',\n            dtype=object), 'params': [{'n_estimators': 50}, {'n_estimators': 100}, {'n_estimators': 250}], 'split0_test_score': array([0.95, 0.95, 0.95]), 'split1_test_score': array([0.95, 0.95, 0.95]), 'split2_test_score': array([0.95, 0.9 , 0.95]), 'split3_test_score': array([0.9, 0.9, 0.9]), 'split4_test_score': array([0.9 , 0.95, 0.95]), 'mean_test_score': array([0.93, 0.93, 0.94]), 'std_test_score': array([0.0244949, 0.0244949, 0.02     ]), 'rank_test_score': array([2, 2, 1], dtype=int32)}, 'multimetric_': False, 'n_features_in_': 20, 'n_splits_': 5, 'refit_time_': 0.21876978874206543, 'scorer_': <function _passthrough_scorer at 0x13b7f9510>}\n\n\n\n\nscikit-learn\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nsklearn_clf = RandomForestClassifier().fit(X, y)\n\n::: {.cell 0=‘c’ 1=‘o’ 2=‘l’ 3=‘l’ 4=‘a’ 5=‘p’ 6=‘s’ 7=‘e’ 8=’_’ 9=‘o’ 10=‘u’ 11=‘t’ 12=‘p’ 13=‘u’ 14=‘t’}\nmodel_doc_sklearn = ModelCard()\nmodel_doc_sklearn.read_model(sklearn_clf)\nprint(model_doc_sklearn.show_json()['model_details']['info'])\n\n{'_estimator_type': 'classifier', 'base_estimator_': DecisionTreeClassifier(), 'classes_': array([0, 1]), 'estimators_': [DecisionTreeClassifier(max_features='auto', random_state=41133039), DecisionTreeClassifier(max_features='auto', random_state=1202631833), DecisionTreeClassifier(max_features='auto', random_state=851097655), DecisionTreeClassifier(max_features='auto', random_state=1214981591), DecisionTreeClassifier(max_features='auto', random_state=432508233), DecisionTreeClassifier(max_features='auto', random_state=276374143), DecisionTreeClassifier(max_features='auto', random_state=2089672251), DecisionTreeClassifier(max_features='auto', random_state=1443147939), DecisionTreeClassifier(max_features='auto', random_state=1113101672), DecisionTreeClassifier(max_features='auto', random_state=1660438956), DecisionTreeClassifier(max_features='auto', random_state=1737532035), DecisionTreeClassifier(max_features='auto', random_state=1991274353), DecisionTreeClassifier(max_features='auto', random_state=37854801), DecisionTreeClassifier(max_features='auto', random_state=1319347421), DecisionTreeClassifier(max_features='auto', random_state=1485018085), DecisionTreeClassifier(max_features='auto', random_state=1489861295), DecisionTreeClassifier(max_features='auto', random_state=813618998), DecisionTreeClassifier(max_features='auto', random_state=52812599), DecisionTreeClassifier(max_features='auto', random_state=489375747), DecisionTreeClassifier(max_features='auto', random_state=1359076922), DecisionTreeClassifier(max_features='auto', random_state=751837518), DecisionTreeClassifier(max_features='auto', random_state=382698717), DecisionTreeClassifier(max_features='auto', random_state=1988331547), DecisionTreeClassifier(max_features='auto', random_state=1619665204), DecisionTreeClassifier(max_features='auto', random_state=924701217), DecisionTreeClassifier(max_features='auto', random_state=1463298929), DecisionTreeClassifier(max_features='auto', random_state=785452322), DecisionTreeClassifier(max_features='auto', random_state=1829322095), DecisionTreeClassifier(max_features='auto', random_state=1104922124), DecisionTreeClassifier(max_features='auto', random_state=1557612301), DecisionTreeClassifier(max_features='auto', random_state=1477307212), DecisionTreeClassifier(max_features='auto', random_state=1984706514), DecisionTreeClassifier(max_features='auto', random_state=1815249762), DecisionTreeClassifier(max_features='auto', random_state=246763536), DecisionTreeClassifier(max_features='auto', random_state=2095118571), DecisionTreeClassifier(max_features='auto', random_state=756888051), DecisionTreeClassifier(max_features='auto', random_state=286318778), DecisionTreeClassifier(max_features='auto', random_state=937335164), DecisionTreeClassifier(max_features='auto', random_state=1225113564), DecisionTreeClassifier(max_features='auto', random_state=770538409), DecisionTreeClassifier(max_features='auto', random_state=722741699), DecisionTreeClassifier(max_features='auto', random_state=1787329849), DecisionTreeClassifier(max_features='auto', random_state=445458406), DecisionTreeClassifier(max_features='auto', random_state=1699414818), DecisionTreeClassifier(max_features='auto', random_state=776165389), DecisionTreeClassifier(max_features='auto', random_state=1373554751), DecisionTreeClassifier(max_features='auto', random_state=1319056311), DecisionTreeClassifier(max_features='auto', random_state=665309444), DecisionTreeClassifier(max_features='auto', random_state=583367337), DecisionTreeClassifier(max_features='auto', random_state=1895650927), DecisionTreeClassifier(max_features='auto', random_state=61949215), DecisionTreeClassifier(max_features='auto', random_state=3262118), DecisionTreeClassifier(max_features='auto', random_state=76441463), DecisionTreeClassifier(max_features='auto', random_state=1778623344), DecisionTreeClassifier(max_features='auto', random_state=1311601758), DecisionTreeClassifier(max_features='auto', random_state=1635511540), DecisionTreeClassifier(max_features='auto', random_state=1109977305), DecisionTreeClassifier(max_features='auto', random_state=489739214), DecisionTreeClassifier(max_features='auto', random_state=2126238687), DecisionTreeClassifier(max_features='auto', random_state=960956342), DecisionTreeClassifier(max_features='auto', random_state=1057931643), DecisionTreeClassifier(max_features='auto', random_state=782018073), DecisionTreeClassifier(max_features='auto', random_state=76488698), DecisionTreeClassifier(max_features='auto', random_state=625931025), DecisionTreeClassifier(max_features='auto', random_state=1516536944), DecisionTreeClassifier(max_features='auto', random_state=2077421560), DecisionTreeClassifier(max_features='auto', random_state=1010306127), DecisionTreeClassifier(max_features='auto', random_state=1530989509), DecisionTreeClassifier(max_features='auto', random_state=1888049167), DecisionTreeClassifier(max_features='auto', random_state=252592508), DecisionTreeClassifier(max_features='auto', random_state=745791077), DecisionTreeClassifier(max_features='auto', random_state=2113279341), DecisionTreeClassifier(max_features='auto', random_state=767082320), DecisionTreeClassifier(max_features='auto', random_state=690717513), DecisionTreeClassifier(max_features='auto', random_state=850169541), DecisionTreeClassifier(max_features='auto', random_state=4583203), DecisionTreeClassifier(max_features='auto', random_state=1955343751), DecisionTreeClassifier(max_features='auto', random_state=439078627), DecisionTreeClassifier(max_features='auto', random_state=1141107859), DecisionTreeClassifier(max_features='auto', random_state=540920150), DecisionTreeClassifier(max_features='auto', random_state=165831915), DecisionTreeClassifier(max_features='auto', random_state=1949478337), DecisionTreeClassifier(max_features='auto', random_state=1645693753), DecisionTreeClassifier(max_features='auto', random_state=1700475198), DecisionTreeClassifier(max_features='auto', random_state=852844086), DecisionTreeClassifier(max_features='auto', random_state=922439881), DecisionTreeClassifier(max_features='auto', random_state=785562595), DecisionTreeClassifier(max_features='auto', random_state=1013049307), DecisionTreeClassifier(max_features='auto', random_state=1287719325), DecisionTreeClassifier(max_features='auto', random_state=1258286125), DecisionTreeClassifier(max_features='auto', random_state=1064837157), DecisionTreeClassifier(max_features='auto', random_state=1495246752), DecisionTreeClassifier(max_features='auto', random_state=1466862244), DecisionTreeClassifier(max_features='auto', random_state=2060094054), DecisionTreeClassifier(max_features='auto', random_state=456054286), DecisionTreeClassifier(max_features='auto', random_state=1560787013), DecisionTreeClassifier(max_features='auto', random_state=1512040060), DecisionTreeClassifier(max_features='auto', random_state=468369193), DecisionTreeClassifier(max_features='auto', random_state=1717764713), DecisionTreeClassifier(max_features='auto', random_state=1748026652)], 'feature_importances_': array([0.26329534, 0.01894208, 0.01027939, 0.01685944, 0.10884405,\n       0.02636191, 0.01986916, 0.01088893, 0.01228042, 0.32213409,\n       0.0127138 , 0.01552959, 0.02895565, 0.01458237, 0.02318153,\n       0.02219858, 0.01753817, 0.01232438, 0.01833626, 0.02488488]), 'n_classes_': 2, 'n_features_': 20, 'n_features_in_': 20, 'n_outputs_': 1}\n\n\n/Users/douglasaraujo/Coding/.venv_gingado/lib/python3.10/site-packages/sklearn/utils/deprecation.py:103: FutureWarning: Attribute `n_features_` was deprecated in version 1.0 and will be removed in 1.2. Use `n_features_in_` instead.\n  warnings.warn(msg, category=FutureWarning)\n\n:::\n\n\nKeras\n\nfrom tensorflow import keras\n\nkeras_clf = keras.Sequential()\nkeras_clf.add(keras.layers.Dense(16, activation='relu', input_shape=(20,)))\nkeras_clf.add(keras.layers.Dense(8, activation='relu'))\nkeras_clf.add(keras.layers.Dense(1, activation='sigmoid'))\nkeras_clf.compile(optimizer='sgd', loss='binary_crossentropy')\nkeras_clf.fit(X, y, batch_size=10, epochs=10)\n\nEpoch 1/10\n\n 1/10 [==>...........................] - ETA: 1s - loss: 0.6874\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n10/10 [==============================] - 0s 901us/step - loss: 0.8318\nEpoch 2/10\n\n 1/10 [==>...........................] - ETA: 0s - loss: 0.6482\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n10/10 [==============================] - 0s 747us/step - loss: 0.8075\nEpoch 3/10\n\n 1/10 [==>...........................] - ETA: 0s - loss: 0.8411\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n10/10 [==============================] - 0s 782us/step - loss: 0.7869\nEpoch 4/10\n\n 1/10 [==>...........................] - ETA: 0s - loss: 0.6835\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n10/10 [==============================] - 0s 698us/step - loss: 0.7721\nEpoch 5/10\n\n 1/10 [==>...........................] - ETA: 0s - loss: 0.7928\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n10/10 [==============================] - 0s 740us/step - loss: 0.7576\nEpoch 6/10\n\n 1/10 [==>...........................] - ETA: 0s - loss: 0.7115\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n10/10 [==============================] - 0s 634us/step - loss: 0.7456\nEpoch 7/10\n\n 1/10 [==>...........................] - ETA: 0s - loss: 0.8325\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n10/10 [==============================] - 0s 649us/step - loss: 0.7358\nEpoch 8/10\n\n 1/10 [==>...........................] - ETA: 0s - loss: 0.6794\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n10/10 [==============================] - 0s 685us/step - loss: 0.7271\nEpoch 9/10\n\n 1/10 [==>...........................] - ETA: 0s - loss: 0.7410\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n10/10 [==============================] - 0s 725us/step - loss: 0.7185\nEpoch 10/10\n\n 1/10 [==>...........................] - ETA: 0s - loss: 0.6521\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n10/10 [==============================] - 0s 695us/step - loss: 0.7112\n\n\n<keras.callbacks.History at 0x1485e7fa0>\n\n\n\nmodel_doc_keras = ModelCard()\nmodel_doc_keras.read_model(keras_clf)\nmodel_doc_keras.show_json()['model_details']['info']\n\n'{\"class_name\": \"Sequential\", \"config\": {\"name\": \"sequential\", \"layers\": [{\"class_name\": \"InputLayer\", \"config\": {\"batch_input_shape\": [null, 20], \"dtype\": \"float32\", \"sparse\": false, \"ragged\": false, \"name\": \"dense_input\"}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense\", \"trainable\": true, \"batch_input_shape\": [null, 20], \"dtype\": \"float32\", \"units\": 16, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_1\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 8, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_2\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 1, \"activation\": \"sigmoid\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}]}, \"keras_version\": \"2.8.0\", \"backend\": \"tensorflow\"}'\n\n\n\n\nOther models\nNative support for automatic documentation of other model types, such as from fastai, pytorch is expected to be available in future versions. Until then, these models,any models coded form scratch by the user as well as any other model can be documented by passing the information as an argument to the Documenter’s fill_model_info method. This can be done in any core Python format (a string, a list, a dictionary, etc). For example:\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\nclass MockDataset(torch.utils.data.Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X.astype(np.float32))\n        self.y = torch.from_numpy(y.astype(np.float32))\n        self.len = self.X.shape[0]\n\n    def __len__(self):\n        return self.len\n\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n\nclass PytorchNet(torch.nn.Module):\n    def __init__(self):\n        super(PytorchNet, self).__init__()\n        self.layer1 = torch.nn.Linear(20, 16)\n        self.layer2 = torch.nn.Linear(16, 8)\n        self.layer3 = torch.nn.Linear(8, 1)\n\n    def forward(self, x):\n        x = torch.relu(self.layer1(x))\n        x = torch.relu(self.layer2(x))\n        x = torch.sigmoid(self.layer3(x))\n        return x\n\npytorch_clf = PytorchNet()\n\ndataloader = MockDataset(X, y)\n\n\nloss_func = torch.nn.BCELoss()\noptimizer = torch.optim.SGD(pytorch_clf.parameters(), lr=0.001, momentum=0.9)\n\nfor epoch in range(10):\n    running_loss = 0.0\n    for i, data in enumerate(dataloader, 0):\n        _X, _y = data\n        optimizer.zero_grad()\n        y_pred_epoch = pytorch_clf(_X)\n        loss = loss_func(y_pred_epoch, _y.reshape(1))\n        loss.backward()\n        optimizer.step()\n\n\nmodel_doc_pytorch = ModelCard()\nmodel_doc_pytorch.fill_model_info(\"This model is a neural network consisting of two fully connected layers and ending in a linear layer with a sigmoid activation\")\nmodel_doc_pytorch.show_json()['model_details']['info']\n\n'This model is a neural network consisting of two fully connected layers and ending in a linear layer with a sigmoid activation'"
  },
  {
    "objectID": "model_documentation.html#creating-a-custom-documenter",
    "href": "model_documentation.html#creating-a-custom-documenter",
    "title": "Model documentation",
    "section": "Creating a custom Documenter",
    "text": "Creating a custom Documenter\ngingado users can easily transform their model documentation needs into a Documenter object. The main advantages of doing this are: * the documentation template becomes a “recyclable” object that can be saved, loaded, and used in other models or code routines; and * model documentation can be more closely aligned with model creation and training, thus decreasing the probability that the model and its documentation diverge during the process of model development.\nThe requirements for an object to be a gingado Documenter are: * it must subclass ggdModelDocumentation (or implement all its methods if the user does not want to keep a dependency to gingado), * include the actual template for the documentation as a dictionary (with at most two levels of keys) in a class attribute called template, * follow the scikit-learn convention of storing the __init__ parameters in attributes with the same name, * implement the autofill_template method using the fill_info method to set the automatically filled information fields, and * implement the fill_model_info method that assigns the information in model_info into the class custom template.\n::: {.cell 0=‘h’ 1=‘i’ 2=‘d’ 3=‘e’}\nimport nbdev; nbdev.nbdev_export()\n:::"
  },
  {
    "objectID": "barrolee1994.html#establishing-a-benchmark-model",
    "href": "barrolee1994.html#establishing-a-benchmark-model",
    "title": "Using gingado to understand economic growth",
    "section": "2 Establishing a benchmark model",
    "text": "2 Establishing a benchmark model\nGenerally speaking, it is a good idea to establish a benchmark model at the first stages of development of the machine learning model. gingado offers a class of automatic benchmarks that can be used off-the-shelf depending on the task at hand: RegressionBenchmark and ClassificationBenchmark. It is also good to keep in mind that more advanced users can create their own benchmark on top of a base class provided by gingado: ggdBenchmark.\nFor this application, since we are interested in running a regression task, we will use RegressionBenchmark:\n\n\nCode\nfrom gingado.benchmark import RegressionBenchmark\n\n\nWhat this object does is the following: * it creates a random forest * three different versions of the random forest are trained on the user data * the version that performs better is chosen as the benchmark * right after it is trained, the benchmark is documented using gingado’s ModelCard documenter.\nThe user can easily change the parameters above. For example, instead of a random forest the user might prefer a neural network as the benchmark. Or, in lieu of the default parameters provided by gingado, users might have their own idea of what could be a reasonable parameter space to search.\nRandom forests are chosen as the go-to benchmark algorithm because of their reasonably good performance in a wide variety of settings, the fact that they don’t require much data transformation (ie, normalising the data to have zero mean and one standard deviation), and by virtue of their relatively transparency about the importance of each regressor.\nThe first step is to initialise the benchmark object. At this time, we pass some arguments about how we want it to behave. In this case, we set the verbosity level to produce output related to each alternative considered. Then we fit it to the data.\n\n\nCode\nbenchmark = RegressionBenchmark(verbose_grid=2)\nbenchmark.fit(X, y)\n\n\nNameError: name 'RegressionBenchmark' is not defined\n\n\nAs we can see above, with a few lines we have trained a random forest on the dataset. In this case, the benchmark was the better of six versions of the random forest, according to the default hyperparameters: 100 and 250 estimators were alternated with models for which the maximum number of regressors analysed by individual trees changesd fom the maximum, a square root and a log of the number of regressors. They were each trained using a 5-fold cross-validation.\nLet’s see which one was the best performing in this case, and hence our benchmark model:\n\n\nCode\npd.DataFrame(benchmark.benchmark.cv_results_).T\n\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n    \n  \n  \n    \n      mean_fit_time\n      0.112749\n      0.264248\n      0.111832\n      0.268002\n      0.174889\n      0.437714\n    \n    \n      std_fit_time\n      0.007359\n      0.009546\n      0.004156\n      0.010772\n      0.018838\n      0.030926\n    \n    \n      mean_score_time\n      0.006504\n      0.014531\n      0.00663\n      0.01508\n      0.007777\n      0.015449\n    \n    \n      std_score_time\n      0.000647\n      0.000383\n      0.000492\n      0.001202\n      0.003914\n      0.001545\n    \n    \n      param_max_features\n      sqrt\n      sqrt\n      log2\n      log2\n      None\n      None\n    \n    \n      param_n_estimators\n      100\n      250\n      100\n      250\n      100\n      250\n    \n    \n      params\n      {'max_features': 'sqrt', 'n_estimators': 100}\n      {'max_features': 'sqrt', 'n_estimators': 250}\n      {'max_features': 'log2', 'n_estimators': 100}\n      {'max_features': 'log2', 'n_estimators': 250}\n      {'max_features': None, 'n_estimators': 100}\n      {'max_features': None, 'n_estimators': 250}\n    \n    \n      split0_test_score\n      -0.08677\n      -0.134007\n      -0.191492\n      -0.232637\n      -0.450607\n      -0.426676\n    \n    \n      split1_test_score\n      -0.264261\n      -0.270617\n      -0.304131\n      -0.303325\n      -0.364518\n      -0.467272\n    \n    \n      split2_test_score\n      0.115623\n      0.190193\n      -0.018833\n      0.054703\n      0.464112\n      0.425264\n    \n    \n      split3_test_score\n      -0.667337\n      -0.724717\n      -0.819499\n      -0.694758\n      -0.597529\n      -0.558229\n    \n    \n      split4_test_score\n      0.181363\n      0.25248\n      0.111114\n      0.144391\n      0.291862\n      0.200824\n    \n    \n      mean_test_score\n      -0.144276\n      -0.137334\n      -0.244568\n      -0.206325\n      -0.131336\n      -0.165218\n    \n    \n      std_test_score\n      0.304921\n      0.35271\n      0.320742\n      0.296583\n      0.425981\n      0.399177\n    \n    \n      rank_test_score\n      3\n      2\n      6\n      5\n      1\n      4\n    \n  \n\n\n\n\nThe values above are calculated with \\(R^2\\), the default scoring function for a random forest from the scikit-learn package. Suppose that instead we would like a benchmark model that is optimised on the maximum error, ie a benchmark that minimises the worst deviation from prediction to ground truth for all the sample. These are the steps that we would take. Note that a more complete list of ready-made scoring parameters and how to create your own function can be found here.\n\n\nCode\nbenchmark_lower_worsterror = RegressionBenchmark(scoring='max_error', verbose_grid=3)\nbenchmark_lower_worsterror.fit(X, y)\n\n\nFitting 5 folds for each of 6 candidates, totalling 30 fits\n[CV 1/5] END max_features=sqrt, n_estimators=100;, score=-0.099 total time=   0.1s\n[CV 2/5] END max_features=sqrt, n_estimators=100;, score=-0.139 total time=   0.1s\n[CV 3/5] END max_features=sqrt, n_estimators=100;, score=-0.106 total time=   0.1s\n[CV 4/5] END max_features=sqrt, n_estimators=100;, score=-0.157 total time=   0.1s\n[CV 5/5] END max_features=sqrt, n_estimators=100;, score=-0.116 total time=   0.1s\n[CV 1/5] END max_features=sqrt, n_estimators=250;, score=-0.097 total time=   0.3s\n[CV 2/5] END max_features=sqrt, n_estimators=250;, score=-0.140 total time=   0.3s\n[CV 3/5] END max_features=sqrt, n_estimators=250;, score=-0.103 total time=   0.3s\n[CV 4/5] END max_features=sqrt, n_estimators=250;, score=-0.155 total time=   0.3s\n[CV 5/5] END max_features=sqrt, n_estimators=250;, score=-0.120 total time=   0.3s\n[CV 1/5] END max_features=log2, n_estimators=100;, score=-0.102 total time=   0.1s\n[CV 2/5] END max_features=log2, n_estimators=100;, score=-0.123 total time=   0.1s\n[CV 3/5] END max_features=log2, n_estimators=100;, score=-0.109 total time=   0.1s\n[CV 4/5] END max_features=log2, n_estimators=100;, score=-0.159 total time=   0.1s\n[CV 5/5] END max_features=log2, n_estimators=100;, score=-0.134 total time=   0.1s\n[CV 1/5] END max_features=log2, n_estimators=250;, score=-0.097 total time=   0.3s\n[CV 2/5] END max_features=log2, n_estimators=250;, score=-0.128 total time=   0.3s\n[CV 3/5] END max_features=log2, n_estimators=250;, score=-0.107 total time=   0.3s\n[CV 4/5] END max_features=log2, n_estimators=250;, score=-0.156 total time=   0.3s\n[CV 5/5] END max_features=log2, n_estimators=250;, score=-0.118 total time=   0.3s\n[CV 1/5] END max_features=None, n_estimators=100;, score=-0.100 total time=   0.2s\n[CV 2/5] END max_features=None, n_estimators=100;, score=-0.169 total time=   0.2s\n[CV 3/5] END max_features=None, n_estimators=100;, score=-0.090 total time=   0.2s\n[CV 4/5] END max_features=None, n_estimators=100;, score=-0.156 total time=   0.2s\n[CV 5/5] END max_features=None, n_estimators=100;, score=-0.110 total time=   0.2s\n[CV 1/5] END max_features=None, n_estimators=250;, score=-0.102 total time=   0.4s\n[CV 2/5] END max_features=None, n_estimators=250;, score=-0.161 total time=   0.4s\n[CV 3/5] END max_features=None, n_estimators=250;, score=-0.093 total time=   0.5s\n[CV 4/5] END max_features=None, n_estimators=250;, score=-0.158 total time=   0.5s\n[CV 5/5] END max_features=None, n_estimators=250;, score=-0.110 total time=   0.4s\n\n\nRegressionBenchmark(cv=StratifiedShuffleSplit(n_splits=10, random_state=None, test_size=None,\n            train_size=None),\n                    scoring='max_error', verbose_grid=3)\n\n\n\n\nCode\npd.DataFrame(benchmark_lower_worsterror.benchmark.cv_results_).T\n\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n    \n  \n  \n    \n      mean_fit_time\n      0.110616\n      0.258865\n      0.099738\n      0.246337\n      0.157\n      0.427363\n    \n    \n      std_fit_time\n      0.007504\n      0.010952\n      0.002257\n      0.002972\n      0.00523\n      0.039774\n    \n    \n      mean_score_time\n      0.006296\n      0.013911\n      0.00579\n      0.014069\n      0.005802\n      0.014855\n    \n    \n      std_score_time\n      0.000833\n      0.00087\n      0.000535\n      0.00035\n      0.000383\n      0.001451\n    \n    \n      param_max_features\n      sqrt\n      sqrt\n      log2\n      log2\n      None\n      None\n    \n    \n      param_n_estimators\n      100\n      250\n      100\n      250\n      100\n      250\n    \n    \n      params\n      {'max_features': 'sqrt', 'n_estimators': 100}\n      {'max_features': 'sqrt', 'n_estimators': 250}\n      {'max_features': 'log2', 'n_estimators': 100}\n      {'max_features': 'log2', 'n_estimators': 250}\n      {'max_features': None, 'n_estimators': 100}\n      {'max_features': None, 'n_estimators': 250}\n    \n    \n      split0_test_score\n      -0.098508\n      -0.096543\n      -0.102332\n      -0.097405\n      -0.09957\n      -0.101895\n    \n    \n      split1_test_score\n      -0.138769\n      -0.139704\n      -0.122678\n      -0.128035\n      -0.168995\n      -0.160625\n    \n    \n      split2_test_score\n      -0.106232\n      -0.102681\n      -0.108897\n      -0.106981\n      -0.089673\n      -0.092798\n    \n    \n      split3_test_score\n      -0.157448\n      -0.155037\n      -0.158671\n      -0.15589\n      -0.155839\n      -0.158399\n    \n    \n      split4_test_score\n      -0.115993\n      -0.119582\n      -0.134051\n      -0.11794\n      -0.110193\n      -0.109686\n    \n    \n      mean_test_score\n      -0.12339\n      -0.122709\n      -0.125326\n      -0.12125\n      -0.124854\n      -0.124681\n    \n    \n      std_test_score\n      0.021747\n      0.022043\n      0.019968\n      0.020145\n      0.031624\n      0.028946\n    \n    \n      rank_test_score\n      3\n      2\n      6\n      1\n      5\n      4\n    \n  \n\n\n\n\nNow we even have two benchmark models.\nWe could further tweak and adjust them, but one of the ideas behind having a benchmark is that it is simple and easy to set up.\nLet’s retain only the first benchmark, for simplicity, and now look at the predictions, comparing them to the original growth values.\n\n\nCode\ny_pred = benchmark.predict(X)\n\npd.DataFrame({\n    'y': y,\n    'y_pred': y_pred\n    }).plot.scatter(\n        x='y', y='y_pred',\n         grid=True, \n         title='Actual and predicted outcome',\n         xlabel='actual GDP growth',\n         ylabel='predicted GDP growth')\n\n\n/Users/douglasaraujo/Coding/.venv_gingado/lib/python3.10/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but RandomForestRegressor was fitted without feature names\n  warnings.warn(\n\n\n<AxesSubplot:title={'center':'Actual and predicted outcome'}, xlabel='actual GDP growth', ylabel='predicted GDP growth'>\n\n\n\n\n\nAnd now a histogram of the benchmark’s errors:\n\n\nCode\npd.DataFrame(y - y_pred).plot.hist(bins=30, title='Residual')\n\n\n<AxesSubplot:title={'center':'Residual'}, ylabel='Frequency'>\n\n\n\n\n\nSince the benchmark is a random forest model, we can see what are the most important regressors, measured as the average reduction in impurity across the trees in the random forest that actually use that particular regressor. They are scaled so that the sum for all features is one. Higher importance amounts indicate that that particular regressor is a more important contributor to the final prediction.\n\n\nCode\nregressor_importance = pd.DataFrame(\n    benchmark.benchmark.best_estimator_.feature_importances_, \n    index=X.columns, \n    columns=[\"Importance\"]\n    )\n\nregressor_importance.sort_values(by=\"Importance\", ascending=False) \\\n    .plot.bar(figsize=(20, 8), title='Regressor importance')\n\n\n<AxesSubplot:title={'center':'Regressor importance'}>\n\n\n\n\n\nFrom the graph above, we can see that the regressor bmp1l (black-market premium on foreign exchange) predominates. Interestingly, Belloni et al (2011) using squared-root lasso also find this regressor to be important."
  },
  {
    "objectID": "barrolee1994.html#references-1",
    "href": "barrolee1994.html#references-1",
    "title": "Using gingado to understand economic growth",
    "section": "References",
    "text": "References\n\n\n\nModuleNotFoundError: No module named 'nbdev'"
  }
]