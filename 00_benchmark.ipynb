{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#export\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic benchmark model\n",
    "> Functions to create a relevant, fast and reasonably well-performing benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Benchmark object has a similar API to a `sciki-learn` estimator: you build an instance with the desired arguments, and fit it to the data at a later moment. Benchmarks is a convenience wrapper for reading the training data, passing it through a simplified pipeline consisting of data imputation and a standard scalar, and then the benchmark function calibrated with a grid search.\n",
    "\n",
    "A `gingado` Benchmark object seeks to automatise a significant part of creating a benchmark model. Importantly, the Benchmark object also has a `compare` method that helps users evaluate if candidate models are better than the benchmark, and if one of them is, it becomes the new benchmark. This `compare` method takes as argument another fitted estimator (which could be itself a solo estimator or a whole pipeline) or a list of fitted estimators. \n",
    "\n",
    "Benchmarks start with default values that should perform reasonably well in most settings, but the user is also free to choose any of the benchmark's components by passing as arguments the data split, pipeline, and/or a dictionary of parameters for the hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base class\n",
    "\n",
    "`gingado` has a `ggdBenchmark` base class that contains the basic functionalities for Benchmark objects. It is not meant to be used by itself, but only as a hyperclass for Benchmark objects. `gingado` ships with two of these objects that subclass `ggdBenchmark`: `ClassificationBenchmark` and `RegressionBenchmark`. They are both described below in their respective sections.\n",
    "\n",
    "Users are encouraged to submit a PR with their own benchmark models subclassing `ggdBenchmark`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import TimeSeriesSplit, StratifiedShuffleSplit, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils.metaestimators import available_if\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from gingado.model_documentation import ModelCard\n",
    "\n",
    "def _benchmark_has(attr):\n",
    "    \"\"\"This function is used in `ggdBenchmark` to check if the benchmark has certain attributes\"\"\"\n",
    "    def check(self):\n",
    "        getattr(self.benchmark, attr)\n",
    "        return True\n",
    "    return check\n",
    "        \n",
    "class ggdBenchmark(BaseEstimator):\n",
    "    \"\"\"\n",
    "    The base class for gingado's Benchmark objects.\n",
    "    \"\"\"\n",
    "    def _check_is_time_series(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Checks whether the data is a time series, and sets a data splitter\n",
    "        accordingly if no data splitter is provided by the user\n",
    "        Note: all data without an index (eg, a Numpy array) are considered to NOT be a time series\n",
    "        \"\"\"\n",
    "        if hasattr(X, \"index\"):\n",
    "            self.is_timeseries = pd.core.dtypes.common.is_datetime_or_timedelta_dtype(X.index)\n",
    "        else:\n",
    "            self.is_timeseries = False\n",
    "        if self.is_timeseries and y:\n",
    "            if hasattr(y, \"index\"):\n",
    "                self.is_timeseries = pd.core.dtypes.common.is_datetime_or_timedelta_dtype(y.index)\n",
    "            else:\n",
    "                self.is_timeseries = False\n",
    "\n",
    "        if self.cv is None:\n",
    "            self.cv = TimeSeriesSplit() if self.is_timeseries else StratifiedShuffleSplit()\n",
    "\n",
    "    def _creates_estimator(self):\n",
    "        if self.estimator is None:\n",
    "            pass\n",
    "\n",
    "    def _fit(self, X, y):\n",
    "        self._check_is_time_series(X, y)\n",
    "\n",
    "        X, y = self._validate_data(X, y)\n",
    "\n",
    "        if hasattr(self.estimator, \"random_state\"):\n",
    "            self.estimator.random_state = self.random_state\n",
    "\n",
    "        if self.param_search and self.param_grid:                \n",
    "            self.benchmark = self.param_search(estimator=self.estimator, param_grid=self.param_grid, scoring=self.scoring, verbose=self.verbose_grid)\n",
    "        else:\n",
    "            self.benchmark = self.estimator\n",
    "            \n",
    "        self.benchmark.fit(X, y)\n",
    "\n",
    "        if self.auto_document is not None:\n",
    "            self.document()\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def set_benchmark(self, estimator):\n",
    "        self.benchmark = estimator\n",
    "\n",
    "    def _read_candidate_params(self, candidates, ensemble_method):\n",
    "                param_grid = []\n",
    "                for i, model in enumerate(candidates):\n",
    "                    check_is_fitted(model)\n",
    "                    param_grid.append({\n",
    "                        **{'candidate_estimator': [model]},\n",
    "                        **{\n",
    "                            'candidate_estimator__' + k: (v,)\n",
    "                            for k, v in model.get_params().items()\n",
    "                        }}\n",
    "                    )\n",
    "                if ensemble_method is not None:\n",
    "                    candidate_models = [('candidate_'+str(i+1), model) for i, model in enumerate(candidates)]\n",
    "                    voting = ensemble_method(estimators=candidate_models)\n",
    "                    ensemble = {'candidate_estimator': [voting]}\n",
    "                    param_grid.append(ensemble)\n",
    "                return param_grid\n",
    "\n",
    "    def compare(self, X, y, candidates, ensemble_method='object_default', update_benchmark=True):\n",
    "        \"\"\"\n",
    "        Uses a test dataset to compare the performance of the fitted benchmark model with one or more candidate models\n",
    "        This method achieves this by conducting a grid search \n",
    "        \"\"\"\n",
    "        # Step 0: check if the benchmark is fitted\n",
    "        check_is_fitted(self.benchmark)\n",
    "        old_benchmark_params = self.benchmark.get_params()\n",
    "        \n",
    "        candidates = list(candidates) if type(candidates) != list else candidates\n",
    "        list_candidates = [self.benchmark] + candidates\n",
    "        \n",
    "        est = self.benchmark.base_estimator_ if hasattr(self.benchmark, \"base_estimator_\") else self.benchmark.best_estimator_\n",
    "        cand_pipeline = Pipeline([('candidate_estimator', est)])\n",
    "        \n",
    "        if ensemble_method == 'object_default':\n",
    "            ensemble_method = self.ensemble_method\n",
    "        cand_params = self._read_candidate_params(list_candidates, ensemble_method=ensemble_method)\n",
    "        cand_grid = GridSearchCV(cand_pipeline, cand_params, verbose=self.verbose_grid).fit(X, y)\n",
    "        \n",
    "        self.model_comparison_ = cand_grid\n",
    "\n",
    "        if update_benchmark:\n",
    "            if cand_grid.best_estimator_.get_params() != old_benchmark_params:\n",
    "                self.set_benchmark(cand_grid)\n",
    "                print(\"Benchmark updated!\")\n",
    "\n",
    "        if self.auto_document is not None:\n",
    "            self.document()\n",
    "            \n",
    "    def document(self):\n",
    "        self.auto_document()\n",
    "\n",
    "    @available_if(_benchmark_has(\"predict\"))\n",
    "    def predict(self, X, **predict_params):\n",
    "        return self.benchmark.predict(X, **predict_params)\n",
    "\n",
    "    @available_if(_benchmark_has(\"fit_predict\"))\n",
    "    def fit_predict(self, X, y=None, **predict_params):\n",
    "        return self.benchmark.fit_predict(X, y, **predict_params)\n",
    "\n",
    "    @available_if(_benchmark_has(\"predict_proba\"))\n",
    "    def predict_proba(self, X, **predict_proba_params):\n",
    "        return self.benchmark.predict_proba(X, **predict_proba_params)\n",
    "\n",
    "    @available_if(_benchmark_has(\"decision_function\"))\n",
    "    def decision_function(self, X):\n",
    "        return self.benchmark.decision_function(X)\n",
    "    \n",
    "    @available_if(_benchmark_has(\"decision_function\"))\n",
    "    def decision_function(self, X):\n",
    "        return self.benchmark.decision_function(X)\n",
    "\n",
    "    @available_if(_benchmark_has(\"score_samples\"))\n",
    "    def score_samples(self, X):\n",
    "        return self.benchmark.score_samples(X)\n",
    "\n",
    "    @available_if(_benchmark_has(\"predict_log_proba\"))\n",
    "    def predict_log_proba(self, X, **predict_log_proba_params):\n",
    "        return self.benchmark.predict_log_proba(X, **predict_log_proba_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'show_doc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/douglasaraujo/Coding/gingado/00_benchmark.ipynb Cell 7'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/gingado/00_benchmark.ipynb#ch0000006?line=0'>1</a>\u001b[0m show_doc(ggdBenchmark)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'show_doc' is not defined"
     ]
    }
   ],
   "source": [
    "show_doc(ggdBenchmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification tasks\n",
    "\n",
    "The default benchmark for classification tasks is a [`RandomForestClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) object. Its parameters are fine-tuned in each case according to the user's data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from gingado.model_documentation import ModelCard\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "\n",
    "class ClassificationBenchmark(ggdBenchmark, ClassifierMixin):\n",
    "    def __init__(self, \n",
    "    cv=None, \n",
    "    estimator=RandomForestClassifier(), \n",
    "    param_grid={'n_estimators': [50, 100, 250]}, \n",
    "    param_search=GridSearchCV, \n",
    "    scoring=None, \n",
    "    auto_document=ModelCard, \n",
    "    random_state=None,\n",
    "    verbose_grid=False,\n",
    "    ensemble_method=VotingClassifier):\n",
    "        self.cv = cv\n",
    "        self.estimator = estimator\n",
    "        self.param_grid = param_grid\n",
    "        self.param_search = param_search\n",
    "        self.scoring = scoring\n",
    "        self.auto_document = auto_document\n",
    "        self.random_state = random_state\n",
    "        self.verbose_grid = verbose_grid\n",
    "        self.ensemble_method = ensemble_method\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self._fit(X, y)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"ClassificationBenchmark\" class=\"doc_header\"><code>class</code> <code>ClassificationBenchmark</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>ClassificationBenchmark</code>(**`cv`**=*`None`*, **`estimator`**=*`RandomForestClassifier()`*, **`param_grid`**=*`{'n_estimators': [50, 100, 250]}`*, **`param_search`**=*`GridSearchCV`*, **`scoring`**=*`None`*, **`auto_document`**=*`<gingado.model_documentation.ModelCard object at 0x13f9645e0>`*, **`random_state`**=*`None`*, **`verbose_grid`**=*`3`*, **`ensemble_method`**=*`VotingClassifier`*) :: [`ggdBenchmark`](/gingado/benchmark.html#ggdBenchmark)\n",
       "\n",
       "The base class for gingado's Benchmark objects."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(ClassificationBenchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "[CV 1/5] END ...................n_estimators=50;, score=0.850 total time=   0.1s\n",
      "[CV 2/5] END ...................n_estimators=50;, score=0.900 total time=   0.1s\n",
      "[CV 3/5] END ...................n_estimators=50;, score=0.850 total time=   0.1s\n",
      "[CV 4/5] END ...................n_estimators=50;, score=0.900 total time=   0.1s\n",
      "[CV 5/5] END ...................n_estimators=50;, score=0.750 total time=   0.1s\n",
      "[CV 1/5] END ..................n_estimators=100;, score=0.800 total time=   0.1s\n",
      "[CV 2/5] END ..................n_estimators=100;, score=0.850 total time=   0.1s\n",
      "[CV 3/5] END ..................n_estimators=100;, score=0.900 total time=   0.1s\n",
      "[CV 4/5] END ..................n_estimators=100;, score=0.850 total time=   0.1s\n",
      "[CV 5/5] END ..................n_estimators=100;, score=0.800 total time=   0.1s\n",
      "[CV 1/5] END ..................n_estimators=250;, score=0.800 total time=   0.2s\n",
      "[CV 2/5] END ..................n_estimators=250;, score=0.900 total time=   0.2s\n",
      "[CV 3/5] END ..................n_estimators=250;, score=0.950 total time=   0.2s\n",
      "[CV 4/5] END ..................n_estimators=250;, score=0.900 total time=   0.3s\n",
      "[CV 5/5] END ..................n_estimators=250;, score=0.850 total time=   0.2s\n"
     ]
    }
   ],
   "source": [
    "from gingado.benchmark import ClassificationBenchmark\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# some mock up data\n",
    "X, y = make_classification()\n",
    "\n",
    "# the gingado benchmark\n",
    "bm = ClassificationBenchmark(verbose_grid=3).fit(X, y)\n",
    "\n",
    "# note that now the `bm` object can be used as an estimator\n",
    "assert bm.predict(X).shape == y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also simple to define as benchmark a model that you already fitted and still benefit from the other functionalities provided by `Benchmark` class. This can also be done in case you are using a saved version of a fitted model (eg, the model you are using in production) and want to have that as the benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier().fit(X, y)\n",
    "\n",
    "bm.set_benchmark(estimator=forest)\n",
    "\n",
    "assert forest == bm.benchmark\n",
    "assert hasattr(bm.benchmark, \"predict\")\n",
    "assert bm.predict(X).shape == y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression tasks\n",
    "\n",
    "The default benchmark for regression tasks is a [`RandomForestRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) object.  Its parameters are fine-tuned in each case according to the user's data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from sklearn.base import RegressorMixin\n",
    "from sklearn.ensemble import RandomForestRegressor, VotingRegressor\n",
    "\n",
    "class RegressionBenchmark(ggdBenchmark, RegressorMixin):\n",
    "    def __init__(self, \n",
    "    cv=None, \n",
    "    estimator=RandomForestRegressor(), \n",
    "    param_grid={'n_estimators': [50, 100, 250]}, \n",
    "    param_search=GridSearchCV, \n",
    "    scoring=None, \n",
    "    auto_document=ModelCard, \n",
    "    random_state=None,\n",
    "    verbose_grid=False,\n",
    "    ensemble_method=VotingRegressor):\n",
    "        self.cv = cv\n",
    "        self.estimator = estimator\n",
    "        self.param_grid = param_grid\n",
    "        self.param_search = param_search\n",
    "        self.scoring = scoring\n",
    "        self.auto_document = auto_document\n",
    "        self.random_state = random_state\n",
    "        self.verbose_grid = verbose_grid\n",
    "        self.ensemble_method = ensemble_method\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self._fit(X, y)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"RegressionBenchmark\" class=\"doc_header\"><code>class</code> <code>RegressionBenchmark</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>RegressionBenchmark</code>(**`cv`**=*`None`*, **`estimator`**=*`RandomForestRegressor()`*, **`param_grid`**=*`{'n_estimators': [50, 100, 250]}`*, **`param_search`**=*`GridSearchCV`*, **`scoring`**=*`None`*, **`auto_document`**=*`<gingado.model_documentation.ModelCard object at 0x13f9673d0>`*, **`random_state`**=*`None`*, **`verbose_grid`**=*`False`*, **`ensemble_method`**=*`VotingRegressor`*) :: [`ggdBenchmark`](/gingado/benchmark.html#ggdBenchmark)\n",
       "\n",
       "The base class for gingado's Benchmark objects."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(RegressionBenchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    }
   ],
   "source": [
    "from gingado.benchmark import RegressionBenchmark\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# some mock up data\n",
    "X, y = make_regression()\n",
    "\n",
    "# the gingado benchmark\n",
    "bm = RegressionBenchmark(verbose_grid=3).fit(X, y)\n",
    "\n",
    "# note that now the `bm` object can be used as an estimator\n",
    "assert bm.predict(X).shape == y.shape\n",
    "\n",
    "# the user might also like to set another model as the benchmark\n",
    "forest = RandomForestRegressor().fit(X, y)\n",
    "bm.set_benchmark(estimator=forest)\n",
    "\n",
    "assert forest == bm.benchmark\n",
    "assert hasattr(bm.benchmark, \"predict\")\n",
    "assert bm.predict(X).shape == y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 101 candidates, totalling 505 fits\n",
      "Benchmark updated!\n"
     ]
    }
   ],
   "source": [
    "bm.compare(X, y, forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General comments on benchmarks\n",
    "\n",
    "#### Scoring\n",
    "\n",
    "`ClassificationBenchmark` and `RegressionBenchmark` both use the default scoring method for comparing model alternatives, both during estimation of the benchmark model and when comparing this benchmark with candidate models. Users are encouraged to consider if another scoring method is more suitable for their use case. More information on available scoring methods that are compatible with `gingado` Benchmark objects can be found [here](https://scikit-learn.org/stable/modules/model_evaluation.html).\n",
    "\n",
    "### Data split\n",
    "\n",
    "Please refer to [this page](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection) for more information on the different `Splitter` classes available on `scikit-learn`, and [this page](https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html#sphx-glr-auto-examples-model-selection-plot-cv-indices-py) for practical advice on how to choose a splitter for data that are not time series. Any one of these objects (or a custom splitter that is compatible with them) can be passed to a `Benchmark` object.\n",
    "\n",
    "The API does not accept custom parameters for the splitters. Users that wish to use specific parameters should include the actual `Splitter` object as the parameter.\n",
    "\n",
    "### Custom benchmarks\n",
    "\n",
    "`gingado` provides users with two `Benchmark` objects out of the box: `ClassificationBenchmark` and `RegressionBenchmark`, to be used depending on the task at hand. Both classes derive from a base class `ggdBenchmark`, which implements methods that facilitate model comparison. Users that want to create a customised benchmark model for themselves have two options:\n",
    "\n",
    "* the simpler possibility is to train the estimator as usual, and then assign the fitted estimator to a `Benchmark` object. \n",
    "* if the user wants more control over the fitting process of estimating the benchmark, they can create a class that subclasses from `ggdBenchmark` and either implements custom `fit`, `predict` and `score` methods, or also subclasses from [`scikit-learn`'s `BaseEstimator`](https://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html). \n",
    "  * In any case, if the user wants the benchmark to automatically detect if the data is a time series and also to document the model right after fitting, the `fit` method should call `self._fit` on the data. Otherwise, the user can simply implement any consistent logic in fit as the user sees fit (pun intended).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit ('.venv_gingado': venv)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
