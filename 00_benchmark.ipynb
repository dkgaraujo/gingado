{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "badges: true\n",
    "description: Functions to create a relevant, fast and reasonably well-performing benchmark\n",
    "output-file: benchmark.html\n",
    "title: Automatic benchmark model\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| include: false\n",
    "#| echo: false\n",
    "from __future__ import annotations # allows multiple typing of arguments in Python versions prior to 3.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "! [ -e /content ] && pip install -Uqq gingado nbdev # install or upgrade gingado on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#| include: false\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "#| echo: false\n",
    "\n",
    "# Code below included to ensure compatibility with scikit-learn v1.1.x\n",
    "from sklearn import set_config\n",
    "set_config(display='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Benchmark object has a similar API to a `sciki-learn` estimator: you build an instance with the desired arguments, and fit it to the data at a later moment. Benchmarks is a convenience wrapper for reading the training data, passing it through a simplified pipeline consisting of data imputation and a standard scalar, and then the benchmark function calibrated with a grid search.\n",
    "\n",
    "A `gingado` Benchmark object seeks to automatise a significant part of creating a benchmark model. Importantly, the Benchmark object also has a `compare` method that helps users evaluate if candidate models are better than the benchmark, and if one of them is, it becomes the new benchmark. This `compare` method takes as argument another fitted estimator (which could be itself a solo estimator or a whole pipeline) or a list of fitted estimators. \n",
    "\n",
    "Benchmarks start with default values that should perform reasonably well in most settings, but the user is also free to choose any of the benchmark's components by passing as arguments the data split, pipeline, and/or a dictionary of parameters for the hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base class\n",
    "\n",
    "`gingado` has a `ggdBenchmark` base class that contains the basic functionalities for Benchmark objects. It is not meant to be used by itself, but only as a hyperclass for Benchmark objects. `gingado` ships with two of these objects that subclass `ggdBenchmark`: `ClassificationBenchmark` and `RegressionBenchmark`. They are both described below in their respective sections.\n",
    "\n",
    "Users are encouraged to submit a PR with their own benchmark models subclassing `ggdBenchmark`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "#| export\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas.api.types as ptypes\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils.metaestimators import available_if\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from gingado.model_documentation import ggdModelDocumentation, ModelCard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "#| export\n",
    "\n",
    "def _benchmark_has(attr):\n",
    "    # Check if the benchmark has certain attributes\n",
    "    def check(self):\n",
    "        getattr(self.benchmark, attr)\n",
    "        return True\n",
    "    return check\n",
    "        \n",
    "class ggdBenchmark(BaseEstimator):\n",
    "    \"The base class for gingado's Benchmark objects.\"\n",
    "    \n",
    "    def _check_is_time_series(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Checks whether the data is a time series, and sets a data splitter\n",
    "        accordingly if no data splitter is provided by the user\n",
    "        Note: all data without an index (eg, a Numpy array) are considered to NOT be a time series\n",
    "        \"\"\"\n",
    "        if hasattr(X, \"index\"):\n",
    "            self.is_timeseries = (ptypes.is_datetime64_dtype(X.index)\n",
    "                                  or ptypes.is_timedelta64_dtype(X.index))\n",
    "        else:\n",
    "            self.is_timeseries = False\n",
    "        if self.is_timeseries and y is not None:\n",
    "            if hasattr(y, \"index\"):\n",
    "                self.is_timeseries = (ptypes.is_datetime64_dtype(y.index)\n",
    "                                      or ptypes.is_timedelta64_dtype(y.index))\n",
    "            else:\n",
    "                self.is_timeseries = False\n",
    "\n",
    "        if self.cv is None:\n",
    "            self.cv = TimeSeriesSplit() if self.is_timeseries else self.default_cv\n",
    "\n",
    "    def _creates_estimator(self):\n",
    "        if self.estimator is None:\n",
    "            pass\n",
    "\n",
    "    def _fit(self, X, y):\n",
    "        self._check_is_time_series(X, y)\n",
    "\n",
    "        X, y = self._validate_data(X, y)\n",
    "\n",
    "        if hasattr(self.estimator, \"random_state\"):\n",
    "            self.estimator.random_state = self.random_state\n",
    "\n",
    "        if self.param_search and self.param_grid:                \n",
    "            self.benchmark = self.param_search(\n",
    "                estimator=self.estimator, \n",
    "                param_grid=self.param_grid, \n",
    "                scoring=self.scoring, \n",
    "                cv=self.cv,\n",
    "                verbose=self.verbose_grid)\n",
    "        else:\n",
    "            self.benchmark = self.estimator\n",
    "            \n",
    "        self.benchmark.fit(X, y)\n",
    "\n",
    "        if self.auto_document is not None:\n",
    "            self.document()\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def set_benchmark(\n",
    "        self,\n",
    "        estimator # A fitted estimator object\n",
    "        ):\n",
    "        # Define a fitted `estimator` as the new benchmark model\n",
    "        check_is_fitted(estimator)\n",
    "        self.benchmark = estimator\n",
    "\n",
    "    def _read_candidate_params(self, candidates, ensemble_method):\n",
    "                param_grid = []\n",
    "                for i, model in enumerate(candidates):\n",
    "                    check_is_fitted(model)\n",
    "                    param_grid.append({\n",
    "                        **{'candidate_estimator': [model]},\n",
    "                        **{\n",
    "                            'candidate_estimator__' + k: (v,)\n",
    "                            for k, v in model.get_params().items()\n",
    "                        }}\n",
    "                    )\n",
    "                if ensemble_method is not None:\n",
    "                    candidate_models = [('candidate_'+str(i+1), model) for i, model in enumerate(candidates)]\n",
    "                    voting = ensemble_method(estimators=candidate_models)\n",
    "                    ensemble = {'candidate_estimator': [voting]}\n",
    "                    param_grid.append(ensemble)\n",
    "                return param_grid\n",
    "\n",
    "    def compare(\n",
    "        self,\n",
    "        X:np.ndarray, # Array-like data of shape (n_samples, n_features)\n",
    "        y:np.ndarray, # Array-like data of shape (n_samples,) or (n_samples, n_targets)\n",
    "        candidates, # Candidate estimator or list of candidate estimator(s)\n",
    "        ensemble_method='object_default', \n",
    "        update_benchmark:bool=True # Whether to use the best performing candidate model as the new benchmark\n",
    "        ):\n",
    "        \"Use a testing dataset to compare the performance of the fitted benchmark model with one or more candidate models using a grid search\"\n",
    "    \n",
    "        check_is_fitted(self.benchmark)\n",
    "        old_benchmark_params = self.benchmark.get_params()\n",
    "\n",
    "        candidates = list(candidates) if type(candidates) != list else candidates\n",
    "        list_candidates = [self.benchmark] + candidates\n",
    "        \n",
    "        est = self.benchmark.best_estimator_ if hasattr(self.benchmark, \"best_estimator_\") else self.benchmark\n",
    "        cand_pipeline = Pipeline([('candidate_estimator', est)])\n",
    "        \n",
    "        if ensemble_method == 'object_default':\n",
    "            ensemble_method = self.ensemble_method\n",
    "        cand_params = self._read_candidate_params(list_candidates, ensemble_method=ensemble_method)\n",
    "        cand_grid = GridSearchCV(cand_pipeline, cand_params, cv=self.cv, verbose=self.verbose_grid).fit(X, y)\n",
    "        \n",
    "        self.model_comparison_ = cand_grid\n",
    "\n",
    "        if update_benchmark:\n",
    "            if cand_grid.best_estimator_.get_params() != old_benchmark_params:\n",
    "                self.set_benchmark(cand_grid)\n",
    "                print(\"Benchmark updated!\")\n",
    "                print(\"New benchmark:\")\n",
    "                print(self.benchmark.best_estimator_)\n",
    "\n",
    "        if self.auto_document is not None:\n",
    "            self.document()\n",
    "\n",
    "    def compare_fitted_candidates(self, X, y, candidates, scoring_func):\n",
    "        check_is_fitted(self.benchmark)\n",
    "        candidates = list(candidates) if type(candidates) != list else candidates\n",
    "        for candidate in candidates:\n",
    "            check_is_fitted(candidate)\n",
    "        list_candidates = [self.benchmark] + candidates\n",
    "        \n",
    "        return {candidate.__repr__(): scoring_func(y, candidate.predict(X)) for candidate in list_candidates}\n",
    "\n",
    "    def _read_attr(self):\n",
    "        for a in dir(self.benchmark):\n",
    "            if a == '_estimator_type' or a.endswith(\"_\") and not a.startswith(\"_\") and not a.endswith(\"__\"):\n",
    "                try:\n",
    "                    model_attr = self.benchmark.__getattribute__(a)\n",
    "                    yield {a: model_attr}\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "    def document(\n",
    "        self, \n",
    "        documenter:ggdModelDocumentation|None=None # A gingado Documenter or the documenter set in `auto_document` if None.\n",
    "        ):\n",
    "        \"Document the benchmark model using the template in `documenter`\"\n",
    "        documenter = self.auto_document if documenter is None else documenter\n",
    "        self.model_documentation = documenter()\n",
    "        model_info = list(self._read_attr())\n",
    "        model_info = {k:v for i in model_info for k, v in i.items()}\n",
    "        #self.model_documentation.read_model(self.benchmark)\n",
    "        self.model_documentation.fill_model_info(model_info)\n",
    "\n",
    "    @available_if(_benchmark_has(\"predict\"))\n",
    "    def predict(self, X, **predict_params):\n",
    "        \"Note: only available if the benchmark implements this method.\"\n",
    "        return self.benchmark.predict(X, **predict_params)\n",
    "\n",
    "    @available_if(_benchmark_has(\"fit_predict\"))\n",
    "    def fit_predict(self, X, y=None, **predict_params):\n",
    "        \"Note: only available if the benchmark implements this method.\"\n",
    "        return self.benchmark.fit_predict(X, y, **predict_params)\n",
    "\n",
    "    @available_if(_benchmark_has(\"predict_proba\"))\n",
    "    def predict_proba(self, X, **predict_proba_params):\n",
    "        \"Note: only available if the benchmark implements this method.\"\n",
    "        return self.benchmark.predict_proba(X, **predict_proba_params)\n",
    "\n",
    "    @available_if(_benchmark_has(\"decision_function\"))\n",
    "    def decision_function(self, X):\n",
    "        \"Note: only available if the benchmark implements this method.\"\n",
    "        return self.benchmark.decision_function(X)\n",
    "\n",
    "    @available_if(_benchmark_has(\"score\"))\n",
    "    def score(self, X):\n",
    "        \"Note: only available if the benchmark implements this method.\"\n",
    "        return self.benchmark.score(X)\n",
    "\n",
    "    @available_if(_benchmark_has(\"score_samples\"))\n",
    "    def score_samples(self, X):\n",
    "        \"Note: only available if the benchmark implements this method.\"\n",
    "        return self.benchmark.score_samples(X)\n",
    "\n",
    "    @available_if(_benchmark_has(\"predict_log_proba\"))\n",
    "    def predict_log_proba(self, X, **predict_log_proba_params):\n",
    "        \"Note: only available if the benchmark implements this method.\"\n",
    "        return self.benchmark.predict_log_proba(X, **predict_log_proba_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/dkgaraujo/gingado/blob/main/gingado/benchmark.py#L24){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ggdBenchmark\n",
       "\n",
       ">      ggdBenchmark ()\n",
       "\n",
       "The base class for gingado's Benchmark objects."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/dkgaraujo/gingado/blob/main/gingado/benchmark.py#L24){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ggdBenchmark\n",
       "\n",
       ">      ggdBenchmark ()\n",
       "\n",
       "The base class for gingado's Benchmark objects."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ggdBenchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/dkgaraujo/gingado/blob/main/gingado/benchmark.py#L101){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### compare\n",
       "\n",
       ">      compare (X:numpy.ndarray, y:numpy.ndarray, candidates,\n",
       ">               ensemble_method='object_default', update_benchmark:bool=True)\n",
       "\n",
       "Use a testing dataset to compare the performance of the fitted benchmark model with one or more candidate models using a grid search\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| X | np.ndarray |  | Array-like data of shape (n_samples, n_features) |\n",
       "| y | np.ndarray |  | Array-like data of shape (n_samples,) or (n_samples, n_targets) |\n",
       "| candidates |  |  | Candidate estimator or list of candidate estimator(s) |\n",
       "| ensemble_method | str | object_default |  |\n",
       "| update_benchmark | bool | True | Whether to use the best performing candidate model as the new benchmark |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/dkgaraujo/gingado/blob/main/gingado/benchmark.py#L101){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### compare\n",
       "\n",
       ">      compare (X:numpy.ndarray, y:numpy.ndarray, candidates,\n",
       ">               ensemble_method='object_default', update_benchmark:bool=True)\n",
       "\n",
       "Use a testing dataset to compare the performance of the fitted benchmark model with one or more candidate models using a grid search\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| X | np.ndarray |  | Array-like data of shape (n_samples, n_features) |\n",
       "| y | np.ndarray |  | Array-like data of shape (n_samples,) or (n_samples, n_targets) |\n",
       "| candidates |  |  | Candidate estimator or list of candidate estimator(s) |\n",
       "| ensemble_method | str | object_default |  |\n",
       "| update_benchmark | bool | True | Whether to use the best performing candidate model as the new benchmark |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ggdBenchmark.compare, name=\"compare\", title_level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/dkgaraujo/gingado/blob/main/gingado/benchmark.py#L137){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### compare_fitted_candidates\n",
       "\n",
       ">      compare_fitted_candidates (X, y, candidates, scoring_func)"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/dkgaraujo/gingado/blob/main/gingado/benchmark.py#L137){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### compare_fitted_candidates\n",
       "\n",
       ">      compare_fitted_candidates (X, y, candidates, scoring_func)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ggdBenchmark.compare_fitted_candidates, name=\"compare_fitted_candidates\", title_level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/dkgaraujo/gingado/blob/main/gingado/benchmark.py#L155){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### document\n",
       "\n",
       ">      document\n",
       ">                (documenter:gingado.model_documentation.ggdModelDocumentation|N\n",
       ">                one=None)\n",
       "\n",
       "Document the benchmark model using the template in `documenter`\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| documenter | ggdModelDocumentation \\| None | None | A gingado Documenter or the documenter set in `auto_document` if None. |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/dkgaraujo/gingado/blob/main/gingado/benchmark.py#L155){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### document\n",
       "\n",
       ">      document\n",
       ">                (documenter:gingado.model_documentation.ggdModelDocumentation|N\n",
       ">                one=None)\n",
       "\n",
       "Document the benchmark model using the template in `documenter`\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| documenter | ggdModelDocumentation \\| None | None | A gingado Documenter or the documenter set in `auto_document` if None. |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ggdBenchmark.document, name=\"document\", title_level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/dkgaraujo/gingado/blob/main/gingado/benchmark.py#L168){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### predict\n",
       "\n",
       ">      predict (X, **predict_params)"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/dkgaraujo/gingado/blob/main/gingado/benchmark.py#L168){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### predict\n",
       "\n",
       ">      predict (X, **predict_params)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ggdBenchmark.predict, name=\"predict\", title_level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/dkgaraujo/gingado/blob/main/gingado/benchmark.py#L172){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### fit_predict\n",
       "\n",
       ">      fit_predict (X, y=None, **predict_params)\n",
       "\n",
       "Note: only available if the benchmark implements this method"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/dkgaraujo/gingado/blob/main/gingado/benchmark.py#L172){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### fit_predict\n",
       "\n",
       ">      fit_predict (X, y=None, **predict_params)\n",
       "\n",
       "Note: only available if the benchmark implements this method"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ggdBenchmark.fit_predict, name=\"fit_predict\", title_level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/dkgaraujo/gingado/blob/main/gingado/benchmark.py#L176){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### predict_proba\n",
       "\n",
       ">      predict_proba (X, **predict_proba_params)\n",
       "\n",
       "Note: only available if the benchmark implements this method"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/dkgaraujo/gingado/blob/main/gingado/benchmark.py#L176){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### predict_proba\n",
       "\n",
       ">      predict_proba (X, **predict_proba_params)\n",
       "\n",
       "Note: only available if the benchmark implements this method"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ggdBenchmark.predict_proba, name=\"predict_proba\", title_level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/dkgaraujo/gingado/blob/main/gingado/benchmark.py#L192){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### predict_log_proba\n",
       "\n",
       ">      predict_log_proba (X, **predict_log_proba_params)\n",
       "\n",
       "Note: only available if the benchmark implements this method"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/dkgaraujo/gingado/blob/main/gingado/benchmark.py#L192){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### predict_log_proba\n",
       "\n",
       ">      predict_log_proba (X, **predict_log_proba_params)\n",
       "\n",
       "Note: only available if the benchmark implements this method"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ggdBenchmark.predict_log_proba, name=\"predict_log_proba\", title_level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/dkgaraujo/gingado/blob/main/gingado/benchmark.py#L180){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### decision_function\n",
       "\n",
       ">      decision_function (X)\n",
       "\n",
       "Note: only available if the benchmark implements this method"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/dkgaraujo/gingado/blob/main/gingado/benchmark.py#L180){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### decision_function\n",
       "\n",
       ">      decision_function (X)\n",
       "\n",
       "Note: only available if the benchmark implements this method"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ggdBenchmark.decision_function, name=\"decision_function\", title_level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/dkgaraujo/gingado/blob/main/gingado/benchmark.py#L184){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### score\n",
       "\n",
       ">      score (X)\n",
       "\n",
       "Note: only available if the benchmark implements this method"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/dkgaraujo/gingado/blob/main/gingado/benchmark.py#L184){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### score\n",
       "\n",
       ">      score (X)\n",
       "\n",
       "Note: only available if the benchmark implements this method"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ggdBenchmark.score, name=\"score\", title_level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/dkgaraujo/gingado/blob/main/gingado/benchmark.py#L188){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### score_samples\n",
       "\n",
       ">      score_samples (X)\n",
       "\n",
       "Note: only available if the benchmark implements this method"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/dkgaraujo/gingado/blob/main/gingado/benchmark.py#L188){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### score_samples\n",
       "\n",
       ">      score_samples (X)\n",
       "\n",
       "Note: only available if the benchmark implements this method"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ggdBenchmark.score_samples, name=\"score_samples\", title_level=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification tasks\n",
    "\n",
    "The default benchmark for classification tasks is a [`RandomForestClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) object. Its parameters are fine-tuned in each case according to the user's data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "#| export\n",
    "import numpy as np\n",
    "from gingado.model_documentation import ModelCard\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "#| export\n",
    "\n",
    "class ClassificationBenchmark(ggdBenchmark, ClassifierMixin):\n",
    "    \"A gingado Benchmark object used for classification tasks\"\n",
    "    def __init__(self, \n",
    "    cv=None, \n",
    "    default_cv = StratifiedShuffleSplit(),\n",
    "    estimator=RandomForestClassifier(oob_score=True), \n",
    "    param_grid={'n_estimators': [100, 250], 'max_features': ['sqrt', 'log2', None]}, \n",
    "    param_search=GridSearchCV, \n",
    "    scoring=None, \n",
    "    auto_document=ModelCard, \n",
    "    random_state=None,\n",
    "    verbose_grid=False,\n",
    "    ensemble_method=VotingClassifier):\n",
    "        self.cv = cv\n",
    "        self.default_cv = default_cv\n",
    "        self.estimator = estimator\n",
    "        self.param_grid = param_grid\n",
    "        self.param_search = param_search\n",
    "        self.scoring = scoring\n",
    "        self.auto_document = auto_document\n",
    "        self.random_state = random_state\n",
    "        self.verbose_grid = verbose_grid\n",
    "        self.ensemble_method = ensemble_method\n",
    "        \n",
    "    def fit(\n",
    "        self, \n",
    "        X:np.ndarray, # Array-like data of shape (n_samples, n_features)\n",
    "        y:np.ndarray|None=None # Array-like data of shape (n_samples,) or (n_samples, n_targets) or None\n",
    "        ):\n",
    "        \"Fit the `ClassificationBenchmark` model\"\n",
    "        self._fit(X, y)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/dkgaraujo/gingado/blob/main/gingado/benchmark.py#L203){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ClassificationBenchmark\n",
       "\n",
       ">      ClassificationBenchmark (cv=None,\n",
       ">                               default_cv=StratifiedShuffleSplit(n_splits=10,\n",
       ">                               random_state=None, test_size=None,\n",
       ">                               train_size=None), estimator=RandomForestClassifi\n",
       ">                               er(oob_score=True), param_grid={'n_estimators':\n",
       ">                               [100, 250], 'max_features': ['sqrt', 'log2',\n",
       ">                               None]}, param_search=<class\n",
       ">                               'sklearn.model_selection._search.GridSearchCV'>,\n",
       ">                               scoring=None, auto_document=<class\n",
       ">                               'gingado.model_documentation.ModelCard'>,\n",
       ">                               random_state=None, verbose_grid=False,\n",
       ">                               ensemble_method=<class\n",
       ">                               'sklearn.ensemble._voting.VotingClassifier'>)\n",
       "\n",
       "A gingado Benchmark object used for classification tasks"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/dkgaraujo/gingado/blob/main/gingado/benchmark.py#L203){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ClassificationBenchmark\n",
       "\n",
       ">      ClassificationBenchmark (cv=None,\n",
       ">                               default_cv=StratifiedShuffleSplit(n_splits=10,\n",
       ">                               random_state=None, test_size=None,\n",
       ">                               train_size=None), estimator=RandomForestClassifi\n",
       ">                               er(oob_score=True), param_grid={'n_estimators':\n",
       ">                               [100, 250], 'max_features': ['sqrt', 'log2',\n",
       ">                               None]}, param_search=<class\n",
       ">                               'sklearn.model_selection._search.GridSearchCV'>,\n",
       ">                               scoring=None, auto_document=<class\n",
       ">                               'gingado.model_documentation.ModelCard'>,\n",
       ">                               random_state=None, verbose_grid=False,\n",
       ">                               ensemble_method=<class\n",
       ">                               'sklearn.ensemble._voting.VotingClassifier'>)\n",
       "\n",
       "A gingado Benchmark object used for classification tasks"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ClassificationBenchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/dkgaraujo/gingado/blob/main/gingado/benchmark.py#L227){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### fit\n",
       "\n",
       ">      fit (X:numpy.ndarray, y=None)\n",
       "\n",
       "Fit the `ClassificationBenchmark` model\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| X | np.ndarray |  | Array-like data of shape (n_samples, n_features) |\n",
       "| y | NoneType | None | Array-like data of shape (n_samples,) or (n_samples, n_targets) or None |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/dkgaraujo/gingado/blob/main/gingado/benchmark.py#L227){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### fit\n",
       "\n",
       ">      fit (X:numpy.ndarray, y=None)\n",
       "\n",
       "Fit the `ClassificationBenchmark` model\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| X | np.ndarray |  | Array-like data of shape (n_samples, n_features) |\n",
       "| y | NoneType | None | Array-like data of shape (n_samples,) or (n_samples, n_targets) or None |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ClassificationBenchmark.fit, name=\"fit\", title_level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n",
      "[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n",
      "[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n",
      "[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n",
      "[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n",
      "[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n",
      "[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n",
      "[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n",
      "[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n",
      "[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n",
      "[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n",
      "[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n",
      "[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n",
      "[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n",
      "[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n",
      "[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.4s\n",
      "[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n",
      "[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n",
      "[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n",
      "[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n",
      "[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n",
      "[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n",
      "[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n",
      "[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n",
      "[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n",
      "[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n",
      "[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n",
      "[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n",
      "[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n",
      "[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n",
      "[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n",
      "[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n",
      "[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n",
      "[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n",
      "[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n",
      "[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n",
      "[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n",
      "[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n",
      "[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n",
      "[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n",
      "[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n",
      "[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n",
      "[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n",
      "[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n",
      "[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n",
      "[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n",
      "[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n",
      "[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n",
      "[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n",
      "[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n",
      "[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n",
      "[CV] END ................max_features=None, n_estimators=250; total time=   0.3s\n",
      "[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n",
      "[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n",
      "[CV] END ................max_features=None, n_estimators=250; total time=   0.3s\n",
      "[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n",
      "[CV] END ................max_features=None, n_estimators=250; total time=   0.3s\n",
      "[CV] END ................max_features=None, n_estimators=250; total time=   0.3s\n",
      "[CV] END ................max_features=None, n_estimators=250; total time=   0.3s\n",
      "[CV] END ................max_features=None, n_estimators=250; total time=   0.3s\n",
      "[CV] END ................max_features=None, n_estimators=250; total time=   0.3s\n"
     ]
    }
   ],
   "source": [
    "# some mock up data\n",
    "X, y = make_classification()\n",
    "\n",
    "# the gingado benchmark\n",
    "bm = ClassificationBenchmark(verbose_grid=2).fit(X, y)\n",
    "\n",
    "# note that now the `bm` object can be used as an estimator\n",
    "assert bm.predict(X).shape == y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importantly, `gingado` automatically provides some information to help the user documentat the benchmark model. More specifically, `ggdBenchmark` objects collect model information and pass it to a dictionary with key `info` in a field called `model_details`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_details': {'developer': 'Person or organisation developing the model',\n",
       "  'datetime': '2023-06-22 11:07:52 ',\n",
       "  'version': 'Model version',\n",
       "  'type': 'Model type',\n",
       "  'info': {'_estimator_type': 'classifier',\n",
       "   'best_estimator_': RandomForestClassifier(n_estimators=250, oob_score=True),\n",
       "   'best_index_': 1,\n",
       "   'best_params_': {'max_features': 'sqrt', 'n_estimators': 250},\n",
       "   'best_score_': 0.8700000000000001,\n",
       "   'classes_': array([0, 1]),\n",
       "   'cv_results_': {'mean_fit_time': array([0.11845815, 0.29829991, 0.11383433, 0.27763405, 0.12743814,\n",
       "           0.32717748]),\n",
       "    'std_fit_time': array([0.00532633, 0.02523737, 0.00180408, 0.00226647, 0.00590799,\n",
       "           0.0308631 ]),\n",
       "    'mean_score_time': array([0.0078788 , 0.01841307, 0.00783651, 0.01708548, 0.00718062,\n",
       "           0.01765988]),\n",
       "    'std_score_time': array([0.00068533, 0.00185872, 0.00076878, 0.00036287, 0.00023495,\n",
       "           0.00156245]),\n",
       "    'param_max_features': masked_array(data=['sqrt', 'sqrt', 'log2', 'log2', None, None],\n",
       "                 mask=[False, False, False, False, False, False],\n",
       "           fill_value='?',\n",
       "                dtype=object),\n",
       "    'param_n_estimators': masked_array(data=[100, 250, 100, 250, 100, 250],\n",
       "                 mask=[False, False, False, False, False, False],\n",
       "           fill_value='?',\n",
       "                dtype=object),\n",
       "    'params': [{'max_features': 'sqrt', 'n_estimators': 100},\n",
       "     {'max_features': 'sqrt', 'n_estimators': 250},\n",
       "     {'max_features': 'log2', 'n_estimators': 100},\n",
       "     {'max_features': 'log2', 'n_estimators': 250},\n",
       "     {'max_features': None, 'n_estimators': 100},\n",
       "     {'max_features': None, 'n_estimators': 250}],\n",
       "    'split0_test_score': array([0.9, 0.9, 0.9, 0.9, 1. , 1. ]),\n",
       "    'split1_test_score': array([0.7, 0.8, 0.8, 0.8, 0.7, 0.7]),\n",
       "    'split2_test_score': array([0.8, 0.8, 0.8, 0.8, 0.8, 0.8]),\n",
       "    'split3_test_score': array([1., 1., 1., 1., 1., 1.]),\n",
       "    'split4_test_score': array([0.9, 0.9, 0.9, 0.9, 0.9, 0.9]),\n",
       "    'split5_test_score': array([0.8, 0.8, 0.8, 0.8, 0.8, 0.8]),\n",
       "    'split6_test_score': array([0.8, 0.9, 0.9, 0.9, 0.9, 0.9]),\n",
       "    'split7_test_score': array([0.8, 0.8, 0.8, 0.8, 0.8, 0.8]),\n",
       "    'split8_test_score': array([1. , 1. , 1. , 1. , 0.9, 0.9]),\n",
       "    'split9_test_score': array([0.8, 0.8, 0.7, 0.7, 0.8, 0.8]),\n",
       "    'mean_test_score': array([0.85, 0.87, 0.86, 0.86, 0.86, 0.86]),\n",
       "    'std_test_score': array([0.09219544, 0.0781025 , 0.09165151, 0.09165151, 0.09165151,\n",
       "           0.09165151]),\n",
       "    'rank_test_score': array([6, 1, 4, 4, 2, 2], dtype=int32)},\n",
       "   'multimetric_': False,\n",
       "   'n_features_in_': 20,\n",
       "   'n_splits_': 10,\n",
       "   'refit_time_': 0.2795419692993164,\n",
       "   'scorer_': <function sklearn.metrics._scorer._passthrough_scorer(estimator, *args, **kwargs)>},\n",
       "  'paper': 'Paper or other resource for more information',\n",
       "  'citation': 'Citation details',\n",
       "  'license': 'License',\n",
       "  'contact': 'Where to send questions or comments about the model'},\n",
       " 'intended_use': {'primary_uses': 'Primary intended uses',\n",
       "  'primary_users': 'Primary intended users',\n",
       "  'out_of_scope': 'Out-of-scope use cases'},\n",
       " 'factors': {'relevant': 'Relevant factors',\n",
       "  'evaluation': 'Evaluation factors'},\n",
       " 'metrics': {'performance_measures': 'Model performance measures',\n",
       "  'thresholds': 'Decision thresholds',\n",
       "  'variation_approaches': 'Variation approaches'},\n",
       " 'evaluation_data': {'datasets': 'Datasets',\n",
       "  'motivation': 'Motivation',\n",
       "  'preprocessing': 'Preprocessing'},\n",
       " 'training_data': {'training_data': 'Information on training data'},\n",
       " 'quant_analyses': {'unitary': 'Unitary results',\n",
       "  'intersectional': 'Intersectional results'},\n",
       " 'ethical_considerations': {'sensitive_data': 'Does the model use any sensitive data (e.g., protected classes)?',\n",
       "  'human_life': 'Is the model intended to inform decisions about matters central to human life or flourishing - e.g., health or safety? Or could it be used in such a way?',\n",
       "  'mitigations': 'What risk mitigation strategies were used during model development?',\n",
       "  'risks_and_harms': 'What risks may be present in model usage? Try to identify the potential recipients,likelihood, and magnitude of harms. If these cannot be determined, note that they were considered but remain unknown',\n",
       "  'use_cases': 'Are there any known model use cases that are especially fraught?',\n",
       "  'additional_information': 'If possible, this section should also include any additional ethical considerations that went into model development, for example, review by an external board, or testing with a specific community.'},\n",
       " 'caveats_recommendations': {'caveats': 'For example, did the results suggest any further testing? Were there any relevant groups that were not represented in the evaluation dataset?',\n",
       "  'recommendations': 'Are there additional recommendations for model use? What are the ideal characteristics of an evaluation dataset for this model?'}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm.model_documentation.show_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also simple to define as benchmark a model that you already fitted and still benefit from the other functionalities provided by `Benchmark` class. This can also be done in case you are using a saved version of a fitted model (eg, the model you are using in production) and want to have that as the benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier().fit(X, y)\n",
    "\n",
    "bm.set_benchmark(estimator=forest)\n",
    "\n",
    "assert forest == bm.benchmark\n",
    "assert hasattr(bm.benchmark, \"predict\")\n",
    "assert bm.predict(X).shape == y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression tasks\n",
    "\n",
    "The default benchmark for regression tasks is a [`RandomForestRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) object.  Its parameters are fine-tuned in each case according to the user's data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "#| export\n",
    "import numpy as np\n",
    "from gingado.model_documentation import ModelCard\n",
    "from sklearn.base import RegressorMixin\n",
    "from sklearn.ensemble import RandomForestRegressor, VotingRegressor\n",
    "from sklearn.model_selection import ShuffleSplit, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "#| export\n",
    "\n",
    "class RegressionBenchmark(ggdBenchmark, RegressorMixin):\n",
    "    \"A gingado Benchmark object used for regression tasks\"\n",
    "    def __init__(self, \n",
    "    cv=None, \n",
    "    default_cv=ShuffleSplit(),\n",
    "    estimator=RandomForestRegressor(oob_score=True), \n",
    "    param_grid={'n_estimators': [100, 250], 'max_features': ['sqrt', 'log2', None]}, \n",
    "    param_search=GridSearchCV, \n",
    "    scoring=None, \n",
    "    auto_document=ModelCard, \n",
    "    random_state=None,\n",
    "    verbose_grid=False,\n",
    "    ensemble_method=VotingRegressor):\n",
    "        self.cv = cv\n",
    "        self.default_cv = default_cv\n",
    "        self.estimator = estimator\n",
    "        self.param_grid = param_grid\n",
    "        self.param_search = param_search\n",
    "        self.scoring = scoring\n",
    "        self.auto_document = auto_document\n",
    "        self.random_state = random_state\n",
    "        self.verbose_grid = verbose_grid\n",
    "        self.ensemble_method = ensemble_method\n",
    "\n",
    "    def fit(\n",
    "        self, \n",
    "        X:np.ndarray, # Array-like data of shape (n_samples, n_features)\n",
    "        y:np.ndarray|None=None # Array-like data of shape (n_samples,) or (n_samples, n_targets) or None\n",
    "        ):\n",
    "        \"Fit the `RegressionBenchmark` model\"\n",
    "        self._fit(X, y)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/dkgaraujo/gingado/blob/main/gingado/benchmark.py#L244){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### RegressionBenchmark\n",
       "\n",
       ">      RegressionBenchmark (cv=None, default_cv=ShuffleSplit(n_splits=10,\n",
       ">                           random_state=None, test_size=None, train_size=None),\n",
       ">                           estimator=RandomForestRegressor(oob_score=True),\n",
       ">                           param_grid={'n_estimators': [100, 250],\n",
       ">                           'max_features': ['sqrt', 'log2', None]},\n",
       ">                           param_search=<class\n",
       ">                           'sklearn.model_selection._search.GridSearchCV'>,\n",
       ">                           scoring=None, auto_document=<class\n",
       ">                           'gingado.model_documentation.ModelCard'>,\n",
       ">                           random_state=None, verbose_grid=False,\n",
       ">                           ensemble_method=<class\n",
       ">                           'sklearn.ensemble._voting.VotingRegressor'>)\n",
       "\n",
       "A gingado Benchmark object used for regression tasks"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/dkgaraujo/gingado/blob/main/gingado/benchmark.py#L244){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### RegressionBenchmark\n",
       "\n",
       ">      RegressionBenchmark (cv=None, default_cv=ShuffleSplit(n_splits=10,\n",
       ">                           random_state=None, test_size=None, train_size=None),\n",
       ">                           estimator=RandomForestRegressor(oob_score=True),\n",
       ">                           param_grid={'n_estimators': [100, 250],\n",
       ">                           'max_features': ['sqrt', 'log2', None]},\n",
       ">                           param_search=<class\n",
       ">                           'sklearn.model_selection._search.GridSearchCV'>,\n",
       ">                           scoring=None, auto_document=<class\n",
       ">                           'gingado.model_documentation.ModelCard'>,\n",
       ">                           random_state=None, verbose_grid=False,\n",
       ">                           ensemble_method=<class\n",
       ">                           'sklearn.ensemble._voting.VotingRegressor'>)\n",
       "\n",
       "A gingado Benchmark object used for regression tasks"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(RegressionBenchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/dkgaraujo/gingado/blob/main/gingado/benchmark.py#L268){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### fit\n",
       "\n",
       ">      fit (X:numpy.ndarray, y=None)\n",
       "\n",
       "Fit the `RegressionBenchmark` model\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| X | np.ndarray |  | Array-like data of shape (n_samples, n_features) |\n",
       "| y | NoneType | None | Array-like data of shape (n_samples,) or (n_samples, n_targets) or None |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/dkgaraujo/gingado/blob/main/gingado/benchmark.py#L268){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### fit\n",
       "\n",
       ">      fit (X:numpy.ndarray, y=None)\n",
       "\n",
       "Fit the `RegressionBenchmark` model\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| X | np.ndarray |  | Array-like data of shape (n_samples, n_features) |\n",
       "| y | NoneType | None | Array-like data of shape (n_samples,) or (n_samples, n_targets) or None |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(RegressionBenchmark.fit, name=\"fit\", title_level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.ensemble import AdaBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n",
      "[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n",
      "[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n",
      "[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n",
      "[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n",
      "[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n",
      "[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n",
      "[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n",
      "[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n",
      "[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n",
      "[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n",
      "[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n",
      "[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n",
      "[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n",
      "[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n",
      "[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n",
      "[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n",
      "[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n",
      "[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n",
      "[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n",
      "[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n",
      "[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n",
      "[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n",
      "[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n",
      "[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n",
      "[CV] END ................max_features=None, n_estimators=250; total time=   0.6s\n",
      "[CV] END ................max_features=None, n_estimators=250; total time=   0.6s\n",
      "[CV] END ................max_features=None, n_estimators=250; total time=   0.5s\n",
      "[CV] END ................max_features=None, n_estimators=250; total time=   0.5s\n",
      "[CV] END ................max_features=None, n_estimators=250; total time=   0.5s\n"
     ]
    }
   ],
   "source": [
    "# some mock up data\n",
    "X, y = make_regression()\n",
    "\n",
    "# the gingado benchmark\n",
    "bm = RegressionBenchmark().fit(X, y)\n",
    "\n",
    "# note that now the `bm` object can be used as an estimator\n",
    "assert bm.predict(X).shape == y.shape\n",
    "\n",
    "# the user might also like to set another model as the benchmark\n",
    "adaboost = AdaBoostRegressor().fit(X, y)\n",
    "bm.set_benchmark(estimator=adaboost)\n",
    "\n",
    "assert adaboost == bm.benchmark\n",
    "assert hasattr(bm.benchmark, \"predict\")\n",
    "assert bm.predict(X).shape == y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we compare the benchmark (set above manually to be the adaboost algorithm) with two other candidate models: a Gaussian process and a linear Support Vector Machine (SVM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.svm import LinearSVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark updated!\n",
      "New benchmark:\n",
      "Pipeline(steps=[('candidate_estimator', LinearSVR())])\n"
     ]
    }
   ],
   "source": [
    "gauss_reg = GaussianProcessRegressor().fit(X, y)\n",
    "svm_reg = LinearSVR().fit(X, y)\n",
    "\n",
    "bm.compare(X, y, candidates=[gauss_reg, svm_reg])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when the benchmark object finds a model that performs better than it does, the user is informed that the benchmark is updated and the new benchmark model is shown. This only happens when the argument `update_benchmark` is set to True (as default).\n",
    "\n",
    "Below we can see by how much it outperformed the other candidates, including the previous benchmark model and an ensemble of the previous benchmark and all the candidates. It is also a good opportunity to see how stable the performance of each model was, as judged by the standard deviation of the scores across the validation folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'candidate_estimator': (DecisionTreeRegressor...</td>\n",
       "      <td>0.364793</td>\n",
       "      <td>0.190020</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'candidate_estimator': GaussianProcessRegress...</td>\n",
       "      <td>-0.199963</td>\n",
       "      <td>0.256781</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'candidate_estimator': LinearSVR(), 'candidat...</td>\n",
       "      <td>0.544040</td>\n",
       "      <td>0.094230</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'candidate_estimator': VotingRegressor(estima...</td>\n",
       "      <td>0.344233</td>\n",
       "      <td>0.117852</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              params  mean_test_score  \\\n",
       "0  {'candidate_estimator': (DecisionTreeRegressor...         0.364793   \n",
       "1  {'candidate_estimator': GaussianProcessRegress...        -0.199963   \n",
       "2  {'candidate_estimator': LinearSVR(), 'candidat...         0.544040   \n",
       "3  {'candidate_estimator': VotingRegressor(estima...         0.344233   \n",
       "\n",
       "   std_test_score  rank_test_score  \n",
       "0        0.190020                2  \n",
       "1        0.256781                4  \n",
       "2        0.094230                1  \n",
       "3        0.117852                3  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(bm.benchmark.cv_results_)[['params', 'mean_test_score', 'std_test_score', 'rank_test_score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General comments on benchmarks\n",
    "\n",
    "## Scoring\n",
    "\n",
    "`ClassificationBenchmark` and `RegressionBenchmark` use the default scoring method for comparing model alternatives, both during estimation of the benchmark model and when comparing this benchmark with candidate models. Users are encouraged to consider if another scoring method is more suitable for their use case. More information on available scoring methods that are compatible with `gingado` Benchmark objects can be found [here](https://scikit-learn.org/stable/modules/model_evaluation.html).\n",
    "\n",
    "## Data split\n",
    "\n",
    "`gingado` benchmarks rely on hyperparameter tuning to discover the benchmark specification that is most likely to perform better with the user data. This tuning in turn depends on a data splitting strategy for the cross-validation. By default, `gingado` uses `StratifiedShuffleSplit` (in classification problems) or `ShuffleSplit` (in regression problems) if the data is not time series and `TimeSeriesSplit` otherwise.\n",
    "\n",
    "The user may overrun these defaults either by directly setting the parameter `cv` or `default_cv` when instanciating the `gingado` benchmark class. The difference is that `default_cv` is only used after `gingado` checks that the data is not a time series (if a time dimension exists, then `TimeSeriesSplit` is used).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification()\n",
    "bm_cls = ClassificationBenchmark(cv=TimeSeriesSplit(n_splits=3)).fit(X, y)\n",
    "assert bm_cls.benchmark.n_splits_ == 3\n",
    "\n",
    "X, y = make_regression()\n",
    "bm_reg = RegressionBenchmark(default_cv=ShuffleSplit(n_splits=7)).fit(X, y)\n",
    "assert bm_reg.benchmark.n_splits_ == 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please refer to [this page](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection) for more information on the different `Splitter` classes available on `scikit-learn`, and [this page](https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html#sphx-glr-auto-examples-model-selection-plot-cv-indices-py) for practical advice on how to choose a splitter for data that are not time series. Any one of these objects (or a custom splitter that is compatible with them) can be passed to a `Benchmark` object.\n",
    "\n",
    "Users that wish to use specific parameters should include the actual `Splitter` object as the parameter, as done with the `n_splits` parameter in the chunk above.\n",
    "\n",
    "## Custom benchmarks\n",
    "\n",
    "`gingado` provides users with two `Benchmark` objects out of the box: `ClassificationBenchmark` and `RegressionBenchmark`, to be used depending on the task at hand. Both classes derive from a base class `ggdBenchmark`, which implements methods that facilitate model comparison. Users that want to create a customised benchmark model for themselves have two options:\n",
    "\n",
    "* the simpler possibility is to train the estimator as usual, and then assign the fitted estimator to a `Benchmark` object. \n",
    "\n",
    "* if the user wants more control over the fitting process of estimating the benchmark, they can create a class that subclasses from `ggdBenchmark` and either implements custom `fit`, `predict` and `score` methods, or also subclasses from [`scikit-learn`'s `BaseEstimator`](https://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html). \n",
    "  * In any case, if the user wants the benchmark to automatically detect if the data is a time series and also to document the model right after fitting, the `fit` method should call `self._fit` on the data. Otherwise, the user can simply implement any consistent logic in fit as the user sees fit (pun intended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
