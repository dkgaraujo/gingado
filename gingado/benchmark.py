# AUTOGENERATED! DO NOT EDIT! File to edit: 00_benchmark.ipynb (unless otherwise specified).

__all__ = ['ModelCard', 'ggdBenchmark', 'ClassificationBenchmark', 'RegressionBenchmark']

# Cell
import pandas as pd
from sklearn.model_selection import TimeSeriesSplit, StratifiedShuffleSplit, GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.utils.metaestimators import available_if
#from gingado.model_documentation import ModelCard

class ModelCard:
    def __init__(self):
        pass

def _benchmark_has(attr):
        def check(self):
            getattr(self.benchmark, attr)
            return True
        return check

class ggdBenchmark:
    """
    The base class for gingado's Benchmark objects.
    """
    def _check_is_time_series(self, X, y=None):
        """
        Checks whether the data is a time series, and sets a data splitter
        accordingly if no data splitter is provided by the user
        Note: all data without an index (eg, a Numpy array) are considered to NOT be a time series
        """
        if hasattr(X, "index"):
            self.is_timeseries = pd.core.dtypes.common.is_datetime_or_timedelta_dtype(X.index)
        else:
            self.is_timeseries = False
        if self.is_timeseries and y:
            if hasattr(y, "index"):
                self.is_timeseries = pd.core.dtypes.common.is_datetime_or_timedelta_dtype(y.index)
            else:
                self.is_timeseries = False

        if self.cv is None:
            self.cv = TimeSeriesSplit() if self.is_timeseries else StratifiedShuffleSplit()

    def _creates_estimator(self):
        if self.estimator is None:
            pass

    def _fit(self, X, y):
        self._check_is_time_series(X, y)

        X, y = self._validate_data(X, y)

        if self.param_search and self.param_grid:
            self.benchmark = self.param_search(estimator=self.estimator, param_grid=self.param_grid, scoring=self.scoring)
            self.benchmark.fit(X, y)

        if self.auto_document:
            self.document()

        return self

    def set_benchmark(self, estimator):
        self.benchmark = estimator

    def compare(self, X, candidate):
        """
        Uses a test dataset to compare the performance of the fitted benchmark model with one or more candidate models
        This method achieves this by conducting a grid search
        """
        # Step 1: create a param_grid *list* where the first item is the current benchmark,
        # ... the other elements are the candidate model(s), and the final model is an ensemble
        # ... of all the previous models (including the benchmark), with uniform weights (1/N)

        # Step 2: Evaluate them using the same CV as strategy as defined in self.cv and select the best model

        # Step 3: The best model (or the ensemble) is now the current benchmark
        pass

    def document(self):
        pass

    @available_if(_benchmark_has("predict"))
    def predict(self, X, **predict_params):
        return self.benchmark.predict(X, **predict_params)

    @available_if(_benchmark_has("fit_predict"))
    def fit_predict(self, X, y=None, **predict_params):
        return self.benchmark.fit_predict(X, y, **predict_params)

    @available_if(_benchmark_has("predict_proba"))
    def predict_proba(self, X, **predict_proba_params):
        return self.benchmark.predict_proba(X, **predict_proba_params)

    @available_if(_benchmark_has("decision_function"))
    def decision_function(self, X):
        return self.benchmark.decision_function(X)

    @available_if(_benchmark_has("decision_function"))
    def decision_function(self, X):
        return self.benchmark.decision_function(X)

    @available_if(_benchmark_has("score_samples"))
    def score_samples(self, X):
        return self.benchmark.score_samples(X)

    @available_if(_benchmark_has("predict_log_proba"))
    def predict_log_proba(self, X, **predict_log_proba_params):
        return self.benchmark.predict_log_proba(X, **predict_log_proba_params)

# Cell
from sklearn.ensemble import RandomForestClassifier

class ClassificationBenchmark(ggdBenchmark):
    def __init__(self, cv=None, estimator=RandomForestClassifier(), param_grid=None, param_search=GridSearchCV, scoring=None, auto_document=ModelCard()):
        self.cv = cv
        self.estimator = estimator
        self.param_grid = param_grid
        self.param_search = param_search
        self.scoring = scoring
        self.auto_document = auto_document

    def fit(self, X, y=None):
        self._fit(X, y)
        return self

# Cell
from sklearn.ensemble import RandomForestRegressor

class RegressionBenchmark(ggdBenchmark):
    def __init__(self, cv=None, estimator=RandomForestRegressor(), param_search=GridSearchCV, param_grid=None, scoring=None, auto_document=ModelCard()):
        self.cv = cv
        self.estimator = estimator
        self.param_grid = param_grid
        self.param_search = param_search
        self.scoring = scoring
        self.auto_document = auto_document

    def fit(self, X, y=None):
        self._fit(X, y)
        return self