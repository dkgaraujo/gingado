{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#skip\n",
    "! [ -e /content ] && pip install -Uqq pip gingado nbdev # install or upgrade gingado on colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using gingado to understand economic growth\n",
    "> An illustration with Barro and Lee (1994)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook showcases one possible use of `gingado` by estimating economic growth across countries, using the dataset studied by Barro and Lee (1994, \"Sources of economic growth\", Carnegie-Rochester Conference Series on Public Policy, 40, p.1-46). You can run this notebook interactively, by clicking on the appropriate link above.\n",
    "\n",
    "This dataset has been widely studied in economics. Belloni et al (2011, \"Inference for high-dimensional sparse econometric models\") and Giannone et al (2021, \"Economic predictions with big data: the illusion of sparsity\", Econometrica, 89, 5, p.2409-2437) are two studies of this dataset that are most related to machine learning.\n",
    "\n",
    "This notebook will use `gingado` to compare different classes of models (and their combination in a single model), select the best performing alternative, and document it. Because the notebook is for pedagogical purposes only, please bear in mind some aspects of the machine learning workflow (such as carefully thinking about the cross-validation strategy) are glossed over in this notebook.\n",
    "\n",
    "For a more thorough description of `gingado`, please refer to the package's [website](https://dkgaraujo.github.io/gingado) and to the academic [material](https://www.github.com/dkgaraujo/gingado_comms) about it.\n",
    "\n",
    "## Setting the stage\n",
    "\n",
    "We will import packages as the work progresses. This will help highlight the specific steps in the workflow that `gingado` can be helpful with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is available in the [online annex](https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA17842) to Giannone et al (2021). In that paper, this dataset corresponds to what the authors call \"macro2\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/CvSamplesMacro2.mat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/Coding/.venv_gingado/lib/python3.10/site-packages/scipy/io/matlab/mio.py:39\u001b[0m, in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 39\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mopen\u001b[39;49m(file_like, mode), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mIOError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     41\u001b[0m     \u001b[39m# Probably \"not found\"\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/CvSamplesMacro2.mat'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/douglasaraujo/Coding/gingado/01_BarroLee1994.ipynb Cell 6'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/gingado/01_BarroLee1994.ipynb#ch0000005?line=0'>1</a>\u001b[0m growth_data \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39;49mloadmat(\u001b[39m'\u001b[39;49m\u001b[39mdata/CvSamplesMacro2.mat\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/gingado/01_BarroLee1994.ipynb#ch0000005?line=1'>2</a>\u001b[0m colnames \u001b[39m=\u001b[39m [m[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mstrip() \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m growth_data[\u001b[39m'\u001b[39m\u001b[39mMnem\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m]]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/gingado/01_BarroLee1994.ipynb#ch0000005?line=2'>3</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(growth_data[\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m], columns\u001b[39m=\u001b[39mcolnames)\n",
      "File \u001b[0;32m~/Coding/.venv_gingado/lib/python3.10/site-packages/scipy/io/matlab/mio.py:224\u001b[0m, in \u001b[0;36mloadmat\u001b[0;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[39mLoad MATLAB file.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[39m    3.14159265+3.14159265j])\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    223\u001b[0m variable_names \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39mvariable_names\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 224\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_context(file_name, appendmat) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m    225\u001b[0m     MR, _ \u001b[39m=\u001b[39m mat_reader_factory(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    226\u001b[0m     matfile_dict \u001b[39m=\u001b[39m MR\u001b[39m.\u001b[39mget_variables(variable_names)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/contextlib.py:135\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwds, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc\n\u001b[1;32m    134\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen)\n\u001b[1;32m    136\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgenerator didn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt yield\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "File \u001b[0;32m~/Coding/.venv_gingado/lib/python3.10/site-packages/scipy/io/matlab/mio.py:17\u001b[0m, in \u001b[0;36m_open_file_context\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39m@contextmanager\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_context\u001b[39m(file_like, appendmat, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m---> 17\u001b[0m     f, opened \u001b[39m=\u001b[39m _open_file(file_like, appendmat, mode)\n\u001b[1;32m     18\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     19\u001b[0m         \u001b[39myield\u001b[39;00m f\n",
      "File \u001b[0;32m~/Coding/.venv_gingado/lib/python3.10/site-packages/scipy/io/matlab/mio.py:45\u001b[0m, in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[39mif\u001b[39;00m appendmat \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m file_like\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m.mat\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     44\u001b[0m         file_like \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.mat\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> 45\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mopen\u001b[39;49m(file_like, mode), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(\n\u001b[1;32m     48\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mReader needs file name or open file-like object\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     49\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/CvSamplesMacro2.mat'"
     ]
    }
   ],
   "source": [
    "growth_data = io.loadmat('data/GrowthData.mat')\n",
    "colnames = [m[0].strip() for m in growth_data['Mnem'][0]]\n",
    "df = pd.DataFrame(growth_data['data'], columns=colnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains explanatory variables and the outcome variable, representing per-capita growth between 1960 and 1985, for 90 countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the remainder of this notebook, the dataset will be divided into an `X` dataset of regressors and a `y` vector of outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = df.pop('Outcome')\n",
    "X = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.plot.hist(bins=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Establishing a benchmark model\n",
    "\n",
    "Generally speaking, it is a good idea to establish a benchmark model at the first stages of development of the machine learning model. `gingado` offers a class of automatic benchmarks that can be used off-the-shelf depending on the task at hand: `RegressionBenchmark` and `ClassificationBenchmark`. It is also good to keep in mind that more advanced users can create their own benchmark on top of a base class provided by `gingado`: `ggdBenchmark`.\n",
    "\n",
    "For this application, since we are interested in running a regression task, we will use `RegressionBenchmark`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gingado.benchmark import RegressionBenchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What this object does is the following:\n",
    "* it creates a random forest\n",
    "* three different vesions of the random forest are trained on the user data\n",
    "* the version that performs better is chosen as the benchmark\n",
    "* right after it is trained, the benchmark is documented using `gingado`'s `ModelCard` documenter.\n",
    "\n",
    "The user can easily change the parameters above. For example, instead of a random forest the user might prefer a neural network as the benchmark. Or, in lieu of the default parameters provided by `gingado`, users might have their own idea of what could be a reasonable parameter space to search.\n",
    "\n",
    "Random forests are chosen as the go-to benchmark algorithm because of their reasonably good performance in a wide variety of settings, the fact that they don't require much data transformation (ie, normalising the data to have zero mean and one standard deviation), and by virtue of their relatively transparency about the importance of each regressor.\n",
    "\n",
    "The first step is to initialise the benchmark object. At this time, we pass some arguments about how we want it to behave. In this case, we set the verbosity level to produce output related to each alternative considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark = RegressionBenchmark(verbose_grid=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to fit the `benchmark` object to the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse_output\n",
    "benchmark.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, with a few lines we have trained a random forest on the dataset. In this case, the benchmark was the better of three versions of the random forest: one with 50 estimators, another with 100 and the thid with 250 estimators. They were each trained using a 5-fold cross-validation. Let's see which one was the best performing in this case, and hence our benchmark model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(benchmark.benchmark.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values above are calculated with $R^2$, the default scoring function for a random forest from the `scikit-learn` package. Suppose that instead we would like a benchmark model that is optimised on the maximum error, ie a benchmark that minimises the worst deviation from prediction to ground truth for all the sample. These are the steps that we would take. Note that a more complete list of ready-made scoring parameters and how to create your own function can be found [here](https://scikit-learn.org/stable/modules/model_evaluation.html#)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse_output\n",
    "benchmark_lower_worsterror = RegressionBenchmark(scoring='max_error', verbose_grid=3)\n",
    "benchmark_lower_worsterror.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(benchmark_lower_worsterror.benchmark.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we even have two benchmark models 😎.\n",
    "\n",
    "We could further tweek and adjust them, but one of the ideas behind having a benchmark is that it is simple and easy to set up. \n",
    "\n",
    "Let's now look at the predictions, comparing them to the original growth values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = benchmark.predict(X)\n",
    "\n",
    "pd.DataFrame({\n",
    "    'y': y,\n",
    "    'y_pred': y_pred\n",
    "    }).plot.scatter(x='y', y='y_pred', grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now a histogram of the benchmark's errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y - y_pred).plot.hist(bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the benchmark is a random forest model, we can see what are the most important regressors, measured as the average reduction in impurity across the trees in the random forest that actually use that particular regressor. They are scaled so that the sum for all features is one. Higher importance amounts indicate that that particular regressor is a more important contributor to the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor_importance = pd.DataFrame(benchmark.benchmark.best_estimator_.feature_importances_, index=X.columns, columns=[\"Importance\"])\n",
    "\n",
    "regressor_importance.sort_values(by=\"Importance\", ascending=False).plot.bar(figsize=(20, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph above, we can see that the regressor `bmp1l` (black-market premium on foreign exchange) predominates. Interestingly, Belloni et al (2011) using squared-root lasso also find this regressor to be important.\n",
    "\n",
    "## Model documentation\n",
    "\n",
    "Importantly, the benchmark already has some baseline documentation set up. If the user wishes, they can use that as a basis to document their model. Note that the output is in a raw format that is suitable for machine reading and writing. Intermediary and advanced users may wish to use that format to construct personalised forms, documents, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse_output\n",
    "benchmark.model_documentation.show_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is some information in the model documentation that was automatically added, we might want to concentrate on the fields in the model card that are yet to be answered. Actually, this is the purpose of `gingado`'s automatic documentation: to afford users more time so they can invest, if they want, on model documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse_output\n",
    "benchmark.model_documentation.open_questions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fill some information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark.model_documentation.fill_info({\n",
    "    'intended_use': {\n",
    "        'primary_uses': 'This model is trained for pedagogical uses only.',\n",
    "        'primary_users': 'Everyone is welcome to follow the description showing the development of this benchmark.'\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the format, based on a Python dictionary. In particular, the `open_questions` method results include keys divided by double underscores. As seen above, these should be interpreted as different levels of the documentation template, leading to a nested dictionary. \n",
    "\n",
    "Now when we confirm that the questions answered above are no longer \"open questions\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse_output\n",
    "benchmark.model_documentation.open_questions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want, at any time we can save the documentation to a local JSON file, as well as read another document.\n",
    "\n",
    "## Trying out model alternatives\n",
    "\n",
    "The benchmark model may be enough for some analyses, or maybe the user is interested in using the benchmark to explore the data and have an understanding of the importance of each regressor, to concentrate their work on data that can be meaningful for their purposes. But oftentimes a user will want to seek a machine learning model that performs as well as possible.\n",
    "\n",
    "For users that want to manually create other models, `gingado` allows the possibility of comparing them with the benchmark. If the user model is better, it becomes the new benchmark!\n",
    "\n",
    "For the following analyses, we will use K-fold as cross-validation, with 5 splits of the sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First candidate: a gradient boosting tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse_output\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.25],\n",
    "    'max_depth': [3, 6, 9]\n",
    "}\n",
    "\n",
    "reg_gradbooster = GradientBoostingRegressor()\n",
    "\n",
    "gradboosterg_grid = GridSearchCV(\n",
    "    reg_gradbooster,\n",
    "    param_grid,\n",
    "    n_jobs=-1,\n",
    "    verbose=3\n",
    ").fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gradboosterg_grid.predict(X)\n",
    "pd.DataFrame({\n",
    "    'y': y,\n",
    "    'y_pred': y_pred\n",
    "    }).plot.scatter(x='y', y='y_pred', grid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y - y_pred).plot.hist(bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second candidate: lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse_output\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "param_grid = {\n",
    "    'alpha': [0.5, 1, 1.25],\n",
    "}\n",
    "\n",
    "reg_lasso = Lasso(fit_intercept=True)\n",
    "\n",
    "lasso_grid = GridSearchCV(\n",
    "    reg_lasso,\n",
    "    param_grid,\n",
    "    n_jobs=-1,\n",
    "    verbose=3\n",
    ").fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lasso_grid.predict(X)\n",
    "pd.DataFrame({\n",
    "    'y': y,\n",
    "    'y_pred': y_pred\n",
    "    }).plot.scatter(x='y', y='y_pred', grid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y - y_pred).plot.hist(bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the models with the benchmark\n",
    "\n",
    "`gingado` allows users to compare different candidate models with the existing benchmark in a very simple way: using the `compare` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse_output\n",
    "candidates = [gradboosterg_grid, lasso_grid]\n",
    "benchmark.compare(X, y, candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output above clearly indicates that after evaluating the models - and their ensemble together with the existing benchmark - at least one of them was better than the current benchmark. Therefore, it will now be the new benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = benchmark.predict(X)\n",
    "pd.DataFrame({\n",
    "    'y': y,\n",
    "    'y_pred': y_pred\n",
    "    }).plot.scatter(x='y', y='y_pred', grid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y - y_pred).plot.hist(bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model documentation\n",
    "\n",
    "After this process, we can now see how the model documentation was updated automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse_output\n",
    "benchmark.model_documentation.show_json()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit ('.venv_gingado': venv)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
